# ASPLOS

## 2025

<span id ="asplos2501">[vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention](#asplos2501-1)</span>

<span id ="asplos2502">[Accelerating LLM Serving for Multi-turn Dialogues with Efficient Resource Management](#asplos2502-1)</span>

<span id ="asplos2503">[M5: Mastering Page Migration and Memory  Management for CXL-based Tiered Memory Systems](#asplos2503-1)</span>

<span id ="asplos2504">[MoE-Lightning: High-Throughput MoE Inference on  Memory-constrained GPUs](#asplos2504-1)</span>

[<span id ="asplos2505">CXLfork: Fast Remote Fork over CXL Fabrics</span>](#asplos2505-1)

[<span id ="asplos2506">PIM Is All You Need: A CXL-Enabled GPU-Free System  for Large Language Model Inference</span>](#asplos2506-1)

[<span id ="asplos2507">PAPI: Exploiting Dynamic Parallelism  in Large Language Model Decoding with a  Processing-In-Memory-Enabled Computing System</span>](#asplos2507-1)

[<span id ="asplos2508">POD-Attention: Unlocking Full Prefill-Decode  Overlap for Faster LLM Inference</span>](#asplos2508-1)

## [<span id ="asplos2501-1">vAttention: Dynamic Memory Management for  Serving LLMs without PagedAttention</span>](#asplos2501)

        本文提出针对 LLM 服务的动态内存管理方案 vAttention，旨在解决 PagedAttention 需重写内核、开销高、可移植性差的问题。其核心是通过 CUDA VMM API 解耦虚拟与物理内存分配，提前预留连续虚拟内存缓冲区以保留 KV 缓存虚拟连续性，同时按需映射物理内存缓解碎片；并通过内存分配与计算重叠、延迟回收、小页面支持等优化，降低运行时延迟。实验表明，在 Yi-6B 等模型上，vAttention 预填充阶段比 FlashAttention-2/FlashInfer 的 Paged 版本快 1.17-1.36 倍，端到端离线吞吐量提升 1.13-1.23 倍，还能无缝支持 FlashAttention-3 等新内核，兼顾性能、简洁性与可移植性。

### 一、研究背景与现有方案痛点

LLM 服务中，KV 缓存占 GPU 推理内存的主要部分，其动态增长特性（单请求逐 token 扩展、总长度未知）导致内存分配难题：

1. **静态分配**（如 Orca、FasterTransformer）按模型最大上下文长度预留内存，引发严重内部碎片，限制批处理大小和吞吐量；
2. **PagedAttention 方案**（vLLM 提出，被 TensorRT-LLM 等广泛采用）通过按需分配小内存块缓解碎片，但存在根本缺陷：
   - 需重写注意力内核以支持非连续 KV 缓存访问，难以跟进最新优化（如 vLLM 的 Paged 内核比 FlashAttention-2 慢 2.8 倍）；
   - 服务框架需额外实现内存管理，重复 OS 的虚实地址转换功能，增加冗余；
   - 运行时开销显著：GPU 侧因块表查询、寄存器溢出等慢 37%（FlashAttention-2 预填充阶段）至 42%（FlashInfer 预填充阶段），CPU 侧因块表准备等增加 10%-30% 延迟；
   - 可移植性差，新内核（如 FlashAttention-3）发布时无 PagedAttention 支持。

### 二、vAttention 核心设计

核心思路：**保留 KV 缓存的虚拟内存连续性，同时动态分配物理内存**，通过 CUDA 虚拟内存管理（VMM）API 解耦虚实内存分配，避免 PagedAttention 的非连续布局缺陷。

1. **内存分配策略**
   
   - 虚拟内存：提前预留超大连续缓冲区（按最大批处理大小和模型最大上下文长度配置），利用 64 位系统充足的虚拟地址空间（单进程可达 128TB），无需担心虚拟内存碎片；
   - 物理内存：运行时按需分配，仅在请求需要时将物理页面映射到虚拟缓冲区，避免提前占用。

2. **关键技术细节**
   
   - 支持多粒度页面：修改开源 CUDA 统一内存驱动，新增 64KB/128KB/256KB 小页面支持（默认 CUDA VMM 仅支持 2MB 大页面），降低物理内存碎片；
   - 请求级 KV 缓存索引：通过唯一 reqId 定位批处理中每个请求的 KV 缓存子张量，确保地址访问连续性；
   - 兼容现有框架：作为 Python 库集成到 vLLM 等服务框架，提供 init/alloc_reqid/free_reqid/step 等简洁 API，无需修改模型或注意力内核。

3. **针对性优化**
   
   - 隐藏分配延迟：利用解码阶段内存需求的可预测性，通过后台线程将内存分配与计算重叠；预填充阶段采用延迟回收 + 预分配策略，复用已释放的物理页面；
   - 缓解碎片：小页面支持使分配粒度匹配 KV 缓存增长特性（单 token 仅需数十 KB），且无 TLB 抖动风险；
   - 支持连续批处理：借助 FlashAttention 的 cache_batch_idx API，处理请求退出后的虚拟内存 "空洞"，保持批处理灵活性。

### 三、实验结果

基于 Yi-6B/Llama-3-8B/Yi-34B 模型，在 A100（单卡 / 双卡 NVLink）和 H100 GPU 上的测试表明：

1. **性能优势**
   
   - 预填充阶段：长上下文（192K）下，vAttention 比 FlashAttention-2 的 Paged 版本快 1.24-1.26 倍，比 FlashInfer 的 Paged 版本快 1.17-1.36 倍；
   - 解码阶段：与 FlashAttention-2 的 Paged 版本性能相当，比 vLLM 的 Paged 内核快 1.53-1.99 倍，比 FlashInfer 的 Paged 版本快 1.23 倍；
   - 端到端吞吐量：离线长上下文任务（arXiv 摘要）中，比 FlashAttention-2 Paged 快 1.13-1.18 倍，比 FlashInfer Paged 快 1.14-1.23 倍；在线场景下中位数延迟降低 28%-42%。

2. **可移植性**
   
   - 无需修改代码即可支持 FlashAttention-3（Hopper 架构优化内核），在 H100 上比 FlashAttention-2 Paged 版本吞吐量提升 1.26-1.5 倍。

3. **资源效率**
   
   - 小页面（64KB）使最大批处理大小提升 1.18-1.28 倍；内存分配带宽达 7.6GB/s，远超 LLM 推理需求（750MB/s）。

### 四、核心贡献

1. 提出虚实内存解耦的 KV 缓存管理方案，兼顾连续性（无内核修改）和动态性（无碎片）；
2. 解决 CUDA VMM 的延迟和大页面碎片问题，提供 LLM 专用优化；
3. 实现简单、可移植、高性能的替代方案，支持现有主流注意力内核（FlashAttention-2/3、FlashInfer），降低 LLM 服务的部署和维护成本。
   
    

## [<span id ="asplos2502-1">Accelerating LLM Serving for Multi-turn Dialogues  with Efficient Resource Management</span>](#asplos2502)

        本文针对现有 LLM 服务框架在处理多轮对话时存在的历史注意力键值对（KVs）重计算开销大、FCFS 调度导致 GPU 内存利用率低（头阻塞）两大问题，提出了名为 FlashGen 的解决方案：通过设计包含 GPU、CPU 内存和 SSD 的多级 KV 缓存（FlashGen-Cache），动态选择缓存恢复与重计算以减少冗余计算，同时采用请求重排序调度（FlashGen-Sched），在优先调度可运行短请求提升内存利用率的同时，通过抢占机制避免长请求饥饿；基于 Azure 实例（双 A100 GPU 等配置），在 OPT、Llama-2 系列模型及 ShareGPT 等数据集上的实验表明，FlashGen 在相似延迟下，对 OPT 30B 和 Llama-2 70B 的吞吐量分别提升 1.63 倍和 2.85 倍，显著优化了多轮对话场景下的 LLM 服务性能

### 一、研究背景与核心问题

1. **多轮对话的 LLM 服务挑战**：随着 LLM 在聊天机器人等场景的广泛应用，长上下文（如多轮对话）处理需求激增，但现有框架（如 vLLM、TensorRT-LLM）存在两大关键效率问题：
   
   - **KV 重计算开销**：多轮对话中，用户查询会包含历史对话内容，导致提示词长度 “放大”，现有框架因 GPU 内存有限无法缓存所有历史注意力键值对（KVs），需重复计算，耗费大量资源。
   - **GPU 内存利用率低**：采用先到先服务（FCFS）调度策略时，长提示词请求会阻塞后续短请求，导致 GPU 内存闲置（头阻塞问题），尤其在高负载下缓存竞争加剧，利用率进一步下降。

2. **数据支撑**：基于 ShareGPT 真实对话数据集的分析显示，对话会话平均包含 7 轮、中位数 3 轮，多轮对话的提示词长度较单轮增长 99 倍，历史对话内容占总输入 tokens 的一半以上，验证了 “提示词放大” 问题的严重性。

### 二、核心解决方案：FlashGen

FlashGen 通过**多级 KV 缓存管理**和**请求重排序调度**两大核心技术，高效利用 GPU、CPU（DRAM）和 SSD 资源，解决上述问题。

#### 1. 多级 KV 缓存（FlashGen-Cache）

- **设计目标**：避免历史 KVs 重复计算，通过多级存储分层缓存，平衡内存成本与访问 latency。
- **缓存层级**：
  - 一级缓存（GPU 内存）：缓存当前运行请求的 KVs 及已完成请求的可回收 KVs，优先命中以减少传输开销。
  - 二级缓存（CPU 内存）：异步复制 GPU 生成的 KVs，GPU 内存不足时可快速恢复，通过流水线技术重叠 KV 传输与模型计算，隐藏延迟。
  - 三级缓存（SSD）：当 CPU 内存不足以存储所有历史 KVs 时，异步归档不常用 KVs；通过 CPU 内存 “预加载” 机制，避免直接从 SSD 读取的高延迟，必要时动态选择 “重计算” 而非 “SSD 读取” 以优化性能。
- **关键优化**：批量感知 KV 恢复、主动缓存策略（生成时即复制到 CPU），减少内存回收与传输开销。

#### 2. 请求重排序调度（FlashGen-Sched）

- **设计目标**：解决头阻塞问题，提升 GPU 内存利用率，同时保证请求公平性。
- **核心策略**：
  - 贪心重排序：当队列头部的长请求因内存不足无法执行时，优先调度后续可放入空闲内存的短请求（“提升请求”），避免内存闲置。
  - 无饥饿机制：实时跟踪 GPU 内存使用，当空闲内存 + 提升请求占用内存足以容纳被阻塞的长请求时，抢占提升请求，优先执行长请求，避免其饥饿。
- **效果**：GPU 内存利用率从 vLLM 的 88% 提升至 98% 以上，批量请求规模平均增加 1.06~1.15 倍。

### 三、实验验证与结果

1. **实验环境**：Azure 实例（2×A100 GPU、440GB CPU 内存、2×960GB NVMe SSD），模型包括 OPT（13B/30B/66B/175B）、Llama-2（13B/70B），数据集涵盖 ShareGPT（多轮对话）、Alpaca、HumanEval。

2. **核心性能指标**：
   
   - 吞吐量：在相似延迟下，FlashGen 对 OPT 30B 和 Llama-2 70B 的吞吐量分别提升 1.63 倍和 2.85 倍。
   - 延迟：P95 首 token 延迟（TTFT）较 vLLM 降低 77%（OPT 30B）和 66%（Llama-2 13B）；单 token 生成延迟（TPOT）的 P99 值从 vLLM 的 608ms 降至 103ms（OPT 30B）。
   - 缓存效果：GPU+CPU+SSD 三级缓存的 KV 命中率显著高于单一层级，高负载下仍能维持稳定命中，减少重计算比例。

3. **对比基准**：优于 vLLM（基线）和 CachedAttention（同类 KV 缓存方案），尤其在高负载、长上下文场景下，因动态调度与多级缓存的协同优化，性能优势更明显。

### 四、相关工作与结论

1. **相关工作对比**：
   
   - KV 复用：CachedAttention 仅支持 CPU/SSD 缓存，未动态选择重计算；SGLang 仅依赖 GPU 缓存，不支持异构存储。
   - 调度优化：现有迭代级调度未解决多轮对话的头阻塞问题；Sarathi-Serve 聚焦长提示词拆分，不涉及请求重排序。
   - 内存优化：PagedAttention 优化内存分配，但未解决历史 KV 缓存与调度协同问题。

2. **研究结论**：
   
   - FlashGen 通过多级 KV 缓存和请求重排序的协同设计，有效解决了多轮对话中 “KV 重计算” 和 “GPU 内存闲置” 两大核心问题。
   - 在长上下文、高负载场景下性能优势显著，为 LLM 多轮服务（如聊天机器人）提供了高效、低成本的解决方案，随着对话轮数增加，优化价值更突出。
   
   

## [<span id ="asplos2503-1">M5: Mastering Page Migration and Memory  Management for CXL-based Tiered Memory Systems</span>](#asplos2503)

​	本文针对 CXL 基于分层内存系统中传统 CPU 驱动页面迁移方案精度低、开销大且无法区分稀疏页面的问题，首先提出基于 FPGA 的 CXL 驱动页面与字访问计数方案（PAC 与 WAC），以精准统计 CXL DRAM 中 4KB 页面和 64B 字的访问次数；接着通过 PAC 与 WAC 揭示了 ANB、DAMON 等 CPU 驱动方案误判温页面、盲目迁移稀疏页面及性能开销显著的缺陷；最后设计并实现 M5 平台，该平台依托 CXL 控制器中的硬件热页面 / 热字跟踪器（HPT/HWT）及软件 M5-manager，能低成本、高精度识别热页面与区分页面稀疏性，实验表明 M5 平均比最优 CPU 驱动方案（DAMON）多识别 47% 热页面且提升 14% 性能，为 CXL 分层内存系统的高效管理提供实用解决方案

### 一、研究背景与挑战

1. **技术背景**：数据中心应用对 DRAM 容量和带宽需求持续增长，但传统 DDR 接口已接近缩放极限。CXL 作为基于 PCIe 的新型内存接口，能以更少引脚提供与 DDR 相当的带宽，可低成本扩展内存容量，但 CXL DRAM 访问延迟比 DDR 高 2-3 倍，形成 “DDR（快内存）+ CXL DRAM（慢内存）” 的分层内存系统。
2. **核心挑战**：需高效的页面迁移方案，将频繁访问的 “热页面” 从 CXL DRAM 迁移到 DDR，以降低性能损失。但现有 CPU 驱动的页面迁移方案存在三大问题：
   - 易将 “温页面” 误判为 “热页面”，识别精度低；
   - 无法区分 “稀疏页面”（仅少量 64B 字频繁访问）和 “密集页面”，迁移稀疏页面会造成缓存污染和内存浪费；
   - 识别热页面的过程消耗大量 CPU 周期，性能开销显著，可能抵消迁移收益。

### 二、核心贡献

#### 1. CXL 驱动的页面与字访问计数方案（PAC 与 WAC）

- 基于 FPGA 的 CXL 设备实现，利用 CXL 控制器的近内存处理能力，精准、透明地统计 CXL DRAM 中每个 4KB 页面（PAC）和 64B 字（WAC）的访问次数。
- 相比动态二进制插桩、采样等传统方法，PAC 和 WAC 无需干扰应用执行，计数精度更高，为评估页面迁移方案提供了黄金标准。

#### 2. 揭示 CPU 驱动页面迁移方案的缺陷

通过 PAC 和 WAC 的实测分析：

- **识别精度低**：代表性方案 ANB（自动 NUMA 平衡）和 DAMON 识别的 “热页面”，其实际访问量仅为 PAC 判定的 Top-K 热页面的 21% 和 29%，本质是误判温页面；
- **稀疏页面迁移问题**：Redis 等应用中 86% 的页面仅 25% 以下的字被访问，但 CPU 驱动方案无法区分，盲目迁移导致缓存污染；
- **性能开销大**：ANB 和 DAMON 分别使内核 CPU 周期增加 159% 和 277%，导致 Redis 等延迟敏感应用的 p99 延迟上升 34%-39%，部分应用执行时间延长超 8%。

#### 3. M5 平台设计与实现

M5 是支持 CXL 驱动页面迁移方案开发的硬件 - 软件协同平台，核心目标是解决 CPU 驱动方案的缺陷，包含两大组件：

- **硬件组件**：热页面跟踪器（HPT）和热字跟踪器（HWT）
  - 基于 Count-Min Sketch（CM-Sketch）算法，低成本跟踪 Top-K 热页面和热字，避免 CPU 驱动方案的高开销；
  - 运行于 CXL 控制器，无需修改 CPU 架构，支持 400MHz 以上速率，满足内存访问实时性要求。
- **软件组件**：M5-manager
  - 包含 Monitor（监控内存带宽密度等指标）、Nominator（结合 HPT/HWT 识别密集热页面）、Elector（动态调整迁移频率和策略）、Promoter（与 Linux 内核交互执行页面迁移）；
  - 提供灵活接口，支持用户自定义迁移策略，并给出 4 条核心优化准则（如根据带宽密度决定迁移优先级、区分稀疏 / 密集页面迁移等）。

### 三、实验验证与结果

1. **实验环境**：基于 Intel 第 4 代 Xeon 处理器的双路服务器，搭配 Intel Agilex-7 FPGA（CXL 设备），测试 12 个内存密集型基准测试（含 SPEC CPU 2017、Redis、图计算等）。
2. **关键结果**：
   - **识别精度**：M5 的 CM-Sketch-based HPT 识别热页面的访问量占比达 0.72，比 ANB/DAMON（平均 0.49）高 47%；
   - **性能提升**：M5 平均比 DAMON（现有最优 CPU 驱动方案）提升 14% 性能，比 ANB 提升 20%；对 Redis 等延迟敏感应用，性能提升达 43%；
   - **开销优势**：M5 识别热页面的 CPU 开销可忽略，避免了 ANB/DAMON 的内核资源占用问题。

### 四、研究结论

M5 通过 CXL 控制器的硬件辅助跟踪与灵活的软件策略，解决了传统 CPU 驱动页面迁移方案的精度低、开销大、无法区分稀疏页面等问题，为 CXL 分层内存系统提供了高效、实用的内存管理解决方案，且兼容性强，可与现有 Linux 内核功能（如 MGLRU）协同工作，具备工业应用潜力。



## [<span id="asplos2504-1">MoE-Lightning: High-Throughput MoE Inference on  Memory-constrained GPUs</span>](#asplos2504)

​	该研究提出高吞吐量混合专家（MoE）模型批处理推理系统 MoE-Lightning，旨在解决内存受限 GPU 上 MoE 模型部署的资源利用率低、吞吐量不足问题，核心创新包括 CGOPipe 调度策略（通过权重分页和 CPU-GPU-I/O 任务重叠提升资源利用率）与 HRM 分层性能模型（精准定位瓶颈并搜索最优超参数），同时支持张量并行和动态批处理优化；在单 T4/L4 GPU 及多 T4 GPU 环境下，对 Mixtral 8x7B、Mixtral 8x22B、DBRX 等模型的测试显示，其无请求填充时吞吐量较现有最优系统提升达 10.3 倍，有填充时提升 3.5 倍，多 GPU 场景下实现超线性缩放，且仅需 2-3 倍更少 CPU 内存即可达到 GPU 内存受限下的吞吐量上限，为缺乏高端 GPU 资源的用户高效部署大型 MoE 模型提供了可行方案。

### 一、研究背景与挑战

混合专家（MoE）模型凭借稀疏激活特性，在不显著增加推理运算量的前提下提升模型容量，相比稠密模型降低了令牌生成延迟，在多项任务中表现出色。但 MoE 模型参数规模庞大，对内存需求极高（如 Mixtral 8x22B 的专家前馈网络参数需超 256GB 内存），远超普通 GPU 的存储能力，导致缺乏高端 GPU 资源的用户难以部署。

在内存受限场景下，现有解决方案通常采用模型权重和键值缓存（KV cache）卸载至 CPU 或磁盘、逐层加载到 GPU 计算的方式，但存在显著缺陷：CPU 与 GPU 间的数据传输与计算无法有效重叠，导致 GPU 闲置、资源利用率低下；且未充分考虑工作负载变化对瓶颈资源的影响，难以找到最优调度策略。

### 二、核心创新

#### 1. 调度策略：CGOPipe（CPU-GPU-I/O 流水线调度）

- 采用权重分页机制，将权重划分为与微批次数相等的页面，交错传输不同任务的数据，减少 I/O 流水线气泡。
- 高效重叠 GPU 计算、CPU 计算及各类 I/O 操作（包括中间结果传输、权重传输、KV 缓存传输），避免计算被 I/O 阻塞、不同 I/O 操作相互阻塞，大幅提升资源利用率。
- 针对解码阶段设计细粒度调度，GPU 依次处理当前微批的后注意力任务和下一个微批的前注意力任务，CPU 并行处理下一批的注意力计算，同时预取下一层的权重页面。

#### 2. 性能模型：HRM（分层 Roofline 模型）

- 基于经典 Roofline 模型扩展，适配异构计算设备（CPU、GPU）和多层内存架构（GPU HBM、CPU DRAM 等），考虑跨层级内存带宽和不同处理器的计算能力。
- 引入多个转折点和平衡点，明确不同操作条件下的性能瓶颈（如 CPU-GPU 传输瓶颈、GPU 内存瓶颈、CPU 内存瓶颈等），无需大量数据拟合（区别于 FlexGen），部署开销接近零。
- 支持不同模型、硬件和工作负载，为调度策略搜索提供理论支撑，帮助确定最优超参数（批大小、微批大小、设备分配、权重静态存储比例等）。

#### 3. 其他优化

- 支持张量并行（Tensor Parallelism），在单节点多 GPU 场景下扩展 GPU 内存容量和带宽，实现超线性吞吐量扩展。
- 采用动态请求批处理算法，按输入长度降序排序请求，将最长请求分配至令牌数最少的微批，保证微批大小接近最优值，支持变长请求处理。
- 优化内存管理，通过双缓冲机制实现权重预取与计算重叠，权重先从 CPU 内存转移至固定内存再到 GPU，隐藏传输延迟。

### 三、实验结果

#### 1. 实验配置

- 模型：Mixtral 8x7B、Mixtral 8x22B、DBRX（132B 参数，16 个专家）。
- 硬件：单 T4（16GB）、单 L4（24GB）、2xT4、4xT4 GPU，搭配不同配置的 Intel Xeon CPU。
- 工作负载：MTBench（多轮问答，输出长度 32-256）、HELM 基准（合成推理、摘要生成，长提示长度）。
- 基线系统：FlexGen（含 CPU 注意力变体）、DeepSpeed Zero-Inference。

#### 2. 关键性能表现

- 单 GPU 场景：在 Mixtral 8x7B 上，无请求填充时吞吐量较最先进系统提升达 10.3 倍，有请求填充时提升 3.5 倍；相比 FlexGen、FlexGen (c)、DeepSpeed，最高分别实现 3.5 倍、5 倍、6.7 倍提升。
- 多 GPU 场景：启用张量并行后，4xT4 GPU 运行 Mixtral 8x22B 时，吞吐量较 2xT4 提升 2.77-3.38 倍，实现超线性缩放；DBRX 模型在 2xT4 与 4xT4 间切换时，吞吐量提升 2.1-2.8 倍。
- 资源效率：在 GPU 内存受限时，仅需 2-3 倍更少的 CPU 内存即可达到吞吐量上限；处理长生成长度、长提示长度任务时，避免吞吐量下降（区别于基线系统），始终保持高资源利用率。

#### 3. 消融实验验证

- 策略优化：将 HRM 生成的策略应用于 FlexGen，吞吐量提升 1.77 倍，增大批大小后提升 2.17 倍，但仍低于 MoE-Lightning（受 KV 缓存交换瓶颈限制）。
- CPU 注意力优势：CPU 注意力内核比 KV 缓存传输快 3-4 倍，在大批次、长上下文场景下，CPU 注意力逐渐成为瓶颈，需提升 CPU 内存带宽。
- 硬件适配性：随着 CPU-GPU 带宽增加，更多权重可卸载至 CPU；KV 缓存卸载与 CPU 性能强相关，CPU 内存带宽较低时，即使高传输带宽也不适合卸载 KV 缓存。

### 四、适用场景与局限

#### 适用场景

- 离线批处理工作负载：模型评估、合成数据生成、数据整理、表单处理、关系型分析等追求高吞吐量的场景。
- 内存受限环境：单低 - cost GPU（如 T4、L4）或多低 - cost GPU 部署大型 MoE 模型（如 Mixtral 8x22B、DBRX）。

#### 局限

- 暂不支持磁盘卸载，仅适用于 CPU 内存足够存储模型的场景。
- 性能模型 HRM 目前局限于单节点硬件，未考虑 GPU 间通信和多节点通信。
- 长上下文场景下，CPU 注意力可能成为瓶颈，需结合 KV 缓存稀疏化等进一步优化。

### 五、结论

MoE-Lightning 通过 CGOPipe 调度策略和 HRM 性能模型，解决了内存受限 GPU 上 MoE 模型推理的吞吐量与资源利用率问题。在单 GPU 上实现最高 10.3 倍的吞吐量提升，多 GPU 场景下展现超线性缩放能力，且仅需更少 CPU 内存即可达到性能上限，为缺乏高端 GPU 资源的用户部署大型 MoE 模型提供了高效解决方案。未来可扩展至磁盘卸载、多节点通信支持，并融入 KV 缓存稀疏化等优化，进一步提升长上下文推理效率。



## [<span id ="asplos2505-1">CXLfork: Fast Remote Fork over CXL Fabrics</span>](#asplos2505)

​	该文章提出了一种基于 CXL（Compute Express Link）共享内存架构的远程进程克隆接口 CXLfork，通过近零序列化、零复制的设计，将进程私有状态以原生内存复制方式直接存储于 CXL 内存，全局状态仅序列化必要信息，结合写时复制（CoW）机制实现跨节点状态共享与内存去重，并提供写时迁移、访问时迁移、混合分层三种精细的状态分层策略平衡性能与内存开销；同时基于 CXLfork 构建了无服务器函数自动扩缩容工具 CXLporter，通过智能 checkpoint 管理、幽灵容器池、动态分层策略调整等功能优化冷启动性能与资源利用率。实验表明，CXLfork 相比现有方案（CRIU、Mitosis）平均性能提升 2.26 倍和 1.40 倍，本地内存消耗降低 87% 和 61%，CXLporter 能显著减少无服务器函数的尾延迟并提升集群并发能力，为云原生无服务器计算提供了高效的跨节点进程克隆解决方案。

### 一、研究背景与动机

#### 1. 技术基础

Compute Express Link（CXL）作为新兴的缓存一致性互连技术，支持字节可寻址的低延迟远程内存访问，其 3.0 及以上版本更实现了跨计算节点的机架级缓存一致性内存共享。这一特性为分布式系统的传统软件接口革新提供了可能，尤其适用于高性能云原生无服务器计算中的集群级进程克隆场景。

#### 2. 现有方案局限

现有远程进程克隆机制难以充分利用 CXL 共享内存优势：

- **CRIU**：通过序列化进程状态到文件实现跨节点迁移，存在大量序列化 / 反序列化开销，且父进程与子进程无状态共享，内存消耗极高。
- **Mitosis**：基于 RDMA 实现惰性数据复制，避免了进程内存的序列化，但仍需序列化操作系统管理状态，且数据复制带来显著延迟，无法发挥 CXL 的低延迟共享特性。

#### 3. 无服务器计算的核心需求

无服务器函数（FaaS）频繁创建和销毁实例，冷启动开销（初始化状态、容器创建等）成为性能瓶颈。这类函数的内存足迹中，初始化数据（72.2%）和只读数据（23%）占比极高，具备跨节点共享的潜力，为 CXL 共享内存的应用提供了天然场景。

### 二、核心方案：CXLfork 设计

CXLfork 是专为 CXL 架构设计的远程进程克隆接口，实现近零序列化、零复制的集群级进程克隆，核心目标是平衡性能、内存共享与数据分层。

#### 1. 核心设计理念

利用 CXL 全局共享内存特性，将进程状态直接存储于 CXL 内存而非本地文件或节点内存，实现跨节点状态共享与内存去重，同时通过精细的状态分层策略优化访问延迟。

#### 2. 关键技术突破

#### （1）进程状态的非序列化 checkpoint

- 区分**私有状态**（进程任务结构、内存描述符、页表、寄存器等）和**全局状态**（打开文件、套接字、命名空间等）。
- 私有状态通过原生内存复制直接写入 CXL 内存，无需序列化；页表项被修改为指向 CXL 物理地址并标记为只读，同时保留访问（A）和脏页（D）位以追踪访问模式。
- 全局状态仅序列化必要信息（如文件路径、权限），避免完整序列化带来的开销，恢复时通过重新执行系统调用重建。

#### （2）高效状态共享与零复制恢复

- 恢复时直接将 CXL 中的 checkpoint 状态映射到目标进程地址空间，采用写时复制（CoW）机制处理修改：只读状态常驻 CXL 内存供所有克隆进程共享，写操作触发缺页中断将数据迁移至本地内存。
- 优化页表和虚拟内存区域（VMA）树的恢复：仅在本地内存初始化页表树的上层结构，直接挂载 CXL 中的页表叶子节点，实现近常数时间的状态恢复。

#### （3）精细的状态分层策略

支持三种数据分层策略，平衡内存消耗与执行性能：

- **写时迁移（MoW）**：默认策略，仅在写操作时将 CXL 中的页面迁移至本地，最大化内存共享。
- **访问时迁移（MoA）**：首次访问时将页面迁移至本地，降低后续访问延迟，但增加内存消耗。
- **混合分层（HT）**：基于页表的访问位（A 位）识别热点页面，仅将高频访问页面迁移至本地，兼顾内存效率与性能。

### 三、衍生应用：CXLporter 无服务器自动扩缩容器

基于 CXLfork 构建 CXLporter，专为 CXL 互连集群中的无服务器函数设计，优化冷启动性能与资源利用率。

#### 1. 核心功能

- **智能 checkpoint 管理**：函数执行 16 次后（确保 JIT 编译达到稳定状态）创建 checkpoint，存储于 CXL 分布式对象存储中，并定期清理访问位以更新热点页面识别。
- **幽灵容器池**：维护预初始化的空容器（仅占用 512KB 内存），避免容器创建开销（约 130ms），通过 CXLfork 快速克隆函数状态至容器。
- **动态策略调整**：根据函数性能指标（延迟是否满足 SLO）和节点内存压力，动态切换 CXLfork 分层策略，并调整函数保活窗口（最低至 10 秒）以回收内存。

### 四、实验评估

#### 1. 实验环境

- 硬件：双路 Intel Sapphire Rapids 服务器（每路 64 核、128GB DDR5 内存），Intel Agilex 7 FPGA-based CXL 内存设备（16GB DDR4，访问延迟 391ns）。
- 软件：Linux 6.6 内核，QEMU/KVM 模拟双节点集群，基于 OpenWhisk 的无服务器平台。
- 工作负载：10 个典型无服务器函数（包括 Float、BERT、CNN 等，内存足迹 24-630MB），模拟 Azure 真实负载（150 RPS）。

#### 2. 核心性能结果

#### （1）CXLfork 性能优势

- 恢复延迟：仅 1.2-6.1ms，平均比 CRIU 快 2.26 倍，比 Mitosis 快 1.40 倍，接近本地 fork 性能（仅慢 14%）。
- 内存消耗：平均比 CRIU 减少 87%，比 Mitosis 减少 61%，仅为冷启动函数内存消耗的 13%。
- 分层策略效果：混合分层（HT）在 BERT、BFS 等大内存函数中，实现冷启动延迟与内存消耗的平衡，比写时迁移（MoW）降低 11% 的热执行延迟。

#### （2）CXLporter 扩缩容效果

- 充足内存场景：P99 延迟比 CRIU 降低 70%，比 Mitosis 降低 51%，幽灵容器池消除了容器创建开销。
- 内存受限场景（25% 内存）：CXLfork 的 P99 延迟比 CRIU 和 Mitosis 低 16 倍，吞吐量提升 2 倍，凸显内存去重优势。

#### （3）CXL 延迟敏感性

CXL 内存延迟降低至 200ns 时，BERT、BFS 等大工作集函数的热执行性能接近本地内存；即使延迟为 400ns，冷启动性能仍优于传统方案。

### 五、结论与贡献

#### 1. 核心贡献

- 提出首个基于 CXL 共享内存的远程 fork 接口，实现近零序列化、零复制的集群级进程克隆。
- 设计精细的状态分层策略，平衡内存去重、内存节省与运行时性能。
- 构建 CXLporter 自动扩缩容器，显著降低无服务器函数冷启动延迟与内存消耗。

#### 2. 应用价值

CXLfork 与 CXLporter 充分发挥了 CXL 架构的共享内存优势，为无服务器计算提供了高性能、低内存消耗的跨节点进程克隆方案，尤其适用于负载突发场景，可提升集群吞吐量与资源利用率。未来可进一步优化多节点集群中的 CXL 带宽调度，并扩展至复杂无服务器工作流的跨函数通信优化。



## [<span id ="asplos2506-1">PIM Is All You Need: A CXL-Enabled GPU-Free System  for Large Language Model Inference</span>](#asplos2506)

​	该文档提出了一种名为 CENT 的无 GPU 系统，专为大型语言模型（LLM）推理设计，其核心是通过 CXL 3.0 协议实现内存扩展以满足 LLM 的大容量需求，采用分层 PIM-PNM 架构（PIM 负责 99% 以上的 MAC 运算，PNM 处理特殊复杂操作），支持流水线并行、张量并行及混合并行策略，相比 4 块 NVIDIA A100 GPU，在相似功耗下实现 2.3 倍更高吞吐量、2.3 倍更低能耗，每美元生成令牌数提升 5.2 倍，且在长上下文场景（如 32K 令牌）中表现更优，同时硬件成本和总拥有成本（TCO）显著降低，为 LLM 推理提供了高效、经济的替代方案。

### 核心背景与痛点

- LLM 推理存在两大核心需求：一是模型参数和 KV 缓存需要超大内存容量，二是自回归生成导致运算强度低，对内存带宽要求极高。
- 现有 GPU/TPU 主要优化算力吞吐量，在内存受限的 LLM 推理中算力利用率仅 21% 左右，且成本高昂（如 ChatGPT 每日推理成本约 69 万美元）。
- 传统 PIM（内存内处理）带宽高但内存密度低，PNM（近内存处理）密度优但带宽不足，均难以单独满足 LLM 需求。

### CENT 系统核心设计

- **硬件架构**：基于 CXL 3.0 协议构建可扩展网络，32 个 CXL 设备通过 CXL 交换机互联，每个设备包含 16 个 GDDR6-PIM 芯片和 PNM 单元，总内存 512GB，内部峰值带宽达 512TB/s。
- **分层计算设计**：PIM 芯片负责 99% 以上的 MAC 运算（如 GEMV），PNM 单元包含加速器和 RISC-V 核心，处理 Softmax、平方根等特殊操作，无需 GPU 参与。
- **通信与并行策略**：支持点对点和集合通信原语（广播、汇聚等），适配三种并行映射：流水线并行（PP）提升吞吐量、张量并行（TP）降低延迟、混合 TP-PP 平衡二者。

### 关键性能与成本优势

- 与 4 块 A100 GPU 相比，在相同功耗下，CENT 吞吐量提升 2.3 倍，能耗降低 2.3 倍，每美元生成令牌数提升 5.2 倍。
- 长上下文场景优势显著，32K 上下文时解码吞吐量提速 3.3 倍，且查询延迟远低于 GPU（3.4-7.6 倍）。
- 总成本（TCO）大幅降低，3 年租赁成本仅为 GPU 系统的 19%，硬件成本约 1.49 万美元，仅为 GPU 方案（4.21 万美元）的 35%。

### 对比与扩展性

- 相较于 CXL-PNM 系统，CENT 吞吐量提升 4.5 倍；相较于 GPU-PIM 混合系统（AttAcc/NeuPIM），每美元令牌数提升 1.8-5.3 倍。
- 支持灵活扩展，设备数量从 16 增至 128 时，吞吐量可从 0.68K 令牌 / 秒提升至 5.7K 令牌 / 秒，适配 7B 到 70B 及更大参数模型（如 Grok 314B）。

### 适用场景与价值

- 适用于长上下文推理（如 10 万 - 100 万令牌的视频生成、推理任务）、大规模批量推理场景，可完全替代 GPU 或与 GPU 分工（GPU 负责预填充阶段，CENT 负责解码阶段）。
- 提供开源模拟器和 Python API，支持主流 LLM 的激活函数（GeLU、SiLU）和位置编码，具备较强通用性。





## [<span id ="asplos2507-1">PAPI: Exploiting Dynamic Parallelism  in Large Language Model Decoding with a  Processing-In-Memory-Enabled Computing System</span>](#asplos2507)

​	该文档提出了一种名为 PAPI 的基于内存内处理（PIM）的异构架构，旨在解决大型语言模型（LLM）解码阶段因并行性动态变化导致的性能瓶颈问题。PAPI 针对现有架构静态内核映射无法适配 LLM 解码中全连接（FC）层内核在计算密集型和内存密集型之间动态切换、且单一 PIM 单元难以满足不同内核异构需求的不足，整合了主机 CPU、GPU 张量核心以及两种定制化 PIM 单元（面向 FC 层的高性能 FC-PIM、面向注意力层的大内存容量 Attn-PIM），并通过在线内核特征化与两阶段动态调度机制，实时判断 FC 层内核类型并分配至最优硬件单元，同时让注意力层内核固定运行于 Attn-PIM。实验结果显示，在 LLaMA-65B、GPT-3 66B/175B 等模型上，PAPI 相比最先进的异构加速器（A100+AttAcc）和纯 PIM 加速器（AttAcc-only）分别实现 1.8 倍和 11.1 倍的性能提升，且在创意写作、通用问答等任务中显著优化了能效，为 LLM 推理提供了高效、灵活的解决方案。

### 一、研究背景与问题

1. **LLM 解码的关键地位**：LLM 推理包含预填充（prefill）和解码（decoding）两个阶段，解码阶段占总执行时间的绝大部分（如 GPT-3 175B 模型达 96%），且输出长度越长，解码的性能影响越显著。
2. **现有优化技术的局限**：现有并行优化技术（批处理 batching、推测解码 speculative decoding）虽能提升解码并行性，但导致解码内核（全连接 FC 层、多头注意力层）的特性动态变化 —— 部分内核会在计算密集型（compute-bound）和内存密集型（memory-bound）之间切换。
3. **传统架构的不足**：现有异构架构（如 GPU+PIM）采用静态内核映射策略，无法适应上述动态变化；且现有 PIM 单元仅支持单一计算 / 内存带宽配置，难以满足不同内核的异构需求。

### 二、核心观察与设计目标

1. **关键观察**：解码并行性（请求级 RLP、令牌级 TLP）会因服务等级目标（SLO）、内存容量限制、动态批处理等因素动态变化，进而导致 FC 层内核的算术强度改变，使其在计算密集型和内存密集型之间切换；而注意力层内核始终为内存密集型，但与 FC 层的内存 / 计算需求差异显著。
2. **设计目标**：构建支持动态调度的异构架构，适配 LLM 解码的动态需求，同时提供差异化的计算和内存带宽能力，优化性能与能效。

### 三、PAPI 的核心设计

PAPI 包含三大核心组件，实现动态并行性感知的高效调度：

1. **异构计算架构**：整合三类计算单元 —— 主机 CPU、计算中心型处理器（GPU 张量核心）、内存中心型 PIM 单元（FC-PIM、Attn-PIM），覆盖不同计算 / 内存需求。
2. **混合 PIM 单元**：
   - **FC-PIM**：针对 FC 层内核设计，采用 4 个浮点运算单元（FPU）/ 内存 bank 的高并行配置，平衡计算性能与功耗、面积约束。
   - **Attn-PIM**：针对注意力层内核设计，采用 1 个 FPU/2 个 bank 的低并行配置，侧重内存容量与带宽，通过离散式部署支持大规模 KV 缓存存储。
3. **动态并行性感知调度**：
   - **在线内核特征化**：通过实时计算 RLP×TLP 估算 FC 层内核的算术强度，快速判断其为计算密集型或内存密集型。
   - **两阶段调度**：初始调度基于初始并行性参数分配内核；运行时调度监测 RLP/TLP 变化，动态调整内核到最优硬件单元（计算密集型→GPU，内存密集型→FC-PIM；注意力层固定分配给 Attn-PIM）。

### 四、实验结果

1. **性能提升**：在 LLaMA-65B、GPT-3 66B/175B 模型上，PAPI 相比最先进的异构加速器（A100+AttAcc）提速 1.8 倍，相比纯 PIM 加速器（AttAcc-only）提速 11.1 倍；在长输出任务中（如创意写作），提速效果更显著。
2. **能效优化**：相比 A100+AttAcc，PAPI 在创意写作和通用问答任务中的能效分别提升 3.4 倍和 3.1 倍，通过减少数据迁移和低功耗 PIM 计算降低能耗。
3. **适应性验证**：在不同并行性水平（批大小 4-128、推测长度 1-8）下，PAPI 均保持最优性能，验证了其对动态并行性的适配能力。

### 五、创新贡献

1. 首次揭示 LLM 解码并行性的动态变化特性，以及其对内核需求的影响。
2. 提出混合 PIM 架构与动态调度框架，实现内核与硬件单元的动态最优匹配。
3. 验证了 PAPI 在不同模型、任务和并行配置下的性能与能效优势，为 LLM 推理的动态优化提供了新范式。



## [<span id ="asplos2508-1">POD-Attention: Unlocking Full Prefill-Decode  Overlap for Faster LLM Inference</span>](#asplos2508)

​	POD-Attention 是首个专为混合批处理 LLM 推理设计的 GPU 注意力内核，通过 SM 感知的 CTA 调度及瓦片大小优化、虚拟解码 CTA 等技术，实现预填充与解码注意力计算的并发执行，同时充分利用 GPU 计算资源和内存带宽，相较于 FlashAttention 等独立优化内核，其注意力计算最高提速 59%（平均 28%），集成到 Sarathi-Serve 后，可使 LLM 推理吞吐量最高提升 22%，并显著降低 TTFT、TBT 及请求执行延迟，在长上下文场景中表现突出，还能减少最高 35% 的能量消耗。

### 核心背景与问题

LLM 推理包含计算密集型的预填充（prefill）和内存带宽密集型的解码（decode）两个阶段。现有系统采用混合批处理（hybrid batching）将不同请求的两阶段任务合并到同一批次，优化了线性运算，但注意力计算仍存在效率瓶颈：现有注意力内核（如 FlashAttention、FlashInfer）分别针对两阶段独立优化，导致 GPU 计算资源和内存带宽无法同时充分利用（例如预填充阶段内存带宽利用率常低于 5%，解码阶段计算利用率不足 10%），资源浪费严重。

### 核心方案：POD-Attention

POD-Attention 是首个专为混合批处理设计的 GPU 注意力内核，通过让预填充和解码注意力计算在同一流多处理器（SM）上并发执行，同时最大化计算和内存带宽利用率。其核心设计包括：

#### 1. 关键技术创新

- **SM 感知的 CTA 调度**：通过运行时操作绑定，让协作线程数组（CTA）在分配到 SM 后动态决定执行预填充或解码任务，保证两阶段任务在 SM 级共置，避免资源闲置。支持 50:50 交替调度和按任务比例调度两种策略，后者在高负载下性能更优。
- **多维度性能优化**：
  - 适配性瓦片大小：为解码阶段使用最小 16 的 QSL 瓦片长度，减少冗余计算，释放张量核心供预填充使用；
  - 动态 CTA 配置：预填充主导场景采用 2 个 CTA/SM（保障共享内存使用），其他场景采用 4 个 CTA/SM（优化细粒度调度）；
  - 虚拟解码 CTA：将解码 CTA 拆分为 warp 级虚拟 CTA，平衡预填充与解码的共享内存需求；
  - 限制预填充拆分：避免 KV 维度过度拆分导致的带宽竞争，最多允许两波完整拆分。
- **CTA 并行融合实现**：将预填充和解码内核转换为可调用设备函数，通过包装器内核灵活映射 CTA ID，统一管理共享内存，用 warp 级屏障替代解码阶段的 CTA 级屏障，确保融合兼容性。

#### 2. 实验配置

- **模型与硬件**：基于 Yi-6B（1 块 A100）、Llama-2-7B、Llama-3-8B（2 块 A100），均含 32 个查询头，KV 头数量分别为 4、32、8；
- **基准系统**：对比 vLLM（预填充优先调度）、Sarathi-Serve（混合批处理调度），均使用 FlashAttention 内核；
- **评估场景**：离线推理（吞吐量）、在线推理（TTFT、TBT、请求延迟等），测试上下文长度 4K-32K，预填充与解码令牌比（P:D）0-50。

### 核心实验结果

#### 1. 注意力计算性能

- 相较于 FlashAttention、FlashInfer 等独立优化内核，POD-Attention 最高提速 59%，平均提速 28%，25% 的场景接近理论峰值性能，且从未低于串行执行效率；
- 能量消耗降低最高 35%（平均 20.5%），与运行时缩短比例一致。

#### 2. 离线推理吞吐量

- 相较于 Sarathi-Serve，Yi-6B、Llama-2-7B、Llama-3-8B 的吞吐量分别提升 22%、20%、19%；
- 相较于 vLLM，吞吐量分别提升 27%、13%、12%，突破混合批处理中预填充拆分与线性运算优化的权衡限制。

#### 3. 在线推理延迟

- **TTFT（首令牌时间）**：高负载下，Sarathi+POD 的中位数 TTFT 较 Sarathi 降低至 7.5 秒（内部工作负载）和 11.74 秒（arXiv 工作负载），P99 TTFT 最高降低 4.3 倍；
- **TBT（令牌间隔时间）**：P99 TBT 较 vLLM 降低 10 倍（0.14 秒 vs 1.76 秒），较 Sarathi 进一步降低 10%-20%，几乎消除生成停滞（<5% 请求出现停滞，vLLM 达 97%+）；
- **请求延迟**：P99 请求延迟较 vLLM 最高降低 42%（内部工作负载）和 17%（arXiv 工作负载），平衡吞吐量与延迟权衡。

#### 4. 敏感性分析

- 长上下文（预填充主导）场景下，2 个 CTA/SM 更优；短上下文（解码主导）场景下，4 个 CTA/SM 更优；
- P:D 比例 12-18 时性能增益峰值（混合批处理占比最高），纯预填充或纯解码场景增益有限；
- 比例调度策略较 50:50 调度最高提速 14%，尤其适合高负载场景。

### 相关工作对比

- 现有注意力优化（FlashAttention-3、FlashDecoding++）：均独立优化预填充或解码，未支持两阶段融合；
- 内核融合技术（HFuse、ISPA）：存在拖尾线程、SM 级共置无保障等问题，POD-Attention 通过 SM 感知调度和 CTA 并行融合解决；
- LLM 推理优化（NanoFlow、DistServe）：NanoFlow 适用于小上下文，DistServe 依赖跨节点拆分，POD-Attention 聚焦单设备长上下文混合批处理场景。

### 结论

POD-Attention 通过预填充与解码注意力的并发融合，首次实现 GPU 计算与内存带宽的同时高效利用，为混合批处理 LLM 推理提供了专用注意力内核解决方案。其在吞吐量（最高 + 22%）、延迟（TTFT、TBT、请求延迟显著降低）和能效上的全面提升，对长上下文 LLM 部署具有重要实践价值。未来可扩展至 FlashAttention-3 和 NVIDIA Hopper 架构支持。

