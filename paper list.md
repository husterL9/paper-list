[<span id ="asplos">ASPLOS</span>](#asplos-title)

[<span id ="hpca">HPCA</span>](#hpca-title)

MLsys

SOSP

OSDI

#  [<span id ="asplos-title"> ASPLOS</span>](#asplos)

## 2025

<span id ="asplos2501">[vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention](#asplos2501-1)</span>

<span id ="asplos2502">[Accelerating LLM Serving for Multi-turn Dialogues with Efficient Resource Management](#asplos2502-1)</span>

<span id ="asplos2503">[M5: Mastering Page Migration and Memory  Management for CXL-based Tiered Memory Systems](#asplos2503-1)</span>

<span id ="asplos2504">[MoE-Lightning: High-Throughput MoE Inference on  Memory-constrained GPUs](#asplos2504-1)</span>

[<span id ="asplos2505">CXLfork: Fast Remote Fork over CXL Fabrics</span>](#asplos2505-1)

[<span id ="asplos2506">PIM Is All You Need: A CXL-Enabled GPU-Free System  for Large Language Model Inference</span>](#asplos2506-1)

[<span id ="asplos2507">PAPI: Exploiting Dynamic Parallelism  in Large Language Model Decoding with a  Processing-In-Memory-Enabled Computing System</span>](#asplos2507-1)

[<span id ="asplos2508">POD-Attention: Unlocking Full Prefill-Decode  Overlap for Faster LLM Inference</span>](#asplos2508-1)

## [<span id ="asplos2501-1">vAttention: Dynamic Memory Management for  Serving LLMs without PagedAttention</span>](#asplos2501)

        本文提出针对 LLM 服务的动态内存管理方案 vAttention，旨在解决 PagedAttention 需重写内核、开销高、可移植性差的问题。其核心是通过 CUDA VMM API 解耦虚拟与物理内存分配，提前预留连续虚拟内存缓冲区以保留 KV 缓存虚拟连续性，同时按需映射物理内存缓解碎片；并通过内存分配与计算重叠、延迟回收、小页面支持等优化，降低运行时延迟。实验表明，在 Yi-6B 等模型上，vAttention 预填充阶段比 FlashAttention-2/FlashInfer 的 Paged 版本快 1.17-1.36 倍，端到端离线吞吐量提升 1.13-1.23 倍，还能无缝支持 FlashAttention-3 等新内核，兼顾性能、简洁性与可移植性。

### 一、研究背景与现有方案痛点

LLM 服务中，KV 缓存占 GPU 推理内存的主要部分，其动态增长特性（单请求逐 token 扩展、总长度未知）导致内存分配难题：

1. **静态分配**（如 Orca、FasterTransformer）按模型最大上下文长度预留内存，引发严重内部碎片，限制批处理大小和吞吐量；
2. **PagedAttention 方案**（vLLM 提出，被 TensorRT-LLM 等广泛采用）通过按需分配小内存块缓解碎片，但存在根本缺陷：
   - 需重写注意力内核以支持非连续 KV 缓存访问，难以跟进最新优化（如 vLLM 的 Paged 内核比 FlashAttention-2 慢 2.8 倍）；
   - 服务框架需额外实现内存管理，重复 OS 的虚实地址转换功能，增加冗余；
   - 运行时开销显著：GPU 侧因块表查询、寄存器溢出等慢 37%（FlashAttention-2 预填充阶段）至 42%（FlashInfer 预填充阶段），CPU 侧因块表准备等增加 10%-30% 延迟；
   - 可移植性差，新内核（如 FlashAttention-3）发布时无 PagedAttention 支持。

### 二、vAttention 核心设计

核心思路：**保留 KV 缓存的虚拟内存连续性，同时动态分配物理内存**，通过 CUDA 虚拟内存管理（VMM）API 解耦虚实内存分配，避免 PagedAttention 的非连续布局缺陷。

1. **内存分配策略**
   
   - 虚拟内存：提前预留超大连续缓冲区（按最大批处理大小和模型最大上下文长度配置），利用 64 位系统充足的虚拟地址空间（单进程可达 128TB），无需担心虚拟内存碎片；
   - 物理内存：运行时按需分配，仅在请求需要时将物理页面映射到虚拟缓冲区，避免提前占用。

2. **关键技术细节**
   
   - 支持多粒度页面：修改开源 CUDA 统一内存驱动，新增 64KB/128KB/256KB 小页面支持（默认 CUDA VMM 仅支持 2MB 大页面），降低物理内存碎片；
   - 请求级 KV 缓存索引：通过唯一 reqId 定位批处理中每个请求的 KV 缓存子张量，确保地址访问连续性；
   - 兼容现有框架：作为 Python 库集成到 vLLM 等服务框架，提供 init/alloc_reqid/free_reqid/step 等简洁 API，无需修改模型或注意力内核。

3. **针对性优化**
   
   - 隐藏分配延迟：利用解码阶段内存需求的可预测性，通过后台线程将内存分配与计算重叠；预填充阶段采用延迟回收 + 预分配策略，复用已释放的物理页面；
   - 缓解碎片：小页面支持使分配粒度匹配 KV 缓存增长特性（单 token 仅需数十 KB），且无 TLB 抖动风险；
   - 支持连续批处理：借助 FlashAttention 的 cache_batch_idx API，处理请求退出后的虚拟内存 "空洞"，保持批处理灵活性。

### 三、实验结果

基于 Yi-6B/Llama-3-8B/Yi-34B 模型，在 A100（单卡 / 双卡 NVLink）和 H100 GPU 上的测试表明：

1. **性能优势**
   
   - 预填充阶段：长上下文（192K）下，vAttention 比 FlashAttention-2 的 Paged 版本快 1.24-1.26 倍，比 FlashInfer 的 Paged 版本快 1.17-1.36 倍；
   - 解码阶段：与 FlashAttention-2 的 Paged 版本性能相当，比 vLLM 的 Paged 内核快 1.53-1.99 倍，比 FlashInfer 的 Paged 版本快 1.23 倍；
   - 端到端吞吐量：离线长上下文任务（arXiv 摘要）中，比 FlashAttention-2 Paged 快 1.13-1.18 倍，比 FlashInfer Paged 快 1.14-1.23 倍；在线场景下中位数延迟降低 28%-42%。

2. **可移植性**
   
   - 无需修改代码即可支持 FlashAttention-3（Hopper 架构优化内核），在 H100 上比 FlashAttention-2 Paged 版本吞吐量提升 1.26-1.5 倍。

3. **资源效率**
   
   - 小页面（64KB）使最大批处理大小提升 1.18-1.28 倍；内存分配带宽达 7.6GB/s，远超 LLM 推理需求（750MB/s）。

### 四、核心贡献

1. 提出虚实内存解耦的 KV 缓存管理方案，兼顾连续性（无内核修改）和动态性（无碎片）；
2. 解决 CUDA VMM 的延迟和大页面碎片问题，提供 LLM 专用优化；
3. 实现简单、可移植、高性能的替代方案，支持现有主流注意力内核（FlashAttention-2/3、FlashInfer），降低 LLM 服务的部署和维护成本。
   
    

## [<span id ="asplos2502-1">Accelerating LLM Serving for Multi-turn Dialogues  with Efficient Resource Management</span>](#asplos2502)

        本文针对现有 LLM 服务框架在处理多轮对话时存在的历史注意力键值对（KVs）重计算开销大、FCFS 调度导致 GPU 内存利用率低（头阻塞）两大问题，提出了名为 FlashGen 的解决方案：通过设计包含 GPU、CPU 内存和 SSD 的多级 KV 缓存（FlashGen-Cache），动态选择缓存恢复与重计算以减少冗余计算，同时采用请求重排序调度（FlashGen-Sched），在优先调度可运行短请求提升内存利用率的同时，通过抢占机制避免长请求饥饿；基于 Azure 实例（双 A100 GPU 等配置），在 OPT、Llama-2 系列模型及 ShareGPT 等数据集上的实验表明，FlashGen 在相似延迟下，对 OPT 30B 和 Llama-2 70B 的吞吐量分别提升 1.63 倍和 2.85 倍，显著优化了多轮对话场景下的 LLM 服务性能

### 一、研究背景与核心问题

1. **多轮对话的 LLM 服务挑战**：随着 LLM 在聊天机器人等场景的广泛应用，长上下文（如多轮对话）处理需求激增，但现有框架（如 vLLM、TensorRT-LLM）存在两大关键效率问题：
   
   - **KV 重计算开销**：多轮对话中，用户查询会包含历史对话内容，导致提示词长度 “放大”，现有框架因 GPU 内存有限无法缓存所有历史注意力键值对（KVs），需重复计算，耗费大量资源。
   - **GPU 内存利用率低**：采用先到先服务（FCFS）调度策略时，长提示词请求会阻塞后续短请求，导致 GPU 内存闲置（头阻塞问题），尤其在高负载下缓存竞争加剧，利用率进一步下降。

2. **数据支撑**：基于 ShareGPT 真实对话数据集的分析显示，对话会话平均包含 7 轮、中位数 3 轮，多轮对话的提示词长度较单轮增长 99 倍，历史对话内容占总输入 tokens 的一半以上，验证了 “提示词放大” 问题的严重性。

### 二、核心解决方案：FlashGen

FlashGen 通过**多级 KV 缓存管理**和**请求重排序调度**两大核心技术，高效利用 GPU、CPU（DRAM）和 SSD 资源，解决上述问题。

#### 1. 多级 KV 缓存（FlashGen-Cache）

- **设计目标**：避免历史 KVs 重复计算，通过多级存储分层缓存，平衡内存成本与访问 latency。
- **缓存层级**：
  - 一级缓存（GPU 内存）：缓存当前运行请求的 KVs 及已完成请求的可回收 KVs，优先命中以减少传输开销。
  - 二级缓存（CPU 内存）：异步复制 GPU 生成的 KVs，GPU 内存不足时可快速恢复，通过流水线技术重叠 KV 传输与模型计算，隐藏延迟。
  - 三级缓存（SSD）：当 CPU 内存不足以存储所有历史 KVs 时，异步归档不常用 KVs；通过 CPU 内存 “预加载” 机制，避免直接从 SSD 读取的高延迟，必要时动态选择 “重计算” 而非 “SSD 读取” 以优化性能。
- **关键优化**：批量感知 KV 恢复、主动缓存策略（生成时即复制到 CPU），减少内存回收与传输开销。

#### 2. 请求重排序调度（FlashGen-Sched）

- **设计目标**：解决头阻塞问题，提升 GPU 内存利用率，同时保证请求公平性。
- **核心策略**：
  - 贪心重排序：当队列头部的长请求因内存不足无法执行时，优先调度后续可放入空闲内存的短请求（“提升请求”），避免内存闲置。
  - 无饥饿机制：实时跟踪 GPU 内存使用，当空闲内存 + 提升请求占用内存足以容纳被阻塞的长请求时，抢占提升请求，优先执行长请求，避免其饥饿。
- **效果**：GPU 内存利用率从 vLLM 的 88% 提升至 98% 以上，批量请求规模平均增加 1.06~1.15 倍。

### 三、实验验证与结果

1. **实验环境**：Azure 实例（2×A100 GPU、440GB CPU 内存、2×960GB NVMe SSD），模型包括 OPT（13B/30B/66B/175B）、Llama-2（13B/70B），数据集涵盖 ShareGPT（多轮对话）、Alpaca、HumanEval。

2. **核心性能指标**：
   
   - 吞吐量：在相似延迟下，FlashGen 对 OPT 30B 和 Llama-2 70B 的吞吐量分别提升 1.63 倍和 2.85 倍。
   - 延迟：P95 首 token 延迟（TTFT）较 vLLM 降低 77%（OPT 30B）和 66%（Llama-2 13B）；单 token 生成延迟（TPOT）的 P99 值从 vLLM 的 608ms 降至 103ms（OPT 30B）。
   - 缓存效果：GPU+CPU+SSD 三级缓存的 KV 命中率显著高于单一层级，高负载下仍能维持稳定命中，减少重计算比例。

3. **对比基准**：优于 vLLM（基线）和 CachedAttention（同类 KV 缓存方案），尤其在高负载、长上下文场景下，因动态调度与多级缓存的协同优化，性能优势更明显。

### 四、相关工作与结论

1. **相关工作对比**：
   
   - KV 复用：CachedAttention 仅支持 CPU/SSD 缓存，未动态选择重计算；SGLang 仅依赖 GPU 缓存，不支持异构存储。
   - 调度优化：现有迭代级调度未解决多轮对话的头阻塞问题；Sarathi-Serve 聚焦长提示词拆分，不涉及请求重排序。
   - 内存优化：PagedAttention 优化内存分配，但未解决历史 KV 缓存与调度协同问题。

2. **研究结论**：
   
   - FlashGen 通过多级 KV 缓存和请求重排序的协同设计，有效解决了多轮对话中 “KV 重计算” 和 “GPU 内存闲置” 两大核心问题。
   - 在长上下文、高负载场景下性能优势显著，为 LLM 多轮服务（如聊天机器人）提供了高效、低成本的解决方案，随着对话轮数增加，优化价值更突出。
   
   

## [<span id ="asplos2503-1">M5: Mastering Page Migration and Memory  Management for CXL-based Tiered Memory Systems</span>](#asplos2503)

​	本文针对 CXL 基于分层内存系统中传统 CPU 驱动页面迁移方案精度低、开销大且无法区分稀疏页面的问题，首先提出基于 FPGA 的 CXL 驱动页面与字访问计数方案（PAC 与 WAC），以精准统计 CXL DRAM 中 4KB 页面和 64B 字的访问次数；接着通过 PAC 与 WAC 揭示了 ANB、DAMON 等 CPU 驱动方案误判温页面、盲目迁移稀疏页面及性能开销显著的缺陷；最后设计并实现 M5 平台，该平台依托 CXL 控制器中的硬件热页面 / 热字跟踪器（HPT/HWT）及软件 M5-manager，能低成本、高精度识别热页面与区分页面稀疏性，实验表明 M5 平均比最优 CPU 驱动方案（DAMON）多识别 47% 热页面且提升 14% 性能，为 CXL 分层内存系统的高效管理提供实用解决方案

### 一、研究背景与挑战

1. **技术背景**：数据中心应用对 DRAM 容量和带宽需求持续增长，但传统 DDR 接口已接近缩放极限。CXL 作为基于 PCIe 的新型内存接口，能以更少引脚提供与 DDR 相当的带宽，可低成本扩展内存容量，但 CXL DRAM 访问延迟比 DDR 高 2-3 倍，形成 “DDR（快内存）+ CXL DRAM（慢内存）” 的分层内存系统。
2. **核心挑战**：需高效的页面迁移方案，将频繁访问的 “热页面” 从 CXL DRAM 迁移到 DDR，以降低性能损失。但现有 CPU 驱动的页面迁移方案存在三大问题：
   - 易将 “温页面” 误判为 “热页面”，识别精度低；
   - 无法区分 “稀疏页面”（仅少量 64B 字频繁访问）和 “密集页面”，迁移稀疏页面会造成缓存污染和内存浪费；
   - 识别热页面的过程消耗大量 CPU 周期，性能开销显著，可能抵消迁移收益。

### 二、核心贡献

#### 1. CXL 驱动的页面与字访问计数方案（PAC 与 WAC）

- 基于 FPGA 的 CXL 设备实现，利用 CXL 控制器的近内存处理能力，精准、透明地统计 CXL DRAM 中每个 4KB 页面（PAC）和 64B 字（WAC）的访问次数。
- 相比动态二进制插桩、采样等传统方法，PAC 和 WAC 无需干扰应用执行，计数精度更高，为评估页面迁移方案提供了黄金标准。

#### 2. 揭示 CPU 驱动页面迁移方案的缺陷

通过 PAC 和 WAC 的实测分析：

- **识别精度低**：代表性方案 ANB（自动 NUMA 平衡）和 DAMON 识别的 “热页面”，其实际访问量仅为 PAC 判定的 Top-K 热页面的 21% 和 29%，本质是误判温页面；
- **稀疏页面迁移问题**：Redis 等应用中 86% 的页面仅 25% 以下的字被访问，但 CPU 驱动方案无法区分，盲目迁移导致缓存污染；
- **性能开销大**：ANB 和 DAMON 分别使内核 CPU 周期增加 159% 和 277%，导致 Redis 等延迟敏感应用的 p99 延迟上升 34%-39%，部分应用执行时间延长超 8%。

#### 3. M5 平台设计与实现

M5 是支持 CXL 驱动页面迁移方案开发的硬件 - 软件协同平台，核心目标是解决 CPU 驱动方案的缺陷，包含两大组件：

- **硬件组件**：热页面跟踪器（HPT）和热字跟踪器（HWT）
  - 基于 Count-Min Sketch（CM-Sketch）算法，低成本跟踪 Top-K 热页面和热字，避免 CPU 驱动方案的高开销；
  - 运行于 CXL 控制器，无需修改 CPU 架构，支持 400MHz 以上速率，满足内存访问实时性要求。
- **软件组件**：M5-manager
  - 包含 Monitor（监控内存带宽密度等指标）、Nominator（结合 HPT/HWT 识别密集热页面）、Elector（动态调整迁移频率和策略）、Promoter（与 Linux 内核交互执行页面迁移）；
  - 提供灵活接口，支持用户自定义迁移策略，并给出 4 条核心优化准则（如根据带宽密度决定迁移优先级、区分稀疏 / 密集页面迁移等）。

### 三、实验验证与结果

1. **实验环境**：基于 Intel 第 4 代 Xeon 处理器的双路服务器，搭配 Intel Agilex-7 FPGA（CXL 设备），测试 12 个内存密集型基准测试（含 SPEC CPU 2017、Redis、图计算等）。
2. **关键结果**：
   - **识别精度**：M5 的 CM-Sketch-based HPT 识别热页面的访问量占比达 0.72，比 ANB/DAMON（平均 0.49）高 47%；
   - **性能提升**：M5 平均比 DAMON（现有最优 CPU 驱动方案）提升 14% 性能，比 ANB 提升 20%；对 Redis 等延迟敏感应用，性能提升达 43%；
   - **开销优势**：M5 识别热页面的 CPU 开销可忽略，避免了 ANB/DAMON 的内核资源占用问题。

### 四、研究结论

M5 通过 CXL 控制器的硬件辅助跟踪与灵活的软件策略，解决了传统 CPU 驱动页面迁移方案的精度低、开销大、无法区分稀疏页面等问题，为 CXL 分层内存系统提供了高效、实用的内存管理解决方案，且兼容性强，可与现有 Linux 内核功能（如 MGLRU）协同工作，具备工业应用潜力。



## [<span id="asplos2504-1">MoE-Lightning: High-Throughput MoE Inference on  Memory-constrained GPUs</span>](#asplos2504)

​	该研究提出高吞吐量混合专家（MoE）模型批处理推理系统 MoE-Lightning，旨在解决内存受限 GPU 上 MoE 模型部署的资源利用率低、吞吐量不足问题，核心创新包括 CGOPipe 调度策略（通过权重分页和 CPU-GPU-I/O 任务重叠提升资源利用率）与 HRM 分层性能模型（精准定位瓶颈并搜索最优超参数），同时支持张量并行和动态批处理优化；在单 T4/L4 GPU 及多 T4 GPU 环境下，对 Mixtral 8x7B、Mixtral 8x22B、DBRX 等模型的测试显示，其无请求填充时吞吐量较现有最优系统提升达 10.3 倍，有填充时提升 3.5 倍，多 GPU 场景下实现超线性缩放，且仅需 2-3 倍更少 CPU 内存即可达到 GPU 内存受限下的吞吐量上限，为缺乏高端 GPU 资源的用户高效部署大型 MoE 模型提供了可行方案。

### 一、研究背景与挑战

混合专家（MoE）模型凭借稀疏激活特性，在不显著增加推理运算量的前提下提升模型容量，相比稠密模型降低了令牌生成延迟，在多项任务中表现出色。但 MoE 模型参数规模庞大，对内存需求极高（如 Mixtral 8x22B 的专家前馈网络参数需超 256GB 内存），远超普通 GPU 的存储能力，导致缺乏高端 GPU 资源的用户难以部署。

在内存受限场景下，现有解决方案通常采用模型权重和键值缓存（KV cache）卸载至 CPU 或磁盘、逐层加载到 GPU 计算的方式，但存在显著缺陷：CPU 与 GPU 间的数据传输与计算无法有效重叠，导致 GPU 闲置、资源利用率低下；且未充分考虑工作负载变化对瓶颈资源的影响，难以找到最优调度策略。

### 二、核心创新

#### 1. 调度策略：CGOPipe（CPU-GPU-I/O 流水线调度）

- 采用权重分页机制，将权重划分为与微批次数相等的页面，交错传输不同任务的数据，减少 I/O 流水线气泡。
- 高效重叠 GPU 计算、CPU 计算及各类 I/O 操作（包括中间结果传输、权重传输、KV 缓存传输），避免计算被 I/O 阻塞、不同 I/O 操作相互阻塞，大幅提升资源利用率。
- 针对解码阶段设计细粒度调度，GPU 依次处理当前微批的后注意力任务和下一个微批的前注意力任务，CPU 并行处理下一批的注意力计算，同时预取下一层的权重页面。

#### 2. 性能模型：HRM（分层 Roofline 模型）

- 基于经典 Roofline 模型扩展，适配异构计算设备（CPU、GPU）和多层内存架构（GPU HBM、CPU DRAM 等），考虑跨层级内存带宽和不同处理器的计算能力。
- 引入多个转折点和平衡点，明确不同操作条件下的性能瓶颈（如 CPU-GPU 传输瓶颈、GPU 内存瓶颈、CPU 内存瓶颈等），无需大量数据拟合（区别于 FlexGen），部署开销接近零。
- 支持不同模型、硬件和工作负载，为调度策略搜索提供理论支撑，帮助确定最优超参数（批大小、微批大小、设备分配、权重静态存储比例等）。

#### 3. 其他优化

- 支持张量并行（Tensor Parallelism），在单节点多 GPU 场景下扩展 GPU 内存容量和带宽，实现超线性吞吐量扩展。
- 采用动态请求批处理算法，按输入长度降序排序请求，将最长请求分配至令牌数最少的微批，保证微批大小接近最优值，支持变长请求处理。
- 优化内存管理，通过双缓冲机制实现权重预取与计算重叠，权重先从 CPU 内存转移至固定内存再到 GPU，隐藏传输延迟。

### 三、实验结果

#### 1. 实验配置

- 模型：Mixtral 8x7B、Mixtral 8x22B、DBRX（132B 参数，16 个专家）。
- 硬件：单 T4（16GB）、单 L4（24GB）、2xT4、4xT4 GPU，搭配不同配置的 Intel Xeon CPU。
- 工作负载：MTBench（多轮问答，输出长度 32-256）、HELM 基准（合成推理、摘要生成，长提示长度）。
- 基线系统：FlexGen（含 CPU 注意力变体）、DeepSpeed Zero-Inference。

#### 2. 关键性能表现

- 单 GPU 场景：在 Mixtral 8x7B 上，无请求填充时吞吐量较最先进系统提升达 10.3 倍，有请求填充时提升 3.5 倍；相比 FlexGen、FlexGen (c)、DeepSpeed，最高分别实现 3.5 倍、5 倍、6.7 倍提升。
- 多 GPU 场景：启用张量并行后，4xT4 GPU 运行 Mixtral 8x22B 时，吞吐量较 2xT4 提升 2.77-3.38 倍，实现超线性缩放；DBRX 模型在 2xT4 与 4xT4 间切换时，吞吐量提升 2.1-2.8 倍。
- 资源效率：在 GPU 内存受限时，仅需 2-3 倍更少的 CPU 内存即可达到吞吐量上限；处理长生成长度、长提示长度任务时，避免吞吐量下降（区别于基线系统），始终保持高资源利用率。

#### 3. 消融实验验证

- 策略优化：将 HRM 生成的策略应用于 FlexGen，吞吐量提升 1.77 倍，增大批大小后提升 2.17 倍，但仍低于 MoE-Lightning（受 KV 缓存交换瓶颈限制）。
- CPU 注意力优势：CPU 注意力内核比 KV 缓存传输快 3-4 倍，在大批次、长上下文场景下，CPU 注意力逐渐成为瓶颈，需提升 CPU 内存带宽。
- 硬件适配性：随着 CPU-GPU 带宽增加，更多权重可卸载至 CPU；KV 缓存卸载与 CPU 性能强相关，CPU 内存带宽较低时，即使高传输带宽也不适合卸载 KV 缓存。

### 四、适用场景与局限

#### 适用场景

- 离线批处理工作负载：模型评估、合成数据生成、数据整理、表单处理、关系型分析等追求高吞吐量的场景。
- 内存受限环境：单低 - cost GPU（如 T4、L4）或多低 - cost GPU 部署大型 MoE 模型（如 Mixtral 8x22B、DBRX）。

#### 局限

- 暂不支持磁盘卸载，仅适用于 CPU 内存足够存储模型的场景。
- 性能模型 HRM 目前局限于单节点硬件，未考虑 GPU 间通信和多节点通信。
- 长上下文场景下，CPU 注意力可能成为瓶颈，需结合 KV 缓存稀疏化等进一步优化。

### 五、结论

MoE-Lightning 通过 CGOPipe 调度策略和 HRM 性能模型，解决了内存受限 GPU 上 MoE 模型推理的吞吐量与资源利用率问题。在单 GPU 上实现最高 10.3 倍的吞吐量提升，多 GPU 场景下展现超线性缩放能力，且仅需更少 CPU 内存即可达到性能上限，为缺乏高端 GPU 资源的用户部署大型 MoE 模型提供了高效解决方案。未来可扩展至磁盘卸载、多节点通信支持，并融入 KV 缓存稀疏化等优化，进一步提升长上下文推理效率。



## [<span id ="asplos2505-1">CXLfork: Fast Remote Fork over CXL Fabrics</span>](#asplos2505)

​	该文章提出了一种基于 CXL（Compute Express Link）共享内存架构的远程进程克隆接口 CXLfork，通过近零序列化、零复制的设计，将进程私有状态以原生内存复制方式直接存储于 CXL 内存，全局状态仅序列化必要信息，结合写时复制（CoW）机制实现跨节点状态共享与内存去重，并提供写时迁移、访问时迁移、混合分层三种精细的状态分层策略平衡性能与内存开销；同时基于 CXLfork 构建了无服务器函数自动扩缩容工具 CXLporter，通过智能 checkpoint 管理、幽灵容器池、动态分层策略调整等功能优化冷启动性能与资源利用率。实验表明，CXLfork 相比现有方案（CRIU、Mitosis）平均性能提升 2.26 倍和 1.40 倍，本地内存消耗降低 87% 和 61%，CXLporter 能显著减少无服务器函数的尾延迟并提升集群并发能力，为云原生无服务器计算提供了高效的跨节点进程克隆解决方案。

### 一、研究背景与动机

#### 1. 技术基础

Compute Express Link（CXL）作为新兴的缓存一致性互连技术，支持字节可寻址的低延迟远程内存访问，其 3.0 及以上版本更实现了跨计算节点的机架级缓存一致性内存共享。这一特性为分布式系统的传统软件接口革新提供了可能，尤其适用于高性能云原生无服务器计算中的集群级进程克隆场景。

#### 2. 现有方案局限

现有远程进程克隆机制难以充分利用 CXL 共享内存优势：

- **CRIU**：通过序列化进程状态到文件实现跨节点迁移，存在大量序列化 / 反序列化开销，且父进程与子进程无状态共享，内存消耗极高。
- **Mitosis**：基于 RDMA 实现惰性数据复制，避免了进程内存的序列化，但仍需序列化操作系统管理状态，且数据复制带来显著延迟，无法发挥 CXL 的低延迟共享特性。

#### 3. 无服务器计算的核心需求

无服务器函数（FaaS）频繁创建和销毁实例，冷启动开销（初始化状态、容器创建等）成为性能瓶颈。这类函数的内存足迹中，初始化数据（72.2%）和只读数据（23%）占比极高，具备跨节点共享的潜力，为 CXL 共享内存的应用提供了天然场景。

### 二、核心方案：CXLfork 设计

CXLfork 是专为 CXL 架构设计的远程进程克隆接口，实现近零序列化、零复制的集群级进程克隆，核心目标是平衡性能、内存共享与数据分层。

#### 1. 核心设计理念

利用 CXL 全局共享内存特性，将进程状态直接存储于 CXL 内存而非本地文件或节点内存，实现跨节点状态共享与内存去重，同时通过精细的状态分层策略优化访问延迟。

#### 2. 关键技术突破

#### （1）进程状态的非序列化 checkpoint

- 区分**私有状态**（进程任务结构、内存描述符、页表、寄存器等）和**全局状态**（打开文件、套接字、命名空间等）。
- 私有状态通过原生内存复制直接写入 CXL 内存，无需序列化；页表项被修改为指向 CXL 物理地址并标记为只读，同时保留访问（A）和脏页（D）位以追踪访问模式。
- 全局状态仅序列化必要信息（如文件路径、权限），避免完整序列化带来的开销，恢复时通过重新执行系统调用重建。

#### （2）高效状态共享与零复制恢复

- 恢复时直接将 CXL 中的 checkpoint 状态映射到目标进程地址空间，采用写时复制（CoW）机制处理修改：只读状态常驻 CXL 内存供所有克隆进程共享，写操作触发缺页中断将数据迁移至本地内存。
- 优化页表和虚拟内存区域（VMA）树的恢复：仅在本地内存初始化页表树的上层结构，直接挂载 CXL 中的页表叶子节点，实现近常数时间的状态恢复。

#### （3）精细的状态分层策略

支持三种数据分层策略，平衡内存消耗与执行性能：

- **写时迁移（MoW）**：默认策略，仅在写操作时将 CXL 中的页面迁移至本地，最大化内存共享。
- **访问时迁移（MoA）**：首次访问时将页面迁移至本地，降低后续访问延迟，但增加内存消耗。
- **混合分层（HT）**：基于页表的访问位（A 位）识别热点页面，仅将高频访问页面迁移至本地，兼顾内存效率与性能。

### 三、衍生应用：CXLporter 无服务器自动扩缩容器

基于 CXLfork 构建 CXLporter，专为 CXL 互连集群中的无服务器函数设计，优化冷启动性能与资源利用率。

#### 1. 核心功能

- **智能 checkpoint 管理**：函数执行 16 次后（确保 JIT 编译达到稳定状态）创建 checkpoint，存储于 CXL 分布式对象存储中，并定期清理访问位以更新热点页面识别。
- **幽灵容器池**：维护预初始化的空容器（仅占用 512KB 内存），避免容器创建开销（约 130ms），通过 CXLfork 快速克隆函数状态至容器。
- **动态策略调整**：根据函数性能指标（延迟是否满足 SLO）和节点内存压力，动态切换 CXLfork 分层策略，并调整函数保活窗口（最低至 10 秒）以回收内存。

### 四、实验评估

#### 1. 实验环境

- 硬件：双路 Intel Sapphire Rapids 服务器（每路 64 核、128GB DDR5 内存），Intel Agilex 7 FPGA-based CXL 内存设备（16GB DDR4，访问延迟 391ns）。
- 软件：Linux 6.6 内核，QEMU/KVM 模拟双节点集群，基于 OpenWhisk 的无服务器平台。
- 工作负载：10 个典型无服务器函数（包括 Float、BERT、CNN 等，内存足迹 24-630MB），模拟 Azure 真实负载（150 RPS）。

#### 2. 核心性能结果

#### （1）CXLfork 性能优势

- 恢复延迟：仅 1.2-6.1ms，平均比 CRIU 快 2.26 倍，比 Mitosis 快 1.40 倍，接近本地 fork 性能（仅慢 14%）。
- 内存消耗：平均比 CRIU 减少 87%，比 Mitosis 减少 61%，仅为冷启动函数内存消耗的 13%。
- 分层策略效果：混合分层（HT）在 BERT、BFS 等大内存函数中，实现冷启动延迟与内存消耗的平衡，比写时迁移（MoW）降低 11% 的热执行延迟。

#### （2）CXLporter 扩缩容效果

- 充足内存场景：P99 延迟比 CRIU 降低 70%，比 Mitosis 降低 51%，幽灵容器池消除了容器创建开销。
- 内存受限场景（25% 内存）：CXLfork 的 P99 延迟比 CRIU 和 Mitosis 低 16 倍，吞吐量提升 2 倍，凸显内存去重优势。

#### （3）CXL 延迟敏感性

CXL 内存延迟降低至 200ns 时，BERT、BFS 等大工作集函数的热执行性能接近本地内存；即使延迟为 400ns，冷启动性能仍优于传统方案。

### 五、结论与贡献

#### 1. 核心贡献

- 提出首个基于 CXL 共享内存的远程 fork 接口，实现近零序列化、零复制的集群级进程克隆。
- 设计精细的状态分层策略，平衡内存去重、内存节省与运行时性能。
- 构建 CXLporter 自动扩缩容器，显著降低无服务器函数冷启动延迟与内存消耗。

#### 2. 应用价值

CXLfork 与 CXLporter 充分发挥了 CXL 架构的共享内存优势，为无服务器计算提供了高性能、低内存消耗的跨节点进程克隆方案，尤其适用于负载突发场景，可提升集群吞吐量与资源利用率。未来可进一步优化多节点集群中的 CXL 带宽调度，并扩展至复杂无服务器工作流的跨函数通信优化。



## [<span id ="asplos2506-1">PIM Is All You Need: A CXL-Enabled GPU-Free System  for Large Language Model Inference</span>](#asplos2506)

​	该文档提出了一种名为 CENT 的无 GPU 系统，专为大型语言模型（LLM）推理设计，其核心是通过 CXL 3.0 协议实现内存扩展以满足 LLM 的大容量需求，采用分层 PIM-PNM 架构（PIM 负责 99% 以上的 MAC 运算，PNM 处理特殊复杂操作），支持流水线并行、张量并行及混合并行策略，相比 4 块 NVIDIA A100 GPU，在相似功耗下实现 2.3 倍更高吞吐量、2.3 倍更低能耗，每美元生成令牌数提升 5.2 倍，且在长上下文场景（如 32K 令牌）中表现更优，同时硬件成本和总拥有成本（TCO）显著降低，为 LLM 推理提供了高效、经济的替代方案。

### 核心背景与痛点

- LLM 推理存在两大核心需求：一是模型参数和 KV 缓存需要超大内存容量，二是自回归生成导致运算强度低，对内存带宽要求极高。
- 现有 GPU/TPU 主要优化算力吞吐量，在内存受限的 LLM 推理中算力利用率仅 21% 左右，且成本高昂（如 ChatGPT 每日推理成本约 69 万美元）。
- 传统 PIM（内存内处理）带宽高但内存密度低，PNM（近内存处理）密度优但带宽不足，均难以单独满足 LLM 需求。

### CENT 系统核心设计

- **硬件架构**：基于 CXL 3.0 协议构建可扩展网络，32 个 CXL 设备通过 CXL 交换机互联，每个设备包含 16 个 GDDR6-PIM 芯片和 PNM 单元，总内存 512GB，内部峰值带宽达 512TB/s。
- **分层计算设计**：PIM 芯片负责 99% 以上的 MAC 运算（如 GEMV），PNM 单元包含加速器和 RISC-V 核心，处理 Softmax、平方根等特殊操作，无需 GPU 参与。
- **通信与并行策略**：支持点对点和集合通信原语（广播、汇聚等），适配三种并行映射：流水线并行（PP）提升吞吐量、张量并行（TP）降低延迟、混合 TP-PP 平衡二者。

### 关键性能与成本优势

- 与 4 块 A100 GPU 相比，在相同功耗下，CENT 吞吐量提升 2.3 倍，能耗降低 2.3 倍，每美元生成令牌数提升 5.2 倍。
- 长上下文场景优势显著，32K 上下文时解码吞吐量提速 3.3 倍，且查询延迟远低于 GPU（3.4-7.6 倍）。
- 总成本（TCO）大幅降低，3 年租赁成本仅为 GPU 系统的 19%，硬件成本约 1.49 万美元，仅为 GPU 方案（4.21 万美元）的 35%。

### 对比与扩展性

- 相较于 CXL-PNM 系统，CENT 吞吐量提升 4.5 倍；相较于 GPU-PIM 混合系统（AttAcc/NeuPIM），每美元令牌数提升 1.8-5.3 倍。
- 支持灵活扩展，设备数量从 16 增至 128 时，吞吐量可从 0.68K 令牌 / 秒提升至 5.7K 令牌 / 秒，适配 7B 到 70B 及更大参数模型（如 Grok 314B）。

### 适用场景与价值

- 适用于长上下文推理（如 10 万 - 100 万令牌的视频生成、推理任务）、大规模批量推理场景，可完全替代 GPU 或与 GPU 分工（GPU 负责预填充阶段，CENT 负责解码阶段）。
- 提供开源模拟器和 Python API，支持主流 LLM 的激活函数（GeLU、SiLU）和位置编码，具备较强通用性。





## [<span id ="asplos2507-1">PAPI: Exploiting Dynamic Parallelism  in Large Language Model Decoding with a  Processing-In-Memory-Enabled Computing System</span>](#asplos2507)

​	该文档提出了一种名为 PAPI 的基于内存内处理（PIM）的异构架构，旨在解决大型语言模型（LLM）解码阶段因并行性动态变化导致的性能瓶颈问题。PAPI 针对现有架构静态内核映射无法适配 LLM 解码中全连接（FC）层内核在计算密集型和内存密集型之间动态切换、且单一 PIM 单元难以满足不同内核异构需求的不足，整合了主机 CPU、GPU 张量核心以及两种定制化 PIM 单元（面向 FC 层的高性能 FC-PIM、面向注意力层的大内存容量 Attn-PIM），并通过在线内核特征化与两阶段动态调度机制，实时判断 FC 层内核类型并分配至最优硬件单元，同时让注意力层内核固定运行于 Attn-PIM。实验结果显示，在 LLaMA-65B、GPT-3 66B/175B 等模型上，PAPI 相比最先进的异构加速器（A100+AttAcc）和纯 PIM 加速器（AttAcc-only）分别实现 1.8 倍和 11.1 倍的性能提升，且在创意写作、通用问答等任务中显著优化了能效，为 LLM 推理提供了高效、灵活的解决方案。

### 一、研究背景与问题

1. **LLM 解码的关键地位**：LLM 推理包含预填充（prefill）和解码（decoding）两个阶段，解码阶段占总执行时间的绝大部分（如 GPT-3 175B 模型达 96%），且输出长度越长，解码的性能影响越显著。
2. **现有优化技术的局限**：现有并行优化技术（批处理 batching、推测解码 speculative decoding）虽能提升解码并行性，但导致解码内核（全连接 FC 层、多头注意力层）的特性动态变化 —— 部分内核会在计算密集型（compute-bound）和内存密集型（memory-bound）之间切换。
3. **传统架构的不足**：现有异构架构（如 GPU+PIM）采用静态内核映射策略，无法适应上述动态变化；且现有 PIM 单元仅支持单一计算 / 内存带宽配置，难以满足不同内核的异构需求。

### 二、核心观察与设计目标

1. **关键观察**：解码并行性（请求级 RLP、令牌级 TLP）会因服务等级目标（SLO）、内存容量限制、动态批处理等因素动态变化，进而导致 FC 层内核的算术强度改变，使其在计算密集型和内存密集型之间切换；而注意力层内核始终为内存密集型，但与 FC 层的内存 / 计算需求差异显著。
2. **设计目标**：构建支持动态调度的异构架构，适配 LLM 解码的动态需求，同时提供差异化的计算和内存带宽能力，优化性能与能效。

### 三、PAPI 的核心设计

PAPI 包含三大核心组件，实现动态并行性感知的高效调度：

1. **异构计算架构**：整合三类计算单元 —— 主机 CPU、计算中心型处理器（GPU 张量核心）、内存中心型 PIM 单元（FC-PIM、Attn-PIM），覆盖不同计算 / 内存需求。
2. **混合 PIM 单元**：
   - **FC-PIM**：针对 FC 层内核设计，采用 4 个浮点运算单元（FPU）/ 内存 bank 的高并行配置，平衡计算性能与功耗、面积约束。
   - **Attn-PIM**：针对注意力层内核设计，采用 1 个 FPU/2 个 bank 的低并行配置，侧重内存容量与带宽，通过离散式部署支持大规模 KV 缓存存储。
3. **动态并行性感知调度**：
   - **在线内核特征化**：通过实时计算 RLP×TLP 估算 FC 层内核的算术强度，快速判断其为计算密集型或内存密集型。
   - **两阶段调度**：初始调度基于初始并行性参数分配内核；运行时调度监测 RLP/TLP 变化，动态调整内核到最优硬件单元（计算密集型→GPU，内存密集型→FC-PIM；注意力层固定分配给 Attn-PIM）。

### 四、实验结果

1. **性能提升**：在 LLaMA-65B、GPT-3 66B/175B 模型上，PAPI 相比最先进的异构加速器（A100+AttAcc）提速 1.8 倍，相比纯 PIM 加速器（AttAcc-only）提速 11.1 倍；在长输出任务中（如创意写作），提速效果更显著。
2. **能效优化**：相比 A100+AttAcc，PAPI 在创意写作和通用问答任务中的能效分别提升 3.4 倍和 3.1 倍，通过减少数据迁移和低功耗 PIM 计算降低能耗。
3. **适应性验证**：在不同并行性水平（批大小 4-128、推测长度 1-8）下，PAPI 均保持最优性能，验证了其对动态并行性的适配能力。

### 五、创新贡献

1. 首次揭示 LLM 解码并行性的动态变化特性，以及其对内核需求的影响。
2. 提出混合 PIM 架构与动态调度框架，实现内核与硬件单元的动态最优匹配。
3. 验证了 PAPI 在不同模型、任务和并行配置下的性能与能效优势，为 LLM 推理的动态优化提供了新范式。



## [<span id ="asplos2508-1">POD-Attention: Unlocking Full Prefill-Decode  Overlap for Faster LLM Inference</span>](#asplos2508)

​	POD-Attention 是首个专为混合批处理 LLM 推理设计的 GPU 注意力内核，通过 SM 感知的 CTA 调度及瓦片大小优化、虚拟解码 CTA 等技术，实现预填充与解码注意力计算的并发执行，同时充分利用 GPU 计算资源和内存带宽，相较于 FlashAttention 等独立优化内核，其注意力计算最高提速 59%（平均 28%），集成到 Sarathi-Serve 后，可使 LLM 推理吞吐量最高提升 22%，并显著降低 TTFT、TBT 及请求执行延迟，在长上下文场景中表现突出，还能减少最高 35% 的能量消耗。

### 核心背景与问题

LLM 推理包含计算密集型的预填充（prefill）和内存带宽密集型的解码（decode）两个阶段。现有系统采用混合批处理（hybrid batching）将不同请求的两阶段任务合并到同一批次，优化了线性运算，但注意力计算仍存在效率瓶颈：现有注意力内核（如 FlashAttention、FlashInfer）分别针对两阶段独立优化，导致 GPU 计算资源和内存带宽无法同时充分利用（例如预填充阶段内存带宽利用率常低于 5%，解码阶段计算利用率不足 10%），资源浪费严重。

### 核心方案：POD-Attention

POD-Attention 是首个专为混合批处理设计的 GPU 注意力内核，通过让预填充和解码注意力计算在同一流多处理器（SM）上并发执行，同时最大化计算和内存带宽利用率。其核心设计包括：

#### 1. 关键技术创新

- **SM 感知的 CTA 调度**：通过运行时操作绑定，让协作线程数组（CTA）在分配到 SM 后动态决定执行预填充或解码任务，保证两阶段任务在 SM 级共置，避免资源闲置。支持 50:50 交替调度和按任务比例调度两种策略，后者在高负载下性能更优。
- **多维度性能优化**：
  - 适配性瓦片大小：为解码阶段使用最小 16 的 QSL 瓦片长度，减少冗余计算，释放张量核心供预填充使用；
  - 动态 CTA 配置：预填充主导场景采用 2 个 CTA/SM（保障共享内存使用），其他场景采用 4 个 CTA/SM（优化细粒度调度）；
  - 虚拟解码 CTA：将解码 CTA 拆分为 warp 级虚拟 CTA，平衡预填充与解码的共享内存需求；
  - 限制预填充拆分：避免 KV 维度过度拆分导致的带宽竞争，最多允许两波完整拆分。
- **CTA 并行融合实现**：将预填充和解码内核转换为可调用设备函数，通过包装器内核灵活映射 CTA ID，统一管理共享内存，用 warp 级屏障替代解码阶段的 CTA 级屏障，确保融合兼容性。

#### 2. 实验配置

- **模型与硬件**：基于 Yi-6B（1 块 A100）、Llama-2-7B、Llama-3-8B（2 块 A100），均含 32 个查询头，KV 头数量分别为 4、32、8；
- **基准系统**：对比 vLLM（预填充优先调度）、Sarathi-Serve（混合批处理调度），均使用 FlashAttention 内核；
- **评估场景**：离线推理（吞吐量）、在线推理（TTFT、TBT、请求延迟等），测试上下文长度 4K-32K，预填充与解码令牌比（P:D）0-50。

### 核心实验结果

#### 1. 注意力计算性能

- 相较于 FlashAttention、FlashInfer 等独立优化内核，POD-Attention 最高提速 59%，平均提速 28%，25% 的场景接近理论峰值性能，且从未低于串行执行效率；
- 能量消耗降低最高 35%（平均 20.5%），与运行时缩短比例一致。

#### 2. 离线推理吞吐量

- 相较于 Sarathi-Serve，Yi-6B、Llama-2-7B、Llama-3-8B 的吞吐量分别提升 22%、20%、19%；
- 相较于 vLLM，吞吐量分别提升 27%、13%、12%，突破混合批处理中预填充拆分与线性运算优化的权衡限制。

#### 3. 在线推理延迟

- **TTFT（首令牌时间）**：高负载下，Sarathi+POD 的中位数 TTFT 较 Sarathi 降低至 7.5 秒（内部工作负载）和 11.74 秒（arXiv 工作负载），P99 TTFT 最高降低 4.3 倍；
- **TBT（令牌间隔时间）**：P99 TBT 较 vLLM 降低 10 倍（0.14 秒 vs 1.76 秒），较 Sarathi 进一步降低 10%-20%，几乎消除生成停滞（<5% 请求出现停滞，vLLM 达 97%+）；
- **请求延迟**：P99 请求延迟较 vLLM 最高降低 42%（内部工作负载）和 17%（arXiv 工作负载），平衡吞吐量与延迟权衡。

#### 4. 敏感性分析

- 长上下文（预填充主导）场景下，2 个 CTA/SM 更优；短上下文（解码主导）场景下，4 个 CTA/SM 更优；
- P:D 比例 12-18 时性能增益峰值（混合批处理占比最高），纯预填充或纯解码场景增益有限；
- 比例调度策略较 50:50 调度最高提速 14%，尤其适合高负载场景。

### 相关工作对比

- 现有注意力优化（FlashAttention-3、FlashDecoding++）：均独立优化预填充或解码，未支持两阶段融合；
- 内核融合技术（HFuse、ISPA）：存在拖尾线程、SM 级共置无保障等问题，POD-Attention 通过 SM 感知调度和 CTA 并行融合解决；
- LLM 推理优化（NanoFlow、DistServe）：NanoFlow 适用于小上下文，DistServe 依赖跨节点拆分，POD-Attention 聚焦单设备长上下文混合批处理场景。

### 结论

POD-Attention 通过预填充与解码注意力的并发融合，首次实现 GPU 计算与内存带宽的同时高效利用，为混合批处理 LLM 推理提供了专用注意力内核解决方案。其在吞吐量（最高 + 22%）、延迟（TTFT、TBT、请求延迟显著降低）和能效上的全面提升，对长上下文 LLM 部署具有重要实践价值。未来可扩展至 FlashAttention-3 和 NVIDIA Hopper 架构支持。



# [<span id ="hpca-title"> HPAC</span>](#hpca)

## 2025

[<span id ="hpca2501">Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format</span>](#hpca2501-1)

[<span id ="hpca2502">BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration</span>](#hpca2502-1)

[<span id ="hpca2503">DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency</span>](#hpca2503-1)

[<span id ="hpca2504">HSMU-SpGEMM: Achieving High Shared Memory Utilization for Parallel Sparse General Matrix-Matrix Multiplication on Modern GPUs</span>](#hpca2504-1)

[<span id ="hpca2505">InstAttention: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference</span>](#hpca2505-1)

[<span id ="hpca2506">LAD: Efficient Accelerator for Generative Inference of LLM with Locality Aware Decoding</span>](#hpca2506-1)

[<span id ="hpca2507">M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type</span>](#hpca2507-1)

[<span id ="hpca2508">throttLL’eM: Predictive GPU Throttling for Energy Efficient LLM Inference Serving</span>](#hpca2508-1)

[<span id ="hpca2509">VQ-LLM: High-performance Code Generation for Vector Quantization Augmented LLM Inference</span>](#hpca2509-1)

##  [<span id ="hpca2501-1">Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format</span>](#hpca2501)

​	本文提出 Anda 方案，通过三大核心创新解决权重量化大语言模型（LLM）推理中浮点数（FP）激活的性能与能耗瓶颈：设计组共享指数 + 动态尾数分配的可变长度 Anda 数据格式，开发无需重训练、适配不同模块精度敏感性的自适应精度搜索算法，以及配套的位平面数据布局、位串行处理单元和运行时位平面压缩器等硬件优化，在 OPT、LLaMA/LLaMA2 系列模型上实现平均 2.4 倍加速、4.0 倍面积效率提升和 3.1 倍能效提升，在 0.1%-1% 精度损失约束下达成精度与效率的平衡，适配多种部署场景。

### 研究背景与挑战

- 权重量化（如 W4A16）是 LLM 部署的主流方案，虽降低存储成本，但 FP 激活相关的 FP-INT GeMM 操作占比超 90%，成为 latency 和能耗瓶颈。
- 现有方案存在缺陷：GPU 核需转换 INT 权重为 FP 导致额外开销，专用 FP-INT 单元硬件复杂，块浮点（BFP）格式需重训练或长尾数才能保精度。

### 核心创新

#### 1. Anda 数据格式

- 基于 BFP 改进，采用组共享指数 + 动态尾数分配，支持 1-16 位连续尾数长度，可针对不同 LLM 模块灵活调整精度。
- 相比固定长度 BFP 和有限多长度方案，实现更细粒度的精度控制，平衡精度与效率。

#### 2. 自适应精度搜索算法

- 离线校准过程复用权重量化的校准数据，针对四大关键模块（*A**q**k**v*、*A**o*、*A**u*、*A**d*）搜索最优尾数组合。
- 以比特操作数（BOPs）为优化目标，在用户设定的精度损失阈值（如 0.1%、1%）内，快速找到近优解，无需重训练，搜索效率比现有方法快 2-10 倍。

#### 3. 专用硬件架构

- 位平面数据布局：将同权重位打包存储，保证变长数据的内存访问规律性。
- Anda 增强型位串行处理单元（APU）：适配变长尾数计算，减少硬件开销，支持 INT 点积与 FP 累加。
- 运行时位平面压缩器（BPC）：实时将 FP16 激活转换为 Anda 格式，降低存储和访问成本。

### 实验结果

- 精度表现：在 WikiText2、PTB、C4 数据集上，Anda 在 0.1%-1% 精度损失下，实现 1.8-3.3 倍 BOPs reduction，远超 FIGNA 的 1.23 倍，且避免了 VS-Quant 的严重精度下降。
- 硬件效率：相比 GPU-like FP-FP 基线，平均实现 2.4 倍加速、4.0 倍面积效率提升、3.1 倍能效提升；PE 级功耗和面积均低于 FP-FP、FP-INT 等基线单元。
- 适应性：支持 OPT、LLaMA/LLaMA2 系列（1.3B-30B 参数），可根据不同精度需求、模型及模块敏感性动态调整，适配短上下文（<4K）和长上下文（>10K）场景。

### 结论

Anda 通过数据格式、算法与硬件的协同优化，有效突破权重量化 LLM 的 FP 激活瓶颈，在无需重训练的前提下，实现精度、速度与能耗的平衡，为资源受限场景下的 LLM 部署提供高效解决方案。



 ## [<span id ="hpca2502-1">BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration</span>](#hpca2502)

​	BitMoD是一种算法-硬件协同设计方案，旨在解决大语言模型（LLM）部署中内存占用过大的问题，其通过在算法层设计扩展分辨率（ER）和扩展不对称性（EA）的3bit/4bit浮点数据类型，基于分组量化（每组128个权重）并复用冗余零值为特殊值，在极低权重精度下维持模型精度，同时在硬件层采用统一位串行表示和专用处理单元（PE），支持多精度数据类型高效计算与位串行反量化，最终在6个代表性LLM上实现了优于现有方案的性能——4bit量化判别任务平均精度损失<0.5%，3bit量化生成任务困惑度更优，较ANT/OliVe加速器分别平均提速1.69×/1.48×、能效提升1.48×/1.31×，且可无缝集成AWQ、SmoothQuant等现有量化方案，为边缘设备部署LLM提供了高效解决方案。

### 一、研究背景与动机

1. **LLM 部署痛点**：LLM 参数规模激增（如 Llama-3-8B 含 80 亿参数，FP16 格式需 16GB 内存），远超边缘设备（如 Jetson-TX2 仅 8GB 内存）的存储与计算能力，限制了其广泛应用。
2. **量化技术现状**：
   - 量化是降低内存和计算开销的关键手段，分为训练时量化（QAT，成本高）和训练后量化（PTQ，更实用）。
   - 现有权重量化方案存在不足：GPU 缺乏低精度整数权重与浮点激活的专用计算单元，导致计算效率低；部分方案不支持细粒度量化，或在低精度（3-4bit）下精度损失严重。
3. **核心需求**：需设计适配 LLM 分布特性的低精度数据类型，结合专用硬件架构，在极低权重精度下维持模型精度，同时提升硬件效率。

### 二、BitMoD 核心设计

BitMoD 的核心是 “算法 - 硬件协同优化”，分为算法层和硬件层两部分：

#### （一）算法层：低精度数据类型与量化策略

1. **细粒度数据类型适配**：
   - 基于分组量化（每组 128 个权重），改造 3bit 和 4bit 浮点数据类型：将浮点格式中冗余的 “零值” 替换为特殊值，形成两类扩展数据类型 —— 扩展分辨率（ER，如 FP3-ER、FP4-ER）和扩展不对称性（EA，如 FP3-EA、FP4-EA），兼顾对称分布适配与不对称 outliers 处理。
   - 每组权重动态选择最优特殊值，最小化量化误差，仅需 2bit 编码开销。
2. **高效分组反量化**：将分组量化的缩放因子量化为 INT8（无精度损失），避免浮点运算带来的硬件开销，支持位串行反量化。
3. **兼容性**：可无缝集成 AWQ、SmoothQuant 等现有软件量化优化方案，替换其整数量化器以进一步提升性能。

#### （二）硬件层：位串行加速器架构

1. **统一位串行表示**：将 INT8、INT6、扩展 FP3/FP4 等多种数据类型，统一分解为 “符号 + 指数 + 尾数 + 位重要性” 四部分，支持多精度统一计算，降低硬件复杂度。
2. **位串行处理单元（PE）**：
   - 支持低精度权重（3-6bit）与 FP16 激活的混合精度计算，通过 Booth 编码、移位对齐、累加归一化等步骤高效完成点积运算。
   - 内置位串行反量化单元，实时处理分组缩放因子，无流水线阻塞。
3. **整体架构**：4×4 PE tile 组成的脉动阵列，采用输出固定数据流，通过权重共享和输入共享提升数据复用率，配备 512KB 激活 / 权重缓存，适配边缘场景小批量（batch=1）、短序列（输入 256token）需求。

### 三、实验结果

#### （一）模型精度表现

- **生成任务**：4bit 量化平均困惑度损失 < 0.5，3bit 量化损失 < 3，显著优于 ANT、OliVe 等现有方案（3bit 下困惑度大幅退化）；与 AWQ/OmniQuant 结合后，3bit 量化平均困惑度损失 < 1。
- **判别任务**：4bit 量化平均精度损失 < 0.5%，3bit 量化精度损失较 INT3-Asym 提升 2.2%，在 HellaSwag、WinoGrande 等数据集上表现稳定。
- **兼容性**：与 SmoothQuant 结合量化激活至 INT8 后，仍保持精度优势，3bit 权重量化时困惑度提升明显。

#### （二）硬件性能与效率

- **速度提升**：相比 FP16 基线加速器，无损配置（INT6 量化）平均提速 2.2×，有损配置（3-4bit）较 ANT/OliVe 分别平均提速 1.69×/1.48×。
- **能效优化**：较 FP16 基线能效提升 2.31×，较 ANT/OliVe 分别提升 1.48×/1.31×；PE 面积较 FP16 PE 小 24%，支持多精度灵活切换。
- ** Pareto 最优 **：在困惑度 - 能效积（EDP）权衡中处于 Pareto 前沿，兼顾精度与硬件效率。

### 四、核心贡献

1. 提出适配 LLM 分组量化的扩展浮点数据类型，实现 3-4bit 低精度量化下的高精度保留。
2. 设计统一位串行硬件架构，支持多数据类型 / 精度，降低硬件成本并提升计算效率。
3. 算法 - 硬件深度协同，兼容现有软件量化方案，为边缘设备部署 LLM 提供高效解决方案。



## [<span id ="hpca2503-1">DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency</span>](#hpca2503)

​	DynamoLLM 是首个针对 LLM 推理集群的能量管理框架，旨在解决 LLM 推理依赖高功耗 GPU 导致的高能耗、高碳排放问题，其核心是利用 LLM 推理工作负载的异构性（如请求长度、模型类型、SLO 要求差异）和动态性（如负载波动、请求类型分布变化），通过分层控制架构（集群、池、实例三级控制器）动态优化实例数量、模型并行度（TP2/TP4/TP8）和 GPU 频率，同时通过缓存模型权重、优化重分片策略、预测调度等方式最小化重配置开销，在满足延迟 SLO 的前提下，平均节省 52% 能耗、38% 运营碳排放，并降低 61% 用户成本，经 Microsoft Azure 生产级迹线验证，在不同负载和模型场景下均展现出高效稳定性。

### 一、研究背景与问题

随着生成式大语言模型（LLM）在医疗、开发、教育等领域的广泛应用，LLM 推理集群需处理海量查询并满足严格的服务等级目标（SLOs）。当前 LLM 推理依赖高功耗 GPU，导致集群能耗巨大、碳排放显著，而现有能量管理方案多针对传统数据中心 workload，未充分适配 LLM 推理的独特特性：

1. **异构性**：请求的输入 / 输出令牌长度、不同 LLM 的计算属性、服务 SLO 要求存在显著差异；
2. **动态性**：请求类型分布、负载量（如昼夜波动）、服务与模型组合随时间快速变化；
3. **配置复杂性**：实例数量、模型并行度（张量 / 流水线并行）、GPU 频率等多维度配置形成庞大搜索空间，单一配置难以兼顾能效与性能；
4. **重配置开销**：调整实例数量、并行度或 GPU 频率会产生不可忽视的延迟，影响服务连续性。

### 二、核心洞察

1. LLM 推理工作负载的能效特性高度异构，不同类型请求（长短输入 / 输出、不同模型、SLO）需差异化配置才能实现最优能效；
2. 工作负载的动态变化导致静态配置快速失效，需自动化、透明化的动态配置机制；
3. GPU 功耗与负载呈亚线性缩放，且重配置开销（如实例启动需 6-8 分钟）需纳入能效权衡。

### 三、DynamoLLM 框架设计

DynamoLLM 是首个针对 LLM 推理集群的能量管理框架，核心目标是在满足延迟 SLO 的前提下，最小化能耗、碳排放与用户成本，其设计包含四大关键模块：

#### 1. 分层控制架构

采用集群、池、实例三级控制器，分别负责不同粒度的配置优化，降低计算复杂度并避免集中式瓶颈：

- **集群管理器**：每 30 分钟预测负载，划分请求类型池（如 SS 短输入短输出、LL 长输入长输出），动态调整各池实例数量，合并低负载池以减少资源碎片化；
- **池管理器**：每 5 分钟优化模型并行度（TP2/TP4/TP8），基于当前 GPU 数量分配实例资源；
- **实例管理器**：每 5 秒微调 GPU 频率，筛选满足 SLO 的最低能耗频率配置。

#### 2. 能效配置优化

- **模型 profiling**：预先生成不同负载、请求类型、并行度、频率下的能效 - 性能剖面，预测准确率超 98%，支持跨服务复用；
- **多维度调优**：联合优化实例数量（扩缩容）、模型并行度（张量并行为主）、GPU 频率（800-1980MHz），通过混合整数线性规划求解最优配置；
- **预测调度**：基于 BERT 代理模型预测请求输出长度（准确率 81%），将请求路由至匹配池；针对负载预测采用模板法，适配昼夜波动模式。

#### 3. 重配置开销优化

- **实例扩缩容**：缓存模型权重、预启动虚拟机快照、后台提前扩容，将实例启动开销从 6-8 分钟降至可忽略；
- **并行度调整**：通过图匹配算法最大化 GPU 权重复用，利用 NVLink 直连并行传输，将重分片开销降至 50-100ms；
- **频率调整**：常驻 nvidia-smi 进程，以特权模式运行控制器，减少 OS 交互开销。

#### 4. 鲁棒性机制

- 处理预测错误：优先级调度即将超时请求，动态提升 GPU 频率，或迁移未执行请求至其他池；
- 适配多样 SLO：为不同 SLO 请求单独计算最优配置，仅在能效收益显著时拆分资源池。

### 四、实验结果

基于 Microsoft Azure 生产级迹线（含 1 小时、1 天、1 周数据），在 8 卡 H100 GPU 集群上的评估显示：

1. **核心收益**：平均节省 52% 能耗、38% 运营碳排放、61% 用户成本，同时满足所有延迟 SLO；
2. **性能表现**：P99 TTFT（首令牌延迟）降低 5.3%，P99 TBT（令牌间延迟）降低 11.0%，仅 P50 延迟小幅上升（TTFT+11.4%、TBT+7.6%）；
3. **负载适应性**：低 / 中 / 高负载下分别节省 57%/42%/15% 能耗， Coding 服务（周末低负载显著）比 Conversation 服务节能效果更优（56% vs 47%）；
4. **鲁棒性**：输出长度预测准确率降至 60% 时，能耗仅增加 25%，TTFT 上升 12.3%。

### 五、核心贡献

1. 首次系统分析 LLM 推理的异构性与动态性带来的能效优化机遇；
2. 设计并实现首个支持多维度动态配置的 LLM 推理能效框架，兼顾性能与开销；
3. 基于大规模生产环境验证框架有效性，开源部分迹线数据集（https://github.com/Azure/AzurePublicDataset）。



## [<span id ="hpca2504-1">HSMU-SpGEMM: Achieving High Shared Memory Utilization for Parallel Sparse General Matrix-Matrix Multiplication on Modern GPUs</span>](#hpca2504)

​	该论文提出面向现代GPU的稀疏矩阵乘法算法HSMU-SpGEMM，针对传统哈希基方法在哈希冲突与共享内存利用率间的权衡难题，设计了基于预排序列索引数组与二分查找的新型累加器，结合适配中小规模矩阵的掩码矩阵与大规模矩阵的压缩掩码格式优化符号阶段，并动态融合稠密累加器，在Ampere、Ada Lovelace、Turing三种架构的NVIDIA GPU上，基于338个SuiteSparse稀疏矩阵的测试显示，其相对Nsparse、spECK、OpSparse、cuSPARSE四大主流库的几何平均加速比最高分别达3.19、1.28、2.34、6.92，共享内存平均利用率达68%，同时解决了超大规模矩阵计算的内存溢出问题，在科学计算、图处理等场景具备高效实用价值。

### 一、研究背景与问题

1. **SpGEMM 的重要性**：稀疏矩阵乘法是科学计算、图处理等领域的核心原语，如广度优先搜索、马尔可夫聚类等应用均依赖该运算。
2. **传统方法的局限**：现有哈希基 SpGEMM 库（Nsparse、spECK、OpSparse、cuSPARSE）存在固有缺陷：
   - Nsparse：哈希表长度匹配最大非零元素数（NNZ），共享内存利用率较高但哈希冲突率极高（达 400%+）；
   - spECK/OpSparse：通过扩大哈希表（1.5×/2× 需求空间）降低冲突，但共享内存利用率仅 66%/50%，造成资源浪费；
   - 核心矛盾：哈希表大小需在 “降低冲突” 与 “提高共享内存利用率” 之间妥协，难以兼顾。

### 二、HSMU-SpGEMM 的核心设计

#### 1. 新型累加器架构

- 摒弃传统哈希映射，采用 “预排序列索引数组 + 二分查找” 机制（findInSorted 函数），彻底消除哈希冲突；
- 累加器仅分配与实际 NNZ 匹配的存储空间，共享内存利用率达 100%（理想场景），平均达 68%，远超现有库；
- 结合稠密累加器（源自 spECK），通过阈值（4096 个 NNZ / 行）动态选择：稀疏行用二分查找累加器，稠密行用稠密累加器，兼顾不同场景。

#### 2. 符号阶段优化（生成预排序列索引）

- 针对小规模矩阵：通过掩码矩阵（mask B/mask C）的按位或运算，快速提取矩阵 C 的非零元素列索引并排序；
- 针对大规模矩阵：采用压缩掩码格式（类 CSR 结构），结合 OpSparse 的符号阶段优化，降低存储空间开销并提升运算并行性。

#### 3. 整体工作流程

1. 符号阶段：通过掩码矩阵 / 压缩掩码矩阵计算矩阵 C 的 NNZ 和预排序列索引数组；
2. 数值阶段：根据行密度选择累加器，通过二分查找定位中间产物并累加，避免冲突且高效利用共享内存。

### 三、实验结果

#### 1. 硬件与数据集

- 测试平台：3 款不同架构 NVIDIA GPU（Ampere 架构 RTX 3090 Ti、Ada Lovelace 架构 RTX 4080 SUPER、Turing 架构 RTX 1650 SUPER）；
- 数据集：SuiteSparse 矩阵集合中的 338 个稀疏方阵（含 18 个代表性矩阵、4 个超大规模矩阵），覆盖不同稀疏度与规模。

#### 2. 核心性能提升

| GPU 型号       | 相对 Nsparse 加速比（几何均值） | 相对 spECK 加速比 | 相对 OpSparse 加速比 | 相对 cuSPARSE 加速比 |
| -------------- | ------------------------------- | ----------------- | -------------------- | -------------------- |
| RTX 3090 Ti    | 3.19（最高 131.64）             | 1.28              | 2.34                 | 5.06                 |
| RTX 4080 SUPER | 3.08（最高 125.25）             | 1.24              | 2.24                 | 6.92                 |
| RTX 1650 SUPER | 1.41（最高 70.14）              | 1.10              | 1.46                 | 1.73                 |

- 超大规模矩阵场景：HSMU-SpGEMM 避免内存溢出（OpSparse 在部分矩阵上失败），峰值内存仅为 OpSparse 的 1/3~1/2；
- 共享内存利用率：平均 68%，显著高于 spECK（≤66%）和 OpSparse（35.35%）。

#### 3. 适配性与扩展性

- 矩阵适应性：对非零元素紧凑分布的矩阵加速效果最显著（冲突敏感场景），对稀疏分布矩阵仍保持优势；
- 数据中心 GPU 支持：在 NVIDIA L40S 上性能进一步提升，相对 3090 Ti 的加速比高于 OpSparse；
- 迭代场景优化：格式转换开销仅为单次 SpGEMM 的 1/10，适合代数多重网格等迭代计算场景。

### 四、结论与未来方向

1. 核心贡献：突破哈希基方法的固有权衡，实现 “零冲突 + 高共享内存利用率”，在 3 款 GPU 上全面超越 4 类主流库；
2. 未来工作：将 HSMU-SpGEMM 应用于深度学习等实际场景，进一步优化大规模矩阵的符号阶段开销。

### 补充信息

- 代码开源：https://github.com/wuminqaq/HSMU-SpGEMM；
- 实验可复现性：提供完整的数据集下载脚本、编译流程及性能测试工具，支持在兼容 GPU 上复现全部结果。



## [<span id ="hpca2505-1">InstAttention: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference</span>](#hpca2505)

​	提出InstAttention，一种面向长上下文LLM离线推理的新型系统，核心是将解码阶段关键的注意力计算与KV缓存卸载到计算存储驱动器（CSDs），通过设计闪存感知的SparF稀疏注意力机制、双地址映射的KV缓存管理方案及GPU与CSD间的P2PDMA直接传输，充分利用CSD的高内部带宽规避PCIe带宽限制，同时缓解CSD计算能力弱的问题，在NVIDIA A6000 GPU及OPT、Llama-2系列模型上的实验表明，其长序列推理吞吐量较FlexGen等现有SSD卸载方案最高提升11.1倍，为资源受限场景下低成本、高性能的LLM推理提供了有效解决方案。

### 一、研究背景与问题

1. **LLM 推理的核心挑战**：LLM 推理分为预填充（计算密集型）和解码（内存密集型）两个阶段，解码阶段依赖 KV 缓存存储历史令牌的中间数据以避免重复计算，但随着上下文长度和批处理规模扩大，KV 缓存体积激增（如 13B 模型在批大小 32、4K 令牌下需 100GB KV 缓存，是模型本身的 4.2 倍），给 GPU 显存带来巨大压力。
2. **现有解决方案的缺陷**：现有方案（如 DeepSpeed-MII、FlexGen）将 KV 缓存卸载到主机内存或 SSD 以降低成本，但受限于 PCIe 带宽低、存储与 GPU 无直接数据通路、存储软件栈复杂等问题，导致 KV 缓存访问延迟极高，性能严重下降（如 FlexGen 在 KV 缓存卸载到 SSD 时，解码阶段 KV 缓存访问开销占比达 98.94%）。
3. **CSD 的潜力与挑战**：计算存储驱动器（CSDs）集成存储与计算资源，兼具 SSD 的低成本大容量和内部高带宽（数十 GB/s，远超 PCIe 的 3-7GB/s），但存在计算能力弱（比 GPU 低 2-3 个数量级）、与 GPU 带宽差距大、闪存访问模式与内存不兼容等问题，无法直接承接完整推理任务。

### 二、InstAttention 的核心设计

InstAttention 构建 GPU-CSD 异构系统，仅将解码阶段性能关键的注意力计算和 KV 缓存卸载到 CSD，其余任务由 GPU 执行，通过软硬件协同优化解决存储、带宽和计算适配问题。

#### 1. 任务划分与系统架构

- **三大硬件组件**：
  - InstCSD：存储 KV 缓存，执行解码阶段注意力计算；
  - InstGPU：负责预填充阶段全量计算、解码阶段非注意力计算（如 QKV 投影、FFN）；
  - InstHost：调度任务、协调 GPU 与 CSD 的数据传输，作为控制平面。
- **数据传输优化**：采用 PCIe 对等 DMA（P2PDMA）直接连接 GPU 与 CSD，绕过主机内存，减少数据拷贝；预填充阶段采用分层流水线传输 KV 缓存，与计算重叠以隐藏延迟。

#### 2. 闪存感知的稀疏注意力机制（SparF）

- 基于 SparQ 算法优化，通过 “双步筛选” 适配闪存页访问粒度：先按通道 / 令牌组（与闪存页大小对齐）筛选有效数据，再在组内筛选关键令牌 / 通道，减少无效访问和带宽浪费；
- 在保证精度损失可忽略（压缩比 1/8 时准确率约 95%）的前提下，降低计算强度和 KV 缓存需求，缓解 CSD 计算压力。

#### 3. KV 缓存管理与 FTL 设计

- **双地址映射**：针对令牌索引和通道索引的 KV 缓存访问需求，设计两套地址映射机制，将 KV 缓存按闪存页大小分组，跨多个闪存块和芯片分布，利用闪存并行性；
- **批处理写入**：将 GPU 生成的 KV 缓存按注意力头批量写入 CSD，填充闪存块以避免写放大；
- **简化垃圾回收（GC）**：利用 KV 缓存的顺序写入特性，仅在 CSD 空闲时按 LRU 策略清理过期数据，无数据碎片， overhead 极低。

#### 4. 存储内注意力引擎硬件实现

在 CSD 的 FPGA 上实现 SparF 注意力引擎，通过细粒度并行设计隐藏闪存访问延迟，集成 GeMV、Softmax 等计算单元，适配稀疏数据处理，最大化 CSD 计算效率。

### 三、实验结果

基于 NVIDIA A6000 GPU、Zynq7045 FPGA-based CSD，在 OPT-13B/30B、Llama-2-13B 模型上进行测试，核心结果如下：

1. **吞吐量提升**：
   - 单 CSD 场景下，InstAttention（含 SparF）比 FlexGen 吞吐量最高提升 11.1×（13B 模型，批大小 256）；
   - 双 CSD 场景下，InstA-SparF 比 FlexGen-SparQ 吞吐量提升 3.11×，且支持更大批处理规模（如 13B 模型批大小可达 256，远超 FlexGen 的 64）；
   - 对 Llama-2-13B（4K 上下文），吞吐量比 FlexGen 最高提升 12.48×。
2. **性能瓶颈缓解**：解码阶段 KV 缓存访问开销占比从 FlexGen 的 98.9% 降至 InstA 的 80.7%（ dense 模式）和 74.0%（稀疏模式）。
3. **扩展性与实用性**：
   - 支持多 CSD 扩展，20 个 CSD 可使密集 / 稀疏推理吞吐量分别提升 8.99× 和 7.29×；
   - 闪存耐久性优化：通过简化 FTL 和保留时间松弛，可支持 5 年服务（如 4 个 CSD 可满足 920 名高负载用户需求），性能下降控制在 0.47‰以内。

### 四、核心贡献

1. 首次提出基于 CSD 的 LLM 推理系统，将解码阶段注意力计算与 KV 缓存卸载到 CSD，有效缓解 PCIe 带宽瓶颈，KV 缓存迁移开销降低 94.0%；
2. 软硬件协同设计闪存感知的稀疏注意力引擎和 KV 缓存管理机制，适配 CSD 的计算与存储特性；
3. 实验验证在长上下文、大批次离线推理场景下，性能远超现有 SSD 卸载方案，且成本可控、扩展性强。

### 五、结论

InstAttention 通过存储内计算卸载和软硬件协同优化，在资源受限场景下实现了低成本、高性能的长上下文 LLM 离线推理，比现有 SSD-based 方案吞吐量提升最高 11.1×，为解决 KV 缓存的存储与带宽难题提供了新型异构架构方案。



## [<span id ="hpca2506-1">LAD: Efficient Accelerator for Generative Inference of LLM with Locality Aware Decoding</span>](#hpca2506)

​	本文提出了一种名为LAD（Locality Aware Decoding）的LLM生成推理加速器，通过挖掘注意力分数在多解码步骤中的数值局部性，采用算法-硬件协同设计，将历史解码信息存储在固定大小的中间缓存中，仅对注意力分数超出主导区间的“活跃位置”访问KV缓存进行修正，在保证与原始模型平均97% ROUGE-1相似度的前提下，大幅减少KV缓存访问开销；当KV缓存长度超2048时，其高端配置相对A100 GPU的注意力机制平均提速10.7倍、能效提升52.4倍，端到端推理平均提速2.3倍、能效提升13.4倍，有效解决了LLM长序列生成中KV缓存访问导致的性能瓶颈。

### 一、研究背景与问题

1. **LLM 生成推理的核心瓶颈**：大语言模型（LLM）基于自回归解码，注意力机制需通过 KV 缓存存储全部生成历史（键和值），但随着序列长度增长，KV 缓存规模持续扩大，导致内存访问成为性能瓶颈。当 KV 缓存长度达 4096 时，注意力机制的推理耗时占比超 50%（如 LLaMA2 7B 模型）。
2. **现有方案的局限**：架构优化无法有效减少 KV 缓存访问，模型压缩会牺牲精度且需重训练，投机解码依赖小模型精度并引入额外计算开销，均未能彻底解决问题。

### 二、核心洞察与创新思路

1. **注意力分数的数值局部性**：分析发现，多个解码步骤中，多数位置的注意力分数集中在特定数值区间（数值局部性），top-1 区间命中率超 74%，top-1+top-2 命中率超 95%，且长序列下局部性更显著（LLaMA2 7B 在 KV 长度 4096 时 top-1 命中率超 90%）。
2. **核心创新**：利用数值局部性，将历史解码信息存储在固定大小的中间缓存中，无需每次解码都访问完整 KV 缓存，仅对注意力分数超出其主导区间的 “活跃位置” 进行 KV 缓存访问与修正，在不损失精度的前提下减少内存访问。

### 三、LAD 的关键设计

#### （一）算法层面

1. **分段线性近似**：将注意力分数的指数函数（softmax 核心）拆分为非均匀分段线性函数，近 0 区间更短以适配函数斜率变化，通过最小二乘优化获取线性系数，引入误差小于 10⁻⁶。
2. **局部性感知解码流程**：
   - 维护每个位置的 “主导区间”（注意力分数最常落入的区间）及对应线性系数；
   - 解码时先通过中间缓存快速计算注意力结果，再对活跃位置访问 KV 缓存进行修正；
   - 中间缓存动态更新：仅当位置主导区间变化时，利用活跃位置的系数差异更新缓存，无额外 KV 访问开销。
3. **高效活跃位置识别**：通过提取键的方向中心，近似计算注意力分数以判断区间归属，避免完整键的内存访问，同时对高权重位置计算精确分数以保证精度。

#### （二）硬件架构层面

1. **整体架构**：包含多个 LAD Tile，每个 Tile 集成注意力计算模块、特殊函数模块（SFM）、7 个向量处理单元（VPU）和片上 SRAM，所有 Tile 共享 HBM（高带宽内存）。
2. **核心模块**：
   - 高效注意力分数模块（EAS）：计算近似注意力分数、更新键中心；
   - 活跃位置识别模块（APID）：判断注意力分数是否超出主导区间，标记活跃位置；
   - 模式差异模块（MD）：计算活跃位置精确分数、更新主导区间及系数差异；
   - 注意力计算模块（AC）：基于中间缓存计算注意力结果、修正活跃位置、更新中间缓存。
3. **流水线与调度优化**：设计 6 级流水线平衡 HBM 访问与计算，通过预取利用活跃位置的时间局部性（连续解码步骤活跃位置重叠率超 80%），进一步减少 KV 访问；QKV 层与注意力层交错调度，掩盖内存延迟。

### 四、实验结果

#### （一）精度验证

- 与原始模型生成序列的 ROUGE-1 相似度平均达 97%，显著优于 Qserve（量化方案）和 H2O（KV 缓存剪枝）；
- 在 Wikitext2、OpenBookQA 等数据集上，困惑度（ppl）与准确率（acc）完全匹配原始模型，无精度损失。

#### （二）性能与能效

- **注意力层**：KV 缓存长度超 2048 时，LAD-3.5（3.5MB SRAM 配置）相对 A100 GPU 平均提速 10.7 倍，能效提升 52.4 倍；
- **端到端推理**：KV 缓存长度超 2048 时，平均提速 2.3 倍，能效提升 13.4 倍；
- 长序列优势更显著：KV 长度 4096 时，LLaMA2 13B 的注意力 latency 占比仅增长 3%，有效缓解长序列瓶颈。

#### （三）配置对比

- 三种 SRAM 配置（1.5MB/2.5MB/3.5MB）中，LAD-2.5 和 LAD-3.5 在长序列下性能接近，均显著优于小容量配置，SRAM 主要通过预取提升活跃位置命中率。

### 五、与相关工作对比

- 相比 KV 缓存剪枝（如 H2O）、量化（如 Qserve），LAD 不丢弃 KV 信息、不引入量化误差，精度更高；
- 相比投机解码、非自回归解码，LAD 无需额外模型或修改自回归本质，适配性更强；
- 相比 GPU 优化（如 vLLM），LAD 通过硬件定制化适配不规则访问模式，性能与能效优势显著。

### 六、结论

LAD 通过挖掘注意力分数的数值局部性，结合算法 - 硬件协同设计，在不损失 LLM 生成精度的前提下，大幅减少 KV 缓存访问，解决了长序列生成的内存瓶颈。在主流 LLM（OPT 2.7B/6.7B、LLaMA2 7B/13B）上的实验表明，其相对 A100 GPU 实现了 2.3 倍端到端提速和 13.4 倍能效提升，为大模型长序列生成提供了高效解决方案。





## [<span id ="hpca2507-1">M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type</span>](#hpca2507)

​	该研究针对大型语言模型（LLMs）部署中内存占用大、推理延迟高的问题，提出了基于数学自适应数值类型（MANT）的高效低比特组量化方案，通过灵活的数学编码范式（调整系数a适配不同数据分布）、编解码与计算融合机制，以及针对KV缓存动态特性设计的空间-时间二维实时量化策略，同时优化硬件架构（扩展脉动阵列处理单元、新增实时量化单元），在LLaMA、OPT等系列模型上实现了近FP16的精度（PPL损失低于0.3），相比现有SOTA加速器平均提速2.99倍（最高4.46倍）、能耗降低2.81倍（最高4.10倍），且硬件开销可忽略，有效解决了组量化中组内分布多样性、KV缓存实时量化等核心挑战。

### 一、研究背景与挑战

1. **LLMs 部署痛点**：LLMs（如 Llama3-405B）参数规模庞大，内存需求远超现有硬件（如 H100 GPU 仅 80GB 内存），且自回归推理导致计算资源利用率低，内存带宽成为瓶颈。
2. **现有量化方案的局限**：
   - 组量化已成为主流方案，但现有自适应数据类型（如 ANT、GOBO）难以适配组内数据分布的高多样性（不同组分布差异显著，而张量 / 通道级分布相近）。
   - 基于数据类型的自适应方法（如 ANT）适配性有限，扩展数据类型会增加硬件开销；基于聚类的方法（如 GOBO）虽适配性强，但编码和计算效率低。
   - 动态生成的 KV 缓存（占长序列推理内存的 70% 以上）难以实现实时组量化，现有方案多回避该问题。

### 二、核心方案：MANT（数学自适应数值类型）

#### 1. 核心设计理念

通过灵活的数学编码范式，支持 “无限” 连续的数据分布变体，同时融合解码与计算过程，兼顾适配性与效率。

- **映射表示**：定义量化网格公式 `Value grid = ±(a×|INT|+2^|INT|)`，通过调整系数`a`（0-128，8 比特编码）适配不同分布（如`a=0`匹配 PoT 类型，`a=17`近似 Float，`a=25`近似 NF 类型）。
- **编解码与计算融合**：权重离线编码（选择最优`a`最小化 MSE），激活采用 INT8 量化；解码过程与矩阵乘法融合，无需专用解码器，通过整数乘法和移位操作完成计算，降低开销。

#### 2. KV 缓存实时量化机制

针对 KV 缓存动态生成的特性，提出空间 - 时间二维量化策略：

- **K 缓存（空间量化）**：组内元素同步生成，分配专用量化单元实时处理。
- **V 缓存（时间量化）**：组内元素分迭代生成，采用两阶段量化（先 INT8 临时存储，积累满一组后通过方差选择`a`，再量化为 4 比特 MANT），平衡实时性与精度。

#### 3. 硬件架构优化

- 基于脉动阵列扩展处理单元（PE），集成 MAC（乘加）和 SAC（移位累加）组件，支持混合精度计算（2/4/8 比特权重 / KV 缓存与 8 比特激活）。
- 新增实时量化单元（RQU），通过流水线隐藏量化延迟，支持空间 / 时间数据流模式。

### 三、实验结果

1. **精度表现**：在 LLaMA、OPT 系列模型上，MANT（4 比特权重 + 8 比特激活 + 4 比特 KV 缓存）的困惑度（PPL）接近 FP16，精度损失低于 0.3；生成任务（TruthfulQA、TriviaQA）中，BLEU/F1 分数损失低于 1.7%。
2. **性能与能耗**：
   - 相比现有 SOTA 加速器（Tender、OliVe、ANT），平均提速 2.99 倍（最高 4.46 倍），平均能耗降低 2.81 倍（最高 4.10 倍）。
   - 长序列（128K）推理中，因 KV 缓存量化的优势，性能提升更为显著，远超未量化 KV 缓存的基线方案。
3. **硬件开销**：新增组件（RQU、PE 扩展）的面积开销可忽略，与现有架构兼容性强。

### 四、核心贡献

1. 提出 MANT 数值类型，首次实现组级细粒度自适应量化，兼顾分布适配性与计算效率。
2. 设计 KV 缓存实时量化框架，解决动态生成缓存的低比特量化难题。
3. 硬件 - 算法协同优化，实现权重、激活、KV 缓存的统一量化，在精度损失极小的前提下，大幅提升 LLMs 推理的速度与能效。





## [<span id ="hpca2508-1">throttLL’eM: Predictive GPU Throttling for Energy Efficient LLM Inference Serving</span>](#hpca2508)
​	该文档提出了名为throttLL’eM的LLM推理服务框架，针对LLM推理能耗高且需满足低延迟SLO的核心矛盾，通过生成长度预测、KV缓存与批大小预测、基于梯度提升决策树的性能预测模型、查询调度与准入控制、张量并行自动扩缩容及GPU频率节流控制等模块，动态适配LLM自回归特性、动态批处理和可变内存占用等挑战，在AWS p4d.24xlarge等环境下基于Llama系列及Mixtral 8x7B模型的实验表明，该框架相比NVIDIA Triton服务器，在满足p99分位响应延迟和TBT等SLO的前提下，最高可降低43.8%的能耗，能效提升至少1.71×，且在不同模型、硬件配置和负载场景下均表现稳定，为LLM推理服务提供了高效的能效优化方案。

### 一、研究背景与挑战

1. **LLM 推理的能效问题**：LLM 依赖高功耗 GPU，推理阶段占数据中心计算周期的 90% 以上，能源消耗巨大（单条 GPT-4 响应约 3Wh），2026 年相关能耗预计达 1050 太瓦时，兼具环境与成本压力。
2. **核心矛盾**：降低能耗与保障低延迟 SLO（如 99 分位响应延迟、符合人类阅读速率的令牌间隔时间 TBT）存在冲突。
3. **三大技术障碍**：
   - 自回归特性：令牌生成长度动态变化，难以预分配资源；
   - 动态批处理：批大小波动导致性能变异（延迟最高增加 45%）；
   - 可变内存占用：KV 缓存随序列长度增长动态变化，使用率提升可导致 18.2% 的性能下降。
4. **传统方案局限**：静态功率限制、超分配置、延迟批处理等技术无法适配 LLM 的动态特性，“竞速至空闲” 策略因持续请求流失效。

### 二、throttLL’eM 框架设计

框架核心是通过动态 GPU 频率调节和实例自动扩缩容，在满足 SLO 的前提下最小化能耗，关键模块包括：

1. **生成长度预测**：估算输入查询的输出令牌长度，为后续资源预测奠定基础（现有方案误差约 30%，框架通过保守调整缓解误差影响）。
2. **KV 缓存与批大小预测**：基于生成长度预测和调度元数据（如查询调度迭代、输入长度），通过解析模型预测未来各迭代的 KV 缓存使用率和批大小，平均误差分别为 2.26% 和 0.19%。
3. **性能预测模型**：采用梯度提升决策树（GBDT），输入引擎规模、批大小、KV 缓存使用率和 GPU 频率，预测迭代级吞吐量（IPS），R² 分数超 0.97，平均误差低于 1 IPS。
4. **查询调度与准入控制**：基于上述预测验证 SLO 合规性（KV 缓存容量、TBT、端到端延迟 E2E），避免违规查询直接调度，未合规查询进入队列等待。
5. **自动扩缩容机制**：基于请求率（RPS）动态调整张量并行（TP）级别，采用 “影子实例” 掩盖服务启动延迟（>20s），通过宽限期策略平衡扩缩容与能耗。
6. **GPU 频率节流控制器**：通过二分查找确定满足 SLO 的最小 GPU 频率，统一应用于所有服务 GPU，避免同步问题；对 SLO 风险查询采用最大频率。

### 三、关键实验发现

1. **性能 - 能效权衡**：批大小增大提升吞吐量和能效，但恶化延迟；GPU 频率存在 “能效甜点”（如 1050MHz），低于该频率会因吞吐量下降抵消能耗收益。
2. **KV 缓存影响**：KV 缓存使用率与 TBT 正相关（皮尔逊系数 0.92）、与吞吐量负相关（-0.92），可作为性能建模的有效代理指标。
3. **并行策略优劣**：张量并行（TP）在吞吐量和能效上显著优于数据并行（DDP）和流水线并行（PP），小并行度（如 TP2）比大并行度（如 TP4）更节能。
4. **工作负载特性**：LLM 查询的令牌长度（输入 0-4000 tokens、输出 10-700 tokens）和请求到达率（RPS 1-16）具有高波动性，需动态资源分配。

### 四、实验结果

1. **与 NVIDIA Triton 对比**：
   - 无自动扩缩容时，平均节能 24.7%，最高 30.7%，能效提升 30%-36.3%；
   - 启用自动扩缩容后，最高节能 43.8%（30% 预测误差下仍达 41.7%），能效提升 1.71×-1.78×；
   - 所有配置均满足 SLO（TBT 平均 < 50ms，E2E 99 分位延迟低于阈值）。
2. **与其他方案对比**：
   - 比 ReTail 等 DVFS 方案节能 11.9%， latency 预测更精准（R² 0.97 vs 0.4）；
   - 在 Mixtral 8x7B 模型 + NVIDIA A10G GPU 环境中，仍实现 35.9% 的功耗降低，且满足 SLO。
3. **额外收益**：频率节流间接增大批大小（几何平均提升 24.5%），减少迭代次数（降低 24.3%），进一步优化能效。

### 五、结论与局限

1. 核心贡献：提出基于预测模型的细粒度 GPU 频率调节与自动扩缩容结合方案，首次系统分析 LLM 推理的能效关键因素，实现 SLO 约束下的大幅节能。
2. 局限：未优化计算密集型的预填充阶段；跨节点扩展需结合流水线并行；生成长度预测误差仍可能影响性能。
3. 代码开源：https://github.com/WilliamBlaskowicz/throttLL-eM



## [<span id ="hpca2509-1">VQ-LLM: High-performance Code Generation for Vector Quantization Augmented LLM Inference</span>](#hpca2509)

​	VQ-LLM 是一款面向向量量化（VQ）增强型大语言模型（LLM）推理的高性能融合内核生成框架，旨在解决现有 VQ 算法虽能实现更高压缩比和精度却难以转化为实际 latency 提升的问题，核心通过“码本缓存”（基于访问频率在 GPU 多存储层级自适应分配码本条目，减少银行冲突与资源浪费）和“码本中心计算引擎”（含码本中心数据流与层级融合技术，优化码本加载与计算协同、解决数据布局不匹配问题），结合自适应启发式策略适配 VQ 算法与 LLM 计算内核的多样性，最终实现对现有开源实现 64.36%~99.1% 的 latency 降低，在 4 位等效精度下 latency 与主流逐元素量化方法（如 AWQ、QoQ）相当甚至更优，且准确率更高，端到端推理中能实现 2.2 倍提速，大幅降低内存占用，在带宽受限环境中表现尤为突出。

### 一、研究背景与挑战

1. **LLM 推理的内存瓶颈**：LLM（如 Llama 系列）的权重和 KV 缓存占内存足迹的 95% 以上，量化是关键优化手段。传统逐元素量化（如 AWQ、QoQ）受限于 4 位精度（进一步压缩会导致精度暴跌），而**向量量化（VQ）** 以向量为压缩单元，可在 2 位甚至 1 位精度下保持高准确率，且压缩比更高。
2. **VQ 的核心痛点**：现有 VQ 集成方案未能将内存节省转化为实际 latency 提升，反而因以下问题导致性能劣于 FP16 基线：
   - 码本访问低效：全量码本存入共享内存会导致线程块并发度下降、银行冲突严重；存入全局内存则访问延迟极高。
   - 码本加载与计算不协调：多线程块重复加载码本导致全局内存流量激增，且解量化数据布局与后续计算需求不匹配，引发额外共享内存 - 寄存器数据搬运。
   - 多样性适配困难：VQ 算法（向量大小、码本条目数、残差次数等参数不同）与 LLM 计算内核（GeMM、GeMV、注意力机制）组合多样，手动优化难以覆盖所有场景。

### 二、核心设计：VQ-LLM 框架

VQ-LLM 通过 “码本缓存” 和 “码本中心计算引擎” 两大核心组件，结合自适应启发式策略，解决上述挑战：

#### 1. 码本缓存（Codebook Cache）

- 核心思想：基于码本条目的访问频率，在 GPU 存储层级（全局内存、共享内存、寄存器）中自适应分配，而非全量存入单一内存。
  - 冷条目（低访问频率）：存于全局内存，节省共享内存空间。
  - 中热条目（中等访问频率）：存于共享内存，平衡访问速度与并发度。
  - 极热条目（高访问频率）：存于线程本地寄存器，彻底消除银行冲突。
- 自适应优化：通过离线 profiling 识别 GPU 资源冗余（共享内存 / 寄存器 slack），动态调整各层级码本条目数量，不影响原有计算的并发度。

#### 2. 码本中心计算引擎

围绕码本缓存设计，优化内存流量与计算协同：

- （1）码本中心数据流（Codebook-Centric Dataflow）
  - 按码本切换维度拆分计算任务，使每个线程块仅加载一次对应码本，避免重复加载导致的全局内存流量浪费。
  - 自适应调整拆分因子，平衡全局归约开销与码本访问效率。
- （2）码本中心层级融合（Codebook-Centric Hierarchical Fusion）
  - 寄存器级融合：利用 GPU 的 intra-warp 数据交换指令（shfl_xor），直接在寄存器中重排解量化数据，适配后续计算布局，跳过共享内存中间环节。
  - 共享内存级融合：针对需多次洗牌（shuffle）的场景，保留共享内存融合，自适应选择融合层级。

#### 3. 自适应启发式策略

根据 VQ 配置（向量大小、码本条目数）、计算内核类型（GeMM/GeMV/ 注意力）和 GPU 硬件特性，自动优化：

- 码本缓存的层级分配边界（n_reg、n_shared）。
- 计算任务的拆分因子。
- 融合层级（寄存器 / 共享内存）。

### 三、实验结果

1. **相对现有 VQ 实现**：在 RTX 4090/Tesla A40 GPU 上，VQ-LLM 对 GeMM、GeMV、注意力等内核的 latency 降低 64.36%~99.1%，平均降低 46.13%，最高提速 2.2 倍。
2. **与逐元素量化对比**：在 4 位等效精度下，VQ-LLM 的 latency 与 AWQ（GeMM/GeMV）、QoQ（注意力）相当甚至更优（如 GeMV 达 0.88×），且准确率更高（arc-challenge 任务提升 2.5%）。
3. **端到端性能**：对 Llama-7B/65B 模型，VQ-LLM 在 4 位精度下实现 2.2× 提速，内存占用从 22GB 降至 6GB 以下；2 位精度下仍保持实用准确率，且在带宽受限场景（如 Tesla A40）提速更显著。
4. **优化分解验证**：码本缓存解决了共享内存占用与银行冲突问题，计算引擎消除了重复码本加载与布局不匹配的额外开销，两者协同贡献主要性能提升。

### 四、核心贡献

1. 首次深入分析 VQ 在 LLM 推理中的性能瓶颈，明确码本访问低效与计算协同不足是关键问题。
2. 提出码本缓存抽象，实现码本在 GPU 存储层级的自适应分配，平衡访问速度与硬件利用率。
3. 设计码本中心计算引擎，通过数据流拆分与层级融合，优化内存流量与数据布局适配。
4. 构建自动化内核生成框架，支持多样 VQ 配置与 LLM 计算内核，验证了 VQ 在 LLM 推理中 “高压缩比 + 高准确率 + 低 latency” 的可行性。