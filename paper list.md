[<span id ="asplos">ASPLOS</span>](#asplos-title)

[<span id ="hpca">HPCA</span>](#hpca-title)

[<span id ="mlsys">MLsys</span>](#mlsys-title)

[<span id ="sosp">SOSP</span>](#sosp-title)

[<span id ="osdi">OSDI</span>](#osdi-title)

[<span id ="anchor">anchor</span>](#anchor-title)

#  [<span id ="asplos-title"> ASPLOS</span>](#asplos)

## 2025

<span id ="asplos2501">[vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention](#asplos2501-1)</span>

<span id ="asplos2502">[Accelerating LLM Serving for Multi-turn Dialogues with Efficient Resource Management](#asplos2502-1)</span>

<span id ="asplos2503">[M5: Mastering Page Migration and Memory  Management for CXL-based Tiered Memory Systems](#asplos2503-1)</span>

<span id ="asplos2504">[MoE-Lightning: High-Throughput MoE Inference on  Memory-constrained GPUs](#asplos2504-1)</span>

[<span id ="asplos2505">CXLfork: Fast Remote Fork over CXL Fabrics</span>](#asplos2505-1)

[<span id ="asplos2506">PIM Is All You Need: A CXL-Enabled GPU-Free System  for Large Language Model Inference</span>](#asplos2506-1)

[<span id ="asplos2507">PAPI: Exploiting Dynamic Parallelism  in Large Language Model Decoding with a  Processing-In-Memory-Enabled Computing System</span>](#asplos2507-1)

[<span id ="asplos2508">POD-Attention: Unlocking Full Prefill-Decode  Overlap for Faster LLM Inference</span>](#asplos2508-1)

## [<span id ="asplos2501-1">vAttention: Dynamic Memory Management for  Serving LLMs without PagedAttention</span>](#asplos2501)

        本文提出针对 LLM 服务的动态内存管理方案 vAttention，旨在解决 PagedAttention 需重写内核、开销高、可移植性差的问题。其核心是通过 CUDA VMM API 解耦虚拟与物理内存分配，提前预留连续虚拟内存缓冲区以保留 KV 缓存虚拟连续性，同时按需映射物理内存缓解碎片；并通过内存分配与计算重叠、延迟回收、小页面支持等优化，降低运行时延迟。实验表明，在 Yi-6B 等模型上，vAttention 预填充阶段比 FlashAttention-2/FlashInfer 的 Paged 版本快 1.17-1.36 倍，端到端离线吞吐量提升 1.13-1.23 倍，还能无缝支持 FlashAttention-3 等新内核，兼顾性能、简洁性与可移植性。

### 一、研究背景与现有方案痛点

LLM 服务中，KV 缓存占 GPU 推理内存的主要部分，其动态增长特性（单请求逐 token 扩展、总长度未知）导致内存分配难题：

1. **静态分配**（如 Orca、FasterTransformer）按模型最大上下文长度预留内存，引发严重内部碎片，限制批处理大小和吞吐量；
2. **PagedAttention 方案**（vLLM 提出，被 TensorRT-LLM 等广泛采用）通过按需分配小内存块缓解碎片，但存在根本缺陷：
   - 需重写注意力内核以支持非连续 KV 缓存访问，难以跟进最新优化（如 vLLM 的 Paged 内核比 FlashAttention-2 慢 2.8 倍）；
   - 服务框架需额外实现内存管理，重复 OS 的虚实地址转换功能，增加冗余；
   - 运行时开销显著：GPU 侧因块表查询、寄存器溢出等慢 37%（FlashAttention-2 预填充阶段）至 42%（FlashInfer 预填充阶段），CPU 侧因块表准备等增加 10%-30% 延迟；
   - 可移植性差，新内核（如 FlashAttention-3）发布时无 PagedAttention 支持。

### 二、vAttention 核心设计

核心思路：**保留 KV 缓存的虚拟内存连续性，同时动态分配物理内存**，通过 CUDA 虚拟内存管理（VMM）API 解耦虚实内存分配，避免 PagedAttention 的非连续布局缺陷。

1. **内存分配策略**
   
   - 虚拟内存：提前预留超大连续缓冲区（按最大批处理大小和模型最大上下文长度配置），利用 64 位系统充足的虚拟地址空间（单进程可达 128TB），无需担心虚拟内存碎片；
   - 物理内存：运行时按需分配，仅在请求需要时将物理页面映射到虚拟缓冲区，避免提前占用。

2. **关键技术细节**
   
   - 支持多粒度页面：修改开源 CUDA 统一内存驱动，新增 64KB/128KB/256KB 小页面支持（默认 CUDA VMM 仅支持 2MB 大页面），降低物理内存碎片；
   - 请求级 KV 缓存索引：通过唯一 reqId 定位批处理中每个请求的 KV 缓存子张量，确保地址访问连续性；
   - 兼容现有框架：作为 Python 库集成到 vLLM 等服务框架，提供 init/alloc_reqid/free_reqid/step 等简洁 API，无需修改模型或注意力内核。

3. **针对性优化**
   
   - 隐藏分配延迟：利用解码阶段内存需求的可预测性，通过后台线程将内存分配与计算重叠；预填充阶段采用延迟回收 + 预分配策略，复用已释放的物理页面；
   - 缓解碎片：小页面支持使分配粒度匹配 KV 缓存增长特性（单 token 仅需数十 KB），且无 TLB 抖动风险；
   - 支持连续批处理：借助 FlashAttention 的 cache_batch_idx API，处理请求退出后的虚拟内存 "空洞"，保持批处理灵活性。

### 三、实验结果

基于 Yi-6B/Llama-3-8B/Yi-34B 模型，在 A100（单卡 / 双卡 NVLink）和 H100 GPU 上的测试表明：

1. **性能优势**
   
   - 预填充阶段：长上下文（192K）下，vAttention 比 FlashAttention-2 的 Paged 版本快 1.24-1.26 倍，比 FlashInfer 的 Paged 版本快 1.17-1.36 倍；
   - 解码阶段：与 FlashAttention-2 的 Paged 版本性能相当，比 vLLM 的 Paged 内核快 1.53-1.99 倍，比 FlashInfer 的 Paged 版本快 1.23 倍；
   - 端到端吞吐量：离线长上下文任务（arXiv 摘要）中，比 FlashAttention-2 Paged 快 1.13-1.18 倍，比 FlashInfer Paged 快 1.14-1.23 倍；在线场景下中位数延迟降低 28%-42%。

2. **可移植性**
   
   - 无需修改代码即可支持 FlashAttention-3（Hopper 架构优化内核），在 H100 上比 FlashAttention-2 Paged 版本吞吐量提升 1.26-1.5 倍。

3. **资源效率**
   
   - 小页面（64KB）使最大批处理大小提升 1.18-1.28 倍；内存分配带宽达 7.6GB/s，远超 LLM 推理需求（750MB/s）。

### 四、核心贡献

1. 提出虚实内存解耦的 KV 缓存管理方案，兼顾连续性（无内核修改）和动态性（无碎片）；
2. 解决 CUDA VMM 的延迟和大页面碎片问题，提供 LLM 专用优化；
3. 实现简单、可移植、高性能的替代方案，支持现有主流注意力内核（FlashAttention-2/3、FlashInfer），降低 LLM 服务的部署和维护成本。
   
    

## [<span id ="asplos2502-1">Accelerating LLM Serving for Multi-turn Dialogues  with Efficient Resource Management</span>](#asplos2502)

        本文针对现有 LLM 服务框架在处理多轮对话时存在的历史注意力键值对（KVs）重计算开销大、FCFS 调度导致 GPU 内存利用率低（头阻塞）两大问题，提出了名为 FlashGen 的解决方案：通过设计包含 GPU、CPU 内存和 SSD 的多级 KV 缓存（FlashGen-Cache），动态选择缓存恢复与重计算以减少冗余计算，同时采用请求重排序调度（FlashGen-Sched），在优先调度可运行短请求提升内存利用率的同时，通过抢占机制避免长请求饥饿；基于 Azure 实例（双 A100 GPU 等配置），在 OPT、Llama-2 系列模型及 ShareGPT 等数据集上的实验表明，FlashGen 在相似延迟下，对 OPT 30B 和 Llama-2 70B 的吞吐量分别提升 1.63 倍和 2.85 倍，显著优化了多轮对话场景下的 LLM 服务性能

### 一、研究背景与核心问题

1. **多轮对话的 LLM 服务挑战**：随着 LLM 在聊天机器人等场景的广泛应用，长上下文（如多轮对话）处理需求激增，但现有框架（如 vLLM、TensorRT-LLM）存在两大关键效率问题：
   
   - **KV 重计算开销**：多轮对话中，用户查询会包含历史对话内容，导致提示词长度 “放大”，现有框架因 GPU 内存有限无法缓存所有历史注意力键值对（KVs），需重复计算，耗费大量资源。
   - **GPU 内存利用率低**：采用先到先服务（FCFS）调度策略时，长提示词请求会阻塞后续短请求，导致 GPU 内存闲置（头阻塞问题），尤其在高负载下缓存竞争加剧，利用率进一步下降。

2. **数据支撑**：基于 ShareGPT 真实对话数据集的分析显示，对话会话平均包含 7 轮、中位数 3 轮，多轮对话的提示词长度较单轮增长 99 倍，历史对话内容占总输入 tokens 的一半以上，验证了 “提示词放大” 问题的严重性。

### 二、核心解决方案：FlashGen

FlashGen 通过**多级 KV 缓存管理**和**请求重排序调度**两大核心技术，高效利用 GPU、CPU（DRAM）和 SSD 资源，解决上述问题。

#### 1. 多级 KV 缓存（FlashGen-Cache）

- **设计目标**：避免历史 KVs 重复计算，通过多级存储分层缓存，平衡内存成本与访问 latency。
- **缓存层级**：
  - 一级缓存（GPU 内存）：缓存当前运行请求的 KVs 及已完成请求的可回收 KVs，优先命中以减少传输开销。
  - 二级缓存（CPU 内存）：异步复制 GPU 生成的 KVs，GPU 内存不足时可快速恢复，通过流水线技术重叠 KV 传输与模型计算，隐藏延迟。
  - 三级缓存（SSD）：当 CPU 内存不足以存储所有历史 KVs 时，异步归档不常用 KVs；通过 CPU 内存 “预加载” 机制，避免直接从 SSD 读取的高延迟，必要时动态选择 “重计算” 而非 “SSD 读取” 以优化性能。
- **关键优化**：批量感知 KV 恢复、主动缓存策略（生成时即复制到 CPU），减少内存回收与传输开销。

#### 2. 请求重排序调度（FlashGen-Sched）

- **设计目标**：解决头阻塞问题，提升 GPU 内存利用率，同时保证请求公平性。
- **核心策略**：
  - 贪心重排序：当队列头部的长请求因内存不足无法执行时，优先调度后续可放入空闲内存的短请求（“提升请求”），避免内存闲置。
  - 无饥饿机制：实时跟踪 GPU 内存使用，当空闲内存 + 提升请求占用内存足以容纳被阻塞的长请求时，抢占提升请求，优先执行长请求，避免其饥饿。
- **效果**：GPU 内存利用率从 vLLM 的 88% 提升至 98% 以上，批量请求规模平均增加 1.06~1.15 倍。

### 三、实验验证与结果

1. **实验环境**：Azure 实例（2×A100 GPU、440GB CPU 内存、2×960GB NVMe SSD），模型包括 OPT（13B/30B/66B/175B）、Llama-2（13B/70B），数据集涵盖 ShareGPT（多轮对话）、Alpaca、HumanEval。

2. **核心性能指标**：
   
   - 吞吐量：在相似延迟下，FlashGen 对 OPT 30B 和 Llama-2 70B 的吞吐量分别提升 1.63 倍和 2.85 倍。
   - 延迟：P95 首 token 延迟（TTFT）较 vLLM 降低 77%（OPT 30B）和 66%（Llama-2 13B）；单 token 生成延迟（TPOT）的 P99 值从 vLLM 的 608ms 降至 103ms（OPT 30B）。
   - 缓存效果：GPU+CPU+SSD 三级缓存的 KV 命中率显著高于单一层级，高负载下仍能维持稳定命中，减少重计算比例。

3. **对比基准**：优于 vLLM（基线）和 CachedAttention（同类 KV 缓存方案），尤其在高负载、长上下文场景下，因动态调度与多级缓存的协同优化，性能优势更明显。

### 四、相关工作与结论

1. **相关工作对比**：
   
   - KV 复用：CachedAttention 仅支持 CPU/SSD 缓存，未动态选择重计算；SGLang 仅依赖 GPU 缓存，不支持异构存储。
   - 调度优化：现有迭代级调度未解决多轮对话的头阻塞问题；Sarathi-Serve 聚焦长提示词拆分，不涉及请求重排序。
   - 内存优化：PagedAttention 优化内存分配，但未解决历史 KV 缓存与调度协同问题。

2. **研究结论**：
   
   - FlashGen 通过多级 KV 缓存和请求重排序的协同设计，有效解决了多轮对话中 “KV 重计算” 和 “GPU 内存闲置” 两大核心问题。
   - 在长上下文、高负载场景下性能优势显著，为 LLM 多轮服务（如聊天机器人）提供了高效、低成本的解决方案，随着对话轮数增加，优化价值更突出。
   
   

## [<span id ="asplos2503-1">M5: Mastering Page Migration and Memory  Management for CXL-based Tiered Memory Systems</span>](#asplos2503)

​	本文针对 CXL 基于分层内存系统中传统 CPU 驱动页面迁移方案精度低、开销大且无法区分稀疏页面的问题，首先提出基于 FPGA 的 CXL 驱动页面与字访问计数方案（PAC 与 WAC），以精准统计 CXL DRAM 中 4KB 页面和 64B 字的访问次数；接着通过 PAC 与 WAC 揭示了 ANB、DAMON 等 CPU 驱动方案误判温页面、盲目迁移稀疏页面及性能开销显著的缺陷；最后设计并实现 M5 平台，该平台依托 CXL 控制器中的硬件热页面 / 热字跟踪器（HPT/HWT）及软件 M5-manager，能低成本、高精度识别热页面与区分页面稀疏性，实验表明 M5 平均比最优 CPU 驱动方案（DAMON）多识别 47% 热页面且提升 14% 性能，为 CXL 分层内存系统的高效管理提供实用解决方案

### 一、研究背景与挑战

1. **技术背景**：数据中心应用对 DRAM 容量和带宽需求持续增长，但传统 DDR 接口已接近缩放极限。CXL 作为基于 PCIe 的新型内存接口，能以更少引脚提供与 DDR 相当的带宽，可低成本扩展内存容量，但 CXL DRAM 访问延迟比 DDR 高 2-3 倍，形成 “DDR（快内存）+ CXL DRAM（慢内存）” 的分层内存系统。
2. **核心挑战**：需高效的页面迁移方案，将频繁访问的 “热页面” 从 CXL DRAM 迁移到 DDR，以降低性能损失。但现有 CPU 驱动的页面迁移方案存在三大问题：
   - 易将 “温页面” 误判为 “热页面”，识别精度低；
   - 无法区分 “稀疏页面”（仅少量 64B 字频繁访问）和 “密集页面”，迁移稀疏页面会造成缓存污染和内存浪费；
   - 识别热页面的过程消耗大量 CPU 周期，性能开销显著，可能抵消迁移收益。

### 二、核心贡献

#### 1. CXL 驱动的页面与字访问计数方案（PAC 与 WAC）

- 基于 FPGA 的 CXL 设备实现，利用 CXL 控制器的近内存处理能力，精准、透明地统计 CXL DRAM 中每个 4KB 页面（PAC）和 64B 字（WAC）的访问次数。
- 相比动态二进制插桩、采样等传统方法，PAC 和 WAC 无需干扰应用执行，计数精度更高，为评估页面迁移方案提供了黄金标准。

#### 2. 揭示 CPU 驱动页面迁移方案的缺陷

通过 PAC 和 WAC 的实测分析：

- **识别精度低**：代表性方案 ANB（自动 NUMA 平衡）和 DAMON 识别的 “热页面”，其实际访问量仅为 PAC 判定的 Top-K 热页面的 21% 和 29%，本质是误判温页面；
- **稀疏页面迁移问题**：Redis 等应用中 86% 的页面仅 25% 以下的字被访问，但 CPU 驱动方案无法区分，盲目迁移导致缓存污染；
- **性能开销大**：ANB 和 DAMON 分别使内核 CPU 周期增加 159% 和 277%，导致 Redis 等延迟敏感应用的 p99 延迟上升 34%-39%，部分应用执行时间延长超 8%。

#### 3. M5 平台设计与实现

M5 是支持 CXL 驱动页面迁移方案开发的硬件 - 软件协同平台，核心目标是解决 CPU 驱动方案的缺陷，包含两大组件：

- **硬件组件**：热页面跟踪器（HPT）和热字跟踪器（HWT）
  - 基于 Count-Min Sketch（CM-Sketch）算法，低成本跟踪 Top-K 热页面和热字，避免 CPU 驱动方案的高开销；
  - 运行于 CXL 控制器，无需修改 CPU 架构，支持 400MHz 以上速率，满足内存访问实时性要求。
- **软件组件**：M5-manager
  - 包含 Monitor（监控内存带宽密度等指标）、Nominator（结合 HPT/HWT 识别密集热页面）、Elector（动态调整迁移频率和策略）、Promoter（与 Linux 内核交互执行页面迁移）；
  - 提供灵活接口，支持用户自定义迁移策略，并给出 4 条核心优化准则（如根据带宽密度决定迁移优先级、区分稀疏 / 密集页面迁移等）。

### 三、实验验证与结果

1. **实验环境**：基于 Intel 第 4 代 Xeon 处理器的双路服务器，搭配 Intel Agilex-7 FPGA（CXL 设备），测试 12 个内存密集型基准测试（含 SPEC CPU 2017、Redis、图计算等）。
2. **关键结果**：
   - **识别精度**：M5 的 CM-Sketch-based HPT 识别热页面的访问量占比达 0.72，比 ANB/DAMON（平均 0.49）高 47%；
   - **性能提升**：M5 平均比 DAMON（现有最优 CPU 驱动方案）提升 14% 性能，比 ANB 提升 20%；对 Redis 等延迟敏感应用，性能提升达 43%；
   - **开销优势**：M5 识别热页面的 CPU 开销可忽略，避免了 ANB/DAMON 的内核资源占用问题。

### 四、研究结论

M5 通过 CXL 控制器的硬件辅助跟踪与灵活的软件策略，解决了传统 CPU 驱动页面迁移方案的精度低、开销大、无法区分稀疏页面等问题，为 CXL 分层内存系统提供了高效、实用的内存管理解决方案，且兼容性强，可与现有 Linux 内核功能（如 MGLRU）协同工作，具备工业应用潜力。



## [<span id="asplos2504-1">MoE-Lightning: High-Throughput MoE Inference on  Memory-constrained GPUs</span>](#asplos2504)

​	该研究提出高吞吐量混合专家（MoE）模型批处理推理系统 MoE-Lightning，旨在解决内存受限 GPU 上 MoE 模型部署的资源利用率低、吞吐量不足问题，核心创新包括 CGOPipe 调度策略（通过权重分页和 CPU-GPU-I/O 任务重叠提升资源利用率）与 HRM 分层性能模型（精准定位瓶颈并搜索最优超参数），同时支持张量并行和动态批处理优化；在单 T4/L4 GPU 及多 T4 GPU 环境下，对 Mixtral 8x7B、Mixtral 8x22B、DBRX 等模型的测试显示，其无请求填充时吞吐量较现有最优系统提升达 10.3 倍，有填充时提升 3.5 倍，多 GPU 场景下实现超线性缩放，且仅需 2-3 倍更少 CPU 内存即可达到 GPU 内存受限下的吞吐量上限，为缺乏高端 GPU 资源的用户高效部署大型 MoE 模型提供了可行方案。

### 一、研究背景与挑战

混合专家（MoE）模型凭借稀疏激活特性，在不显著增加推理运算量的前提下提升模型容量，相比稠密模型降低了令牌生成延迟，在多项任务中表现出色。但 MoE 模型参数规模庞大，对内存需求极高（如 Mixtral 8x22B 的专家前馈网络参数需超 256GB 内存），远超普通 GPU 的存储能力，导致缺乏高端 GPU 资源的用户难以部署。

在内存受限场景下，现有解决方案通常采用模型权重和键值缓存（KV cache）卸载至 CPU 或磁盘、逐层加载到 GPU 计算的方式，但存在显著缺陷：CPU 与 GPU 间的数据传输与计算无法有效重叠，导致 GPU 闲置、资源利用率低下；且未充分考虑工作负载变化对瓶颈资源的影响，难以找到最优调度策略。

### 二、核心创新

#### 1. 调度策略：CGOPipe（CPU-GPU-I/O 流水线调度）

- 采用权重分页机制，将权重划分为与微批次数相等的页面，交错传输不同任务的数据，减少 I/O 流水线气泡。
- 高效重叠 GPU 计算、CPU 计算及各类 I/O 操作（包括中间结果传输、权重传输、KV 缓存传输），避免计算被 I/O 阻塞、不同 I/O 操作相互阻塞，大幅提升资源利用率。
- 针对解码阶段设计细粒度调度，GPU 依次处理当前微批的后注意力任务和下一个微批的前注意力任务，CPU 并行处理下一批的注意力计算，同时预取下一层的权重页面。

#### 2. 性能模型：HRM（分层 Roofline 模型）

- 基于经典 Roofline 模型扩展，适配异构计算设备（CPU、GPU）和多层内存架构（GPU HBM、CPU DRAM 等），考虑跨层级内存带宽和不同处理器的计算能力。
- 引入多个转折点和平衡点，明确不同操作条件下的性能瓶颈（如 CPU-GPU 传输瓶颈、GPU 内存瓶颈、CPU 内存瓶颈等），无需大量数据拟合（区别于 FlexGen），部署开销接近零。
- 支持不同模型、硬件和工作负载，为调度策略搜索提供理论支撑，帮助确定最优超参数（批大小、微批大小、设备分配、权重静态存储比例等）。

#### 3. 其他优化

- 支持张量并行（Tensor Parallelism），在单节点多 GPU 场景下扩展 GPU 内存容量和带宽，实现超线性吞吐量扩展。
- 采用动态请求批处理算法，按输入长度降序排序请求，将最长请求分配至令牌数最少的微批，保证微批大小接近最优值，支持变长请求处理。
- 优化内存管理，通过双缓冲机制实现权重预取与计算重叠，权重先从 CPU 内存转移至固定内存再到 GPU，隐藏传输延迟。

### 三、实验结果

#### 1. 实验配置

- 模型：Mixtral 8x7B、Mixtral 8x22B、DBRX（132B 参数，16 个专家）。
- 硬件：单 T4（16GB）、单 L4（24GB）、2xT4、4xT4 GPU，搭配不同配置的 Intel Xeon CPU。
- 工作负载：MTBench（多轮问答，输出长度 32-256）、HELM 基准（合成推理、摘要生成，长提示长度）。
- 基线系统：FlexGen（含 CPU 注意力变体）、DeepSpeed Zero-Inference。

#### 2. 关键性能表现

- 单 GPU 场景：在 Mixtral 8x7B 上，无请求填充时吞吐量较最先进系统提升达 10.3 倍，有请求填充时提升 3.5 倍；相比 FlexGen、FlexGen (c)、DeepSpeed，最高分别实现 3.5 倍、5 倍、6.7 倍提升。
- 多 GPU 场景：启用张量并行后，4xT4 GPU 运行 Mixtral 8x22B 时，吞吐量较 2xT4 提升 2.77-3.38 倍，实现超线性缩放；DBRX 模型在 2xT4 与 4xT4 间切换时，吞吐量提升 2.1-2.8 倍。
- 资源效率：在 GPU 内存受限时，仅需 2-3 倍更少的 CPU 内存即可达到吞吐量上限；处理长生成长度、长提示长度任务时，避免吞吐量下降（区别于基线系统），始终保持高资源利用率。

#### 3. 消融实验验证

- 策略优化：将 HRM 生成的策略应用于 FlexGen，吞吐量提升 1.77 倍，增大批大小后提升 2.17 倍，但仍低于 MoE-Lightning（受 KV 缓存交换瓶颈限制）。
- CPU 注意力优势：CPU 注意力内核比 KV 缓存传输快 3-4 倍，在大批次、长上下文场景下，CPU 注意力逐渐成为瓶颈，需提升 CPU 内存带宽。
- 硬件适配性：随着 CPU-GPU 带宽增加，更多权重可卸载至 CPU；KV 缓存卸载与 CPU 性能强相关，CPU 内存带宽较低时，即使高传输带宽也不适合卸载 KV 缓存。

### 四、适用场景与局限

#### 适用场景

- 离线批处理工作负载：模型评估、合成数据生成、数据整理、表单处理、关系型分析等追求高吞吐量的场景。
- 内存受限环境：单低 - cost GPU（如 T4、L4）或多低 - cost GPU 部署大型 MoE 模型（如 Mixtral 8x22B、DBRX）。

#### 局限

- 暂不支持磁盘卸载，仅适用于 CPU 内存足够存储模型的场景。
- 性能模型 HRM 目前局限于单节点硬件，未考虑 GPU 间通信和多节点通信。
- 长上下文场景下，CPU 注意力可能成为瓶颈，需结合 KV 缓存稀疏化等进一步优化。

### 五、结论

MoE-Lightning 通过 CGOPipe 调度策略和 HRM 性能模型，解决了内存受限 GPU 上 MoE 模型推理的吞吐量与资源利用率问题。在单 GPU 上实现最高 10.3 倍的吞吐量提升，多 GPU 场景下展现超线性缩放能力，且仅需更少 CPU 内存即可达到性能上限，为缺乏高端 GPU 资源的用户部署大型 MoE 模型提供了高效解决方案。未来可扩展至磁盘卸载、多节点通信支持，并融入 KV 缓存稀疏化等优化，进一步提升长上下文推理效率。



## [<span id ="asplos2505-1">CXLfork: Fast Remote Fork over CXL Fabrics</span>](#asplos2505)

​	该文章提出了一种基于 CXL（Compute Express Link）共享内存架构的远程进程克隆接口 CXLfork，通过近零序列化、零复制的设计，将进程私有状态以原生内存复制方式直接存储于 CXL 内存，全局状态仅序列化必要信息，结合写时复制（CoW）机制实现跨节点状态共享与内存去重，并提供写时迁移、访问时迁移、混合分层三种精细的状态分层策略平衡性能与内存开销；同时基于 CXLfork 构建了无服务器函数自动扩缩容工具 CXLporter，通过智能 checkpoint 管理、幽灵容器池、动态分层策略调整等功能优化冷启动性能与资源利用率。实验表明，CXLfork 相比现有方案（CRIU、Mitosis）平均性能提升 2.26 倍和 1.40 倍，本地内存消耗降低 87% 和 61%，CXLporter 能显著减少无服务器函数的尾延迟并提升集群并发能力，为云原生无服务器计算提供了高效的跨节点进程克隆解决方案。

### 一、研究背景与动机

#### 1. 技术基础

Compute Express Link（CXL）作为新兴的缓存一致性互连技术，支持字节可寻址的低延迟远程内存访问，其 3.0 及以上版本更实现了跨计算节点的机架级缓存一致性内存共享。这一特性为分布式系统的传统软件接口革新提供了可能，尤其适用于高性能云原生无服务器计算中的集群级进程克隆场景。

#### 2. 现有方案局限

现有远程进程克隆机制难以充分利用 CXL 共享内存优势：

- **CRIU**：通过序列化进程状态到文件实现跨节点迁移，存在大量序列化 / 反序列化开销，且父进程与子进程无状态共享，内存消耗极高。
- **Mitosis**：基于 RDMA 实现惰性数据复制，避免了进程内存的序列化，但仍需序列化操作系统管理状态，且数据复制带来显著延迟，无法发挥 CXL 的低延迟共享特性。

#### 3. 无服务器计算的核心需求

无服务器函数（FaaS）频繁创建和销毁实例，冷启动开销（初始化状态、容器创建等）成为性能瓶颈。这类函数的内存足迹中，初始化数据（72.2%）和只读数据（23%）占比极高，具备跨节点共享的潜力，为 CXL 共享内存的应用提供了天然场景。

### 二、核心方案：CXLfork 设计

CXLfork 是专为 CXL 架构设计的远程进程克隆接口，实现近零序列化、零复制的集群级进程克隆，核心目标是平衡性能、内存共享与数据分层。

#### 1. 核心设计理念

利用 CXL 全局共享内存特性，将进程状态直接存储于 CXL 内存而非本地文件或节点内存，实现跨节点状态共享与内存去重，同时通过精细的状态分层策略优化访问延迟。

#### 2. 关键技术突破

#### （1）进程状态的非序列化 checkpoint

- 区分**私有状态**（进程任务结构、内存描述符、页表、寄存器等）和**全局状态**（打开文件、套接字、命名空间等）。
- 私有状态通过原生内存复制直接写入 CXL 内存，无需序列化；页表项被修改为指向 CXL 物理地址并标记为只读，同时保留访问（A）和脏页（D）位以追踪访问模式。
- 全局状态仅序列化必要信息（如文件路径、权限），避免完整序列化带来的开销，恢复时通过重新执行系统调用重建。

#### （2）高效状态共享与零复制恢复

- 恢复时直接将 CXL 中的 checkpoint 状态映射到目标进程地址空间，采用写时复制（CoW）机制处理修改：只读状态常驻 CXL 内存供所有克隆进程共享，写操作触发缺页中断将数据迁移至本地内存。
- 优化页表和虚拟内存区域（VMA）树的恢复：仅在本地内存初始化页表树的上层结构，直接挂载 CXL 中的页表叶子节点，实现近常数时间的状态恢复。

#### （3）精细的状态分层策略

支持三种数据分层策略，平衡内存消耗与执行性能：

- **写时迁移（MoW）**：默认策略，仅在写操作时将 CXL 中的页面迁移至本地，最大化内存共享。
- **访问时迁移（MoA）**：首次访问时将页面迁移至本地，降低后续访问延迟，但增加内存消耗。
- **混合分层（HT）**：基于页表的访问位（A 位）识别热点页面，仅将高频访问页面迁移至本地，兼顾内存效率与性能。

### 三、衍生应用：CXLporter 无服务器自动扩缩容器

基于 CXLfork 构建 CXLporter，专为 CXL 互连集群中的无服务器函数设计，优化冷启动性能与资源利用率。

#### 1. 核心功能

- **智能 checkpoint 管理**：函数执行 16 次后（确保 JIT 编译达到稳定状态）创建 checkpoint，存储于 CXL 分布式对象存储中，并定期清理访问位以更新热点页面识别。
- **幽灵容器池**：维护预初始化的空容器（仅占用 512KB 内存），避免容器创建开销（约 130ms），通过 CXLfork 快速克隆函数状态至容器。
- **动态策略调整**：根据函数性能指标（延迟是否满足 SLO）和节点内存压力，动态切换 CXLfork 分层策略，并调整函数保活窗口（最低至 10 秒）以回收内存。

### 四、实验评估

#### 1. 实验环境

- 硬件：双路 Intel Sapphire Rapids 服务器（每路 64 核、128GB DDR5 内存），Intel Agilex 7 FPGA-based CXL 内存设备（16GB DDR4，访问延迟 391ns）。
- 软件：Linux 6.6 内核，QEMU/KVM 模拟双节点集群，基于 OpenWhisk 的无服务器平台。
- 工作负载：10 个典型无服务器函数（包括 Float、BERT、CNN 等，内存足迹 24-630MB），模拟 Azure 真实负载（150 RPS）。

#### 2. 核心性能结果

#### （1）CXLfork 性能优势

- 恢复延迟：仅 1.2-6.1ms，平均比 CRIU 快 2.26 倍，比 Mitosis 快 1.40 倍，接近本地 fork 性能（仅慢 14%）。
- 内存消耗：平均比 CRIU 减少 87%，比 Mitosis 减少 61%，仅为冷启动函数内存消耗的 13%。
- 分层策略效果：混合分层（HT）在 BERT、BFS 等大内存函数中，实现冷启动延迟与内存消耗的平衡，比写时迁移（MoW）降低 11% 的热执行延迟。

#### （2）CXLporter 扩缩容效果

- 充足内存场景：P99 延迟比 CRIU 降低 70%，比 Mitosis 降低 51%，幽灵容器池消除了容器创建开销。
- 内存受限场景（25% 内存）：CXLfork 的 P99 延迟比 CRIU 和 Mitosis 低 16 倍，吞吐量提升 2 倍，凸显内存去重优势。

#### （3）CXL 延迟敏感性

CXL 内存延迟降低至 200ns 时，BERT、BFS 等大工作集函数的热执行性能接近本地内存；即使延迟为 400ns，冷启动性能仍优于传统方案。

### 五、结论与贡献

#### 1. 核心贡献

- 提出首个基于 CXL 共享内存的远程 fork 接口，实现近零序列化、零复制的集群级进程克隆。
- 设计精细的状态分层策略，平衡内存去重、内存节省与运行时性能。
- 构建 CXLporter 自动扩缩容器，显著降低无服务器函数冷启动延迟与内存消耗。

#### 2. 应用价值

CXLfork 与 CXLporter 充分发挥了 CXL 架构的共享内存优势，为无服务器计算提供了高性能、低内存消耗的跨节点进程克隆方案，尤其适用于负载突发场景，可提升集群吞吐量与资源利用率。未来可进一步优化多节点集群中的 CXL 带宽调度，并扩展至复杂无服务器工作流的跨函数通信优化。



## [<span id ="asplos2506-1">PIM Is All You Need: A CXL-Enabled GPU-Free System  for Large Language Model Inference</span>](#asplos2506)

​	该文档提出了一种名为 CENT 的无 GPU 系统，专为大型语言模型（LLM）推理设计，其核心是通过 CXL 3.0 协议实现内存扩展以满足 LLM 的大容量需求，采用分层 PIM-PNM 架构（PIM 负责 99% 以上的 MAC 运算，PNM 处理特殊复杂操作），支持流水线并行、张量并行及混合并行策略，相比 4 块 NVIDIA A100 GPU，在相似功耗下实现 2.3 倍更高吞吐量、2.3 倍更低能耗，每美元生成令牌数提升 5.2 倍，且在长上下文场景（如 32K 令牌）中表现更优，同时硬件成本和总拥有成本（TCO）显著降低，为 LLM 推理提供了高效、经济的替代方案。

### 核心背景与痛点

- LLM 推理存在两大核心需求：一是模型参数和 KV 缓存需要超大内存容量，二是自回归生成导致运算强度低，对内存带宽要求极高。
- 现有 GPU/TPU 主要优化算力吞吐量，在内存受限的 LLM 推理中算力利用率仅 21% 左右，且成本高昂（如 ChatGPT 每日推理成本约 69 万美元）。
- 传统 PIM（内存内处理）带宽高但内存密度低，PNM（近内存处理）密度优但带宽不足，均难以单独满足 LLM 需求。

### CENT 系统核心设计

- **硬件架构**：基于 CXL 3.0 协议构建可扩展网络，32 个 CXL 设备通过 CXL 交换机互联，每个设备包含 16 个 GDDR6-PIM 芯片和 PNM 单元，总内存 512GB，内部峰值带宽达 512TB/s。
- **分层计算设计**：PIM 芯片负责 99% 以上的 MAC 运算（如 GEMV），PNM 单元包含加速器和 RISC-V 核心，处理 Softmax、平方根等特殊操作，无需 GPU 参与。
- **通信与并行策略**：支持点对点和集合通信原语（广播、汇聚等），适配三种并行映射：流水线并行（PP）提升吞吐量、张量并行（TP）降低延迟、混合 TP-PP 平衡二者。

### 关键性能与成本优势

- 与 4 块 A100 GPU 相比，在相同功耗下，CENT 吞吐量提升 2.3 倍，能耗降低 2.3 倍，每美元生成令牌数提升 5.2 倍。
- 长上下文场景优势显著，32K 上下文时解码吞吐量提速 3.3 倍，且查询延迟远低于 GPU（3.4-7.6 倍）。
- 总成本（TCO）大幅降低，3 年租赁成本仅为 GPU 系统的 19%，硬件成本约 1.49 万美元，仅为 GPU 方案（4.21 万美元）的 35%。

### 对比与扩展性

- 相较于 CXL-PNM 系统，CENT 吞吐量提升 4.5 倍；相较于 GPU-PIM 混合系统（AttAcc/NeuPIM），每美元令牌数提升 1.8-5.3 倍。
- 支持灵活扩展，设备数量从 16 增至 128 时，吞吐量可从 0.68K 令牌 / 秒提升至 5.7K 令牌 / 秒，适配 7B 到 70B 及更大参数模型（如 Grok 314B）。

### 适用场景与价值

- 适用于长上下文推理（如 10 万 - 100 万令牌的视频生成、推理任务）、大规模批量推理场景，可完全替代 GPU 或与 GPU 分工（GPU 负责预填充阶段，CENT 负责解码阶段）。
- 提供开源模拟器和 Python API，支持主流 LLM 的激活函数（GeLU、SiLU）和位置编码，具备较强通用性。





## [<span id ="asplos2507-1">PAPI: Exploiting Dynamic Parallelism  in Large Language Model Decoding with a  Processing-In-Memory-Enabled Computing System</span>](#asplos2507)

​	该文档提出了一种名为 PAPI 的基于内存内处理（PIM）的异构架构，旨在解决大型语言模型（LLM）解码阶段因并行性动态变化导致的性能瓶颈问题。PAPI 针对现有架构静态内核映射无法适配 LLM 解码中全连接（FC）层内核在计算密集型和内存密集型之间动态切换、且单一 PIM 单元难以满足不同内核异构需求的不足，整合了主机 CPU、GPU 张量核心以及两种定制化 PIM 单元（面向 FC 层的高性能 FC-PIM、面向注意力层的大内存容量 Attn-PIM），并通过在线内核特征化与两阶段动态调度机制，实时判断 FC 层内核类型并分配至最优硬件单元，同时让注意力层内核固定运行于 Attn-PIM。实验结果显示，在 LLaMA-65B、GPT-3 66B/175B 等模型上，PAPI 相比最先进的异构加速器（A100+AttAcc）和纯 PIM 加速器（AttAcc-only）分别实现 1.8 倍和 11.1 倍的性能提升，且在创意写作、通用问答等任务中显著优化了能效，为 LLM 推理提供了高效、灵活的解决方案。

### 一、研究背景与问题

1. **LLM 解码的关键地位**：LLM 推理包含预填充（prefill）和解码（decoding）两个阶段，解码阶段占总执行时间的绝大部分（如 GPT-3 175B 模型达 96%），且输出长度越长，解码的性能影响越显著。
2. **现有优化技术的局限**：现有并行优化技术（批处理 batching、推测解码 speculative decoding）虽能提升解码并行性，但导致解码内核（全连接 FC 层、多头注意力层）的特性动态变化 —— 部分内核会在计算密集型（compute-bound）和内存密集型（memory-bound）之间切换。
3. **传统架构的不足**：现有异构架构（如 GPU+PIM）采用静态内核映射策略，无法适应上述动态变化；且现有 PIM 单元仅支持单一计算 / 内存带宽配置，难以满足不同内核的异构需求。

### 二、核心观察与设计目标

1. **关键观察**：解码并行性（请求级 RLP、令牌级 TLP）会因服务等级目标（SLO）、内存容量限制、动态批处理等因素动态变化，进而导致 FC 层内核的算术强度改变，使其在计算密集型和内存密集型之间切换；而注意力层内核始终为内存密集型，但与 FC 层的内存 / 计算需求差异显著。
2. **设计目标**：构建支持动态调度的异构架构，适配 LLM 解码的动态需求，同时提供差异化的计算和内存带宽能力，优化性能与能效。

### 三、PAPI 的核心设计

PAPI 包含三大核心组件，实现动态并行性感知的高效调度：

1. **异构计算架构**：整合三类计算单元 —— 主机 CPU、计算中心型处理器（GPU 张量核心）、内存中心型 PIM 单元（FC-PIM、Attn-PIM），覆盖不同计算 / 内存需求。
2. **混合 PIM 单元**：
   - **FC-PIM**：针对 FC 层内核设计，采用 4 个浮点运算单元（FPU）/ 内存 bank 的高并行配置，平衡计算性能与功耗、面积约束。
   - **Attn-PIM**：针对注意力层内核设计，采用 1 个 FPU/2 个 bank 的低并行配置，侧重内存容量与带宽，通过离散式部署支持大规模 KV 缓存存储。
3. **动态并行性感知调度**：
   - **在线内核特征化**：通过实时计算 RLP×TLP 估算 FC 层内核的算术强度，快速判断其为计算密集型或内存密集型。
   - **两阶段调度**：初始调度基于初始并行性参数分配内核；运行时调度监测 RLP/TLP 变化，动态调整内核到最优硬件单元（计算密集型→GPU，内存密集型→FC-PIM；注意力层固定分配给 Attn-PIM）。

### 四、实验结果

1. **性能提升**：在 LLaMA-65B、GPT-3 66B/175B 模型上，PAPI 相比最先进的异构加速器（A100+AttAcc）提速 1.8 倍，相比纯 PIM 加速器（AttAcc-only）提速 11.1 倍；在长输出任务中（如创意写作），提速效果更显著。
2. **能效优化**：相比 A100+AttAcc，PAPI 在创意写作和通用问答任务中的能效分别提升 3.4 倍和 3.1 倍，通过减少数据迁移和低功耗 PIM 计算降低能耗。
3. **适应性验证**：在不同并行性水平（批大小 4-128、推测长度 1-8）下，PAPI 均保持最优性能，验证了其对动态并行性的适配能力。

### 五、创新贡献

1. 首次揭示 LLM 解码并行性的动态变化特性，以及其对内核需求的影响。
2. 提出混合 PIM 架构与动态调度框架，实现内核与硬件单元的动态最优匹配。
3. 验证了 PAPI 在不同模型、任务和并行配置下的性能与能效优势，为 LLM 推理的动态优化提供了新范式。



## [<span id ="asplos2508-1">POD-Attention: Unlocking Full Prefill-Decode  Overlap for Faster LLM Inference</span>](#asplos2508)

​	POD-Attention 是首个专为混合批处理 LLM 推理设计的 GPU 注意力内核，通过 SM 感知的 CTA 调度及瓦片大小优化、虚拟解码 CTA 等技术，实现预填充与解码注意力计算的并发执行，同时充分利用 GPU 计算资源和内存带宽，相较于 FlashAttention 等独立优化内核，其注意力计算最高提速 59%（平均 28%），集成到 Sarathi-Serve 后，可使 LLM 推理吞吐量最高提升 22%，并显著降低 TTFT、TBT 及请求执行延迟，在长上下文场景中表现突出，还能减少最高 35% 的能量消耗。

### 核心背景与问题

LLM 推理包含计算密集型的预填充（prefill）和内存带宽密集型的解码（decode）两个阶段。现有系统采用混合批处理（hybrid batching）将不同请求的两阶段任务合并到同一批次，优化了线性运算，但注意力计算仍存在效率瓶颈：现有注意力内核（如 FlashAttention、FlashInfer）分别针对两阶段独立优化，导致 GPU 计算资源和内存带宽无法同时充分利用（例如预填充阶段内存带宽利用率常低于 5%，解码阶段计算利用率不足 10%），资源浪费严重。

### 核心方案：POD-Attention

POD-Attention 是首个专为混合批处理设计的 GPU 注意力内核，通过让预填充和解码注意力计算在同一流多处理器（SM）上并发执行，同时最大化计算和内存带宽利用率。其核心设计包括：

#### 1. 关键技术创新

- **SM 感知的 CTA 调度**：通过运行时操作绑定，让协作线程数组（CTA）在分配到 SM 后动态决定执行预填充或解码任务，保证两阶段任务在 SM 级共置，避免资源闲置。支持 50:50 交替调度和按任务比例调度两种策略，后者在高负载下性能更优。
- **多维度性能优化**：
  - 适配性瓦片大小：为解码阶段使用最小 16 的 QSL 瓦片长度，减少冗余计算，释放张量核心供预填充使用；
  - 动态 CTA 配置：预填充主导场景采用 2 个 CTA/SM（保障共享内存使用），其他场景采用 4 个 CTA/SM（优化细粒度调度）；
  - 虚拟解码 CTA：将解码 CTA 拆分为 warp 级虚拟 CTA，平衡预填充与解码的共享内存需求；
  - 限制预填充拆分：避免 KV 维度过度拆分导致的带宽竞争，最多允许两波完整拆分。
- **CTA 并行融合实现**：将预填充和解码内核转换为可调用设备函数，通过包装器内核灵活映射 CTA ID，统一管理共享内存，用 warp 级屏障替代解码阶段的 CTA 级屏障，确保融合兼容性。

#### 2. 实验配置

- **模型与硬件**：基于 Yi-6B（1 块 A100）、Llama-2-7B、Llama-3-8B（2 块 A100），均含 32 个查询头，KV 头数量分别为 4、32、8；
- **基准系统**：对比 vLLM（预填充优先调度）、Sarathi-Serve（混合批处理调度），均使用 FlashAttention 内核；
- **评估场景**：离线推理（吞吐量）、在线推理（TTFT、TBT、请求延迟等），测试上下文长度 4K-32K，预填充与解码令牌比（P:D）0-50。

### 核心实验结果

#### 1. 注意力计算性能

- 相较于 FlashAttention、FlashInfer 等独立优化内核，POD-Attention 最高提速 59%，平均提速 28%，25% 的场景接近理论峰值性能，且从未低于串行执行效率；
- 能量消耗降低最高 35%（平均 20.5%），与运行时缩短比例一致。

#### 2. 离线推理吞吐量

- 相较于 Sarathi-Serve，Yi-6B、Llama-2-7B、Llama-3-8B 的吞吐量分别提升 22%、20%、19%；
- 相较于 vLLM，吞吐量分别提升 27%、13%、12%，突破混合批处理中预填充拆分与线性运算优化的权衡限制。

#### 3. 在线推理延迟

- **TTFT（首令牌时间）**：高负载下，Sarathi+POD 的中位数 TTFT 较 Sarathi 降低至 7.5 秒（内部工作负载）和 11.74 秒（arXiv 工作负载），P99 TTFT 最高降低 4.3 倍；
- **TBT（令牌间隔时间）**：P99 TBT 较 vLLM 降低 10 倍（0.14 秒 vs 1.76 秒），较 Sarathi 进一步降低 10%-20%，几乎消除生成停滞（<5% 请求出现停滞，vLLM 达 97%+）；
- **请求延迟**：P99 请求延迟较 vLLM 最高降低 42%（内部工作负载）和 17%（arXiv 工作负载），平衡吞吐量与延迟权衡。

#### 4. 敏感性分析

- 长上下文（预填充主导）场景下，2 个 CTA/SM 更优；短上下文（解码主导）场景下，4 个 CTA/SM 更优；
- P:D 比例 12-18 时性能增益峰值（混合批处理占比最高），纯预填充或纯解码场景增益有限；
- 比例调度策略较 50:50 调度最高提速 14%，尤其适合高负载场景。

### 相关工作对比

- 现有注意力优化（FlashAttention-3、FlashDecoding++）：均独立优化预填充或解码，未支持两阶段融合；
- 内核融合技术（HFuse、ISPA）：存在拖尾线程、SM 级共置无保障等问题，POD-Attention 通过 SM 感知调度和 CTA 并行融合解决；
- LLM 推理优化（NanoFlow、DistServe）：NanoFlow 适用于小上下文，DistServe 依赖跨节点拆分，POD-Attention 聚焦单设备长上下文混合批处理场景。

### 结论

POD-Attention 通过预填充与解码注意力的并发融合，首次实现 GPU 计算与内存带宽的同时高效利用，为混合批处理 LLM 推理提供了专用注意力内核解决方案。其在吞吐量（最高 + 22%）、延迟（TTFT、TBT、请求延迟显著降低）和能效上的全面提升，对长上下文 LLM 部署具有重要实践价值。未来可扩展至 FlashAttention-3 和 NVIDIA Hopper 架构支持。



# [<span id ="hpca-title"> HPCA</span>](#hpca)

## 2025

[<span id ="hpca2501">Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format</span>](#hpca2501-1)

[<span id ="hpca2502">BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration</span>](#hpca2502-1)

[<span id ="hpca2503">DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency</span>](#hpca2503-1)

[<span id ="hpca2504">HSMU-SpGEMM: Achieving High Shared Memory Utilization for Parallel Sparse General Matrix-Matrix Multiplication on Modern GPUs</span>](#hpca2504-1)

[<span id ="hpca2505">InstAttention: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference</span>](#hpca2505-1)

[<span id ="hpca2506">LAD: Efficient Accelerator for Generative Inference of LLM with Locality Aware Decoding</span>](#hpca2506-1)

[<span id ="hpca2507">M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type</span>](#hpca2507-1)

[<span id ="hpca2508">throttLL’eM: Predictive GPU Throttling for Energy Efficient LLM Inference Serving</span>](#hpca2508-1)

[<span id ="hpca2509">VQ-LLM: High-performance Code Generation for Vector Quantization Augmented LLM Inference</span>](#hpca2509-1)

##  [<span id ="hpca2501-1">Anda: Unlocking Efficient LLM Inference with a Variable-Length Grouped Activation Data Format</span>](#hpca2501)

​	本文提出 Anda 方案，通过三大核心创新解决权重量化大语言模型（LLM）推理中浮点数（FP）激活的性能与能耗瓶颈：设计组共享指数 + 动态尾数分配的可变长度 Anda 数据格式，开发无需重训练、适配不同模块精度敏感性的自适应精度搜索算法，以及配套的位平面数据布局、位串行处理单元和运行时位平面压缩器等硬件优化，在 OPT、LLaMA/LLaMA2 系列模型上实现平均 2.4 倍加速、4.0 倍面积效率提升和 3.1 倍能效提升，在 0.1%-1% 精度损失约束下达成精度与效率的平衡，适配多种部署场景。

### 研究背景与挑战

- 权重量化（如 W4A16）是 LLM 部署的主流方案，虽降低存储成本，但 FP 激活相关的 FP-INT GeMM 操作占比超 90%，成为 latency 和能耗瓶颈。
- 现有方案存在缺陷：GPU 核需转换 INT 权重为 FP 导致额外开销，专用 FP-INT 单元硬件复杂，块浮点（BFP）格式需重训练或长尾数才能保精度。

### 核心创新

#### 1. Anda 数据格式

- 基于 BFP 改进，采用组共享指数 + 动态尾数分配，支持 1-16 位连续尾数长度，可针对不同 LLM 模块灵活调整精度。
- 相比固定长度 BFP 和有限多长度方案，实现更细粒度的精度控制，平衡精度与效率。

#### 2. 自适应精度搜索算法

- 离线校准过程复用权重量化的校准数据，针对四大关键模块（*A**q**k**v*、*A**o*、*A**u*、*A**d*）搜索最优尾数组合。
- 以比特操作数（BOPs）为优化目标，在用户设定的精度损失阈值（如 0.1%、1%）内，快速找到近优解，无需重训练，搜索效率比现有方法快 2-10 倍。

#### 3. 专用硬件架构

- 位平面数据布局：将同权重位打包存储，保证变长数据的内存访问规律性。
- Anda 增强型位串行处理单元（APU）：适配变长尾数计算，减少硬件开销，支持 INT 点积与 FP 累加。
- 运行时位平面压缩器（BPC）：实时将 FP16 激活转换为 Anda 格式，降低存储和访问成本。

### 实验结果

- 精度表现：在 WikiText2、PTB、C4 数据集上，Anda 在 0.1%-1% 精度损失下，实现 1.8-3.3 倍 BOPs reduction，远超 FIGNA 的 1.23 倍，且避免了 VS-Quant 的严重精度下降。
- 硬件效率：相比 GPU-like FP-FP 基线，平均实现 2.4 倍加速、4.0 倍面积效率提升、3.1 倍能效提升；PE 级功耗和面积均低于 FP-FP、FP-INT 等基线单元。
- 适应性：支持 OPT、LLaMA/LLaMA2 系列（1.3B-30B 参数），可根据不同精度需求、模型及模块敏感性动态调整，适配短上下文（<4K）和长上下文（>10K）场景。

### 结论

Anda 通过数据格式、算法与硬件的协同优化，有效突破权重量化 LLM 的 FP 激活瓶颈，在无需重训练的前提下，实现精度、速度与能耗的平衡，为资源受限场景下的 LLM 部署提供高效解决方案。



 ## [<span id ="hpca2502-1">BitMoD: Bit-serial Mixture-of-Datatype LLM Acceleration</span>](#hpca2502)

​	BitMoD是一种算法-硬件协同设计方案，旨在解决大语言模型（LLM）部署中内存占用过大的问题，其通过在算法层设计扩展分辨率（ER）和扩展不对称性（EA）的3bit/4bit浮点数据类型，基于分组量化（每组128个权重）并复用冗余零值为特殊值，在极低权重精度下维持模型精度，同时在硬件层采用统一位串行表示和专用处理单元（PE），支持多精度数据类型高效计算与位串行反量化，最终在6个代表性LLM上实现了优于现有方案的性能——4bit量化判别任务平均精度损失<0.5%，3bit量化生成任务困惑度更优，较ANT/OliVe加速器分别平均提速1.69×/1.48×、能效提升1.48×/1.31×，且可无缝集成AWQ、SmoothQuant等现有量化方案，为边缘设备部署LLM提供了高效解决方案。

### 一、研究背景与动机

1. **LLM 部署痛点**：LLM 参数规模激增（如 Llama-3-8B 含 80 亿参数，FP16 格式需 16GB 内存），远超边缘设备（如 Jetson-TX2 仅 8GB 内存）的存储与计算能力，限制了其广泛应用。
2. **量化技术现状**：
   - 量化是降低内存和计算开销的关键手段，分为训练时量化（QAT，成本高）和训练后量化（PTQ，更实用）。
   - 现有权重量化方案存在不足：GPU 缺乏低精度整数权重与浮点激活的专用计算单元，导致计算效率低；部分方案不支持细粒度量化，或在低精度（3-4bit）下精度损失严重。
3. **核心需求**：需设计适配 LLM 分布特性的低精度数据类型，结合专用硬件架构，在极低权重精度下维持模型精度，同时提升硬件效率。

### 二、BitMoD 核心设计

BitMoD 的核心是 “算法 - 硬件协同优化”，分为算法层和硬件层两部分：

#### （一）算法层：低精度数据类型与量化策略

1. **细粒度数据类型适配**：
   - 基于分组量化（每组 128 个权重），改造 3bit 和 4bit 浮点数据类型：将浮点格式中冗余的 “零值” 替换为特殊值，形成两类扩展数据类型 —— 扩展分辨率（ER，如 FP3-ER、FP4-ER）和扩展不对称性（EA，如 FP3-EA、FP4-EA），兼顾对称分布适配与不对称 outliers 处理。
   - 每组权重动态选择最优特殊值，最小化量化误差，仅需 2bit 编码开销。
2. **高效分组反量化**：将分组量化的缩放因子量化为 INT8（无精度损失），避免浮点运算带来的硬件开销，支持位串行反量化。
3. **兼容性**：可无缝集成 AWQ、SmoothQuant 等现有软件量化优化方案，替换其整数量化器以进一步提升性能。

#### （二）硬件层：位串行加速器架构

1. **统一位串行表示**：将 INT8、INT6、扩展 FP3/FP4 等多种数据类型，统一分解为 “符号 + 指数 + 尾数 + 位重要性” 四部分，支持多精度统一计算，降低硬件复杂度。
2. **位串行处理单元（PE）**：
   - 支持低精度权重（3-6bit）与 FP16 激活的混合精度计算，通过 Booth 编码、移位对齐、累加归一化等步骤高效完成点积运算。
   - 内置位串行反量化单元，实时处理分组缩放因子，无流水线阻塞。
3. **整体架构**：4×4 PE tile 组成的脉动阵列，采用输出固定数据流，通过权重共享和输入共享提升数据复用率，配备 512KB 激活 / 权重缓存，适配边缘场景小批量（batch=1）、短序列（输入 256token）需求。

### 三、实验结果

#### （一）模型精度表现

- **生成任务**：4bit 量化平均困惑度损失 < 0.5，3bit 量化损失 < 3，显著优于 ANT、OliVe 等现有方案（3bit 下困惑度大幅退化）；与 AWQ/OmniQuant 结合后，3bit 量化平均困惑度损失 < 1。
- **判别任务**：4bit 量化平均精度损失 < 0.5%，3bit 量化精度损失较 INT3-Asym 提升 2.2%，在 HellaSwag、WinoGrande 等数据集上表现稳定。
- **兼容性**：与 SmoothQuant 结合量化激活至 INT8 后，仍保持精度优势，3bit 权重量化时困惑度提升明显。

#### （二）硬件性能与效率

- **速度提升**：相比 FP16 基线加速器，无损配置（INT6 量化）平均提速 2.2×，有损配置（3-4bit）较 ANT/OliVe 分别平均提速 1.69×/1.48×。
- **能效优化**：较 FP16 基线能效提升 2.31×，较 ANT/OliVe 分别提升 1.48×/1.31×；PE 面积较 FP16 PE 小 24%，支持多精度灵活切换。
- ** Pareto 最优 **：在困惑度 - 能效积（EDP）权衡中处于 Pareto 前沿，兼顾精度与硬件效率。

### 四、核心贡献

1. 提出适配 LLM 分组量化的扩展浮点数据类型，实现 3-4bit 低精度量化下的高精度保留。
2. 设计统一位串行硬件架构，支持多数据类型 / 精度，降低硬件成本并提升计算效率。
3. 算法 - 硬件深度协同，兼容现有软件量化方案，为边缘设备部署 LLM 提供高效解决方案。



## [<span id ="hpca2503-1">DynamoLLM: Designing LLM Inference Clusters for Performance and Energy Efficiency</span>](#hpca2503)

​	DynamoLLM 是首个针对 LLM 推理集群的能量管理框架，旨在解决 LLM 推理依赖高功耗 GPU 导致的高能耗、高碳排放问题，其核心是利用 LLM 推理工作负载的异构性（如请求长度、模型类型、SLO 要求差异）和动态性（如负载波动、请求类型分布变化），通过分层控制架构（集群、池、实例三级控制器）动态优化实例数量、模型并行度（TP2/TP4/TP8）和 GPU 频率，同时通过缓存模型权重、优化重分片策略、预测调度等方式最小化重配置开销，在满足延迟 SLO 的前提下，平均节省 52% 能耗、38% 运营碳排放，并降低 61% 用户成本，经 Microsoft Azure 生产级迹线验证，在不同负载和模型场景下均展现出高效稳定性。

### 一、研究背景与问题

随着生成式大语言模型（LLM）在医疗、开发、教育等领域的广泛应用，LLM 推理集群需处理海量查询并满足严格的服务等级目标（SLOs）。当前 LLM 推理依赖高功耗 GPU，导致集群能耗巨大、碳排放显著，而现有能量管理方案多针对传统数据中心 workload，未充分适配 LLM 推理的独特特性：

1. **异构性**：请求的输入 / 输出令牌长度、不同 LLM 的计算属性、服务 SLO 要求存在显著差异；
2. **动态性**：请求类型分布、负载量（如昼夜波动）、服务与模型组合随时间快速变化；
3. **配置复杂性**：实例数量、模型并行度（张量 / 流水线并行）、GPU 频率等多维度配置形成庞大搜索空间，单一配置难以兼顾能效与性能；
4. **重配置开销**：调整实例数量、并行度或 GPU 频率会产生不可忽视的延迟，影响服务连续性。

### 二、核心洞察

1. LLM 推理工作负载的能效特性高度异构，不同类型请求（长短输入 / 输出、不同模型、SLO）需差异化配置才能实现最优能效；
2. 工作负载的动态变化导致静态配置快速失效，需自动化、透明化的动态配置机制；
3. GPU 功耗与负载呈亚线性缩放，且重配置开销（如实例启动需 6-8 分钟）需纳入能效权衡。

### 三、DynamoLLM 框架设计

DynamoLLM 是首个针对 LLM 推理集群的能量管理框架，核心目标是在满足延迟 SLO 的前提下，最小化能耗、碳排放与用户成本，其设计包含四大关键模块：

#### 1. 分层控制架构

采用集群、池、实例三级控制器，分别负责不同粒度的配置优化，降低计算复杂度并避免集中式瓶颈：

- **集群管理器**：每 30 分钟预测负载，划分请求类型池（如 SS 短输入短输出、LL 长输入长输出），动态调整各池实例数量，合并低负载池以减少资源碎片化；
- **池管理器**：每 5 分钟优化模型并行度（TP2/TP4/TP8），基于当前 GPU 数量分配实例资源；
- **实例管理器**：每 5 秒微调 GPU 频率，筛选满足 SLO 的最低能耗频率配置。

#### 2. 能效配置优化

- **模型 profiling**：预先生成不同负载、请求类型、并行度、频率下的能效 - 性能剖面，预测准确率超 98%，支持跨服务复用；
- **多维度调优**：联合优化实例数量（扩缩容）、模型并行度（张量并行为主）、GPU 频率（800-1980MHz），通过混合整数线性规划求解最优配置；
- **预测调度**：基于 BERT 代理模型预测请求输出长度（准确率 81%），将请求路由至匹配池；针对负载预测采用模板法，适配昼夜波动模式。

#### 3. 重配置开销优化

- **实例扩缩容**：缓存模型权重、预启动虚拟机快照、后台提前扩容，将实例启动开销从 6-8 分钟降至可忽略；
- **并行度调整**：通过图匹配算法最大化 GPU 权重复用，利用 NVLink 直连并行传输，将重分片开销降至 50-100ms；
- **频率调整**：常驻 nvidia-smi 进程，以特权模式运行控制器，减少 OS 交互开销。

#### 4. 鲁棒性机制

- 处理预测错误：优先级调度即将超时请求，动态提升 GPU 频率，或迁移未执行请求至其他池；
- 适配多样 SLO：为不同 SLO 请求单独计算最优配置，仅在能效收益显著时拆分资源池。

### 四、实验结果

基于 Microsoft Azure 生产级迹线（含 1 小时、1 天、1 周数据），在 8 卡 H100 GPU 集群上的评估显示：

1. **核心收益**：平均节省 52% 能耗、38% 运营碳排放、61% 用户成本，同时满足所有延迟 SLO；
2. **性能表现**：P99 TTFT（首令牌延迟）降低 5.3%，P99 TBT（令牌间延迟）降低 11.0%，仅 P50 延迟小幅上升（TTFT+11.4%、TBT+7.6%）；
3. **负载适应性**：低 / 中 / 高负载下分别节省 57%/42%/15% 能耗， Coding 服务（周末低负载显著）比 Conversation 服务节能效果更优（56% vs 47%）；
4. **鲁棒性**：输出长度预测准确率降至 60% 时，能耗仅增加 25%，TTFT 上升 12.3%。

### 五、核心贡献

1. 首次系统分析 LLM 推理的异构性与动态性带来的能效优化机遇；
2. 设计并实现首个支持多维度动态配置的 LLM 推理能效框架，兼顾性能与开销；
3. 基于大规模生产环境验证框架有效性，开源部分迹线数据集（https://github.com/Azure/AzurePublicDataset）。



## [<span id ="hpca2504-1">HSMU-SpGEMM: Achieving High Shared Memory Utilization for Parallel Sparse General Matrix-Matrix Multiplication on Modern GPUs</span>](#hpca2504)

​	该论文提出面向现代GPU的稀疏矩阵乘法算法HSMU-SpGEMM，针对传统哈希基方法在哈希冲突与共享内存利用率间的权衡难题，设计了基于预排序列索引数组与二分查找的新型累加器，结合适配中小规模矩阵的掩码矩阵与大规模矩阵的压缩掩码格式优化符号阶段，并动态融合稠密累加器，在Ampere、Ada Lovelace、Turing三种架构的NVIDIA GPU上，基于338个SuiteSparse稀疏矩阵的测试显示，其相对Nsparse、spECK、OpSparse、cuSPARSE四大主流库的几何平均加速比最高分别达3.19、1.28、2.34、6.92，共享内存平均利用率达68%，同时解决了超大规模矩阵计算的内存溢出问题，在科学计算、图处理等场景具备高效实用价值。

### 一、研究背景与问题

1. **SpGEMM 的重要性**：稀疏矩阵乘法是科学计算、图处理等领域的核心原语，如广度优先搜索、马尔可夫聚类等应用均依赖该运算。
2. **传统方法的局限**：现有哈希基 SpGEMM 库（Nsparse、spECK、OpSparse、cuSPARSE）存在固有缺陷：
   - Nsparse：哈希表长度匹配最大非零元素数（NNZ），共享内存利用率较高但哈希冲突率极高（达 400%+）；
   - spECK/OpSparse：通过扩大哈希表（1.5×/2× 需求空间）降低冲突，但共享内存利用率仅 66%/50%，造成资源浪费；
   - 核心矛盾：哈希表大小需在 “降低冲突” 与 “提高共享内存利用率” 之间妥协，难以兼顾。

### 二、HSMU-SpGEMM 的核心设计

#### 1. 新型累加器架构

- 摒弃传统哈希映射，采用 “预排序列索引数组 + 二分查找” 机制（findInSorted 函数），彻底消除哈希冲突；
- 累加器仅分配与实际 NNZ 匹配的存储空间，共享内存利用率达 100%（理想场景），平均达 68%，远超现有库；
- 结合稠密累加器（源自 spECK），通过阈值（4096 个 NNZ / 行）动态选择：稀疏行用二分查找累加器，稠密行用稠密累加器，兼顾不同场景。

#### 2. 符号阶段优化（生成预排序列索引）

- 针对小规模矩阵：通过掩码矩阵（mask B/mask C）的按位或运算，快速提取矩阵 C 的非零元素列索引并排序；
- 针对大规模矩阵：采用压缩掩码格式（类 CSR 结构），结合 OpSparse 的符号阶段优化，降低存储空间开销并提升运算并行性。

#### 3. 整体工作流程

1. 符号阶段：通过掩码矩阵 / 压缩掩码矩阵计算矩阵 C 的 NNZ 和预排序列索引数组；
2. 数值阶段：根据行密度选择累加器，通过二分查找定位中间产物并累加，避免冲突且高效利用共享内存。

### 三、实验结果

#### 1. 硬件与数据集

- 测试平台：3 款不同架构 NVIDIA GPU（Ampere 架构 RTX 3090 Ti、Ada Lovelace 架构 RTX 4080 SUPER、Turing 架构 RTX 1650 SUPER）；
- 数据集：SuiteSparse 矩阵集合中的 338 个稀疏方阵（含 18 个代表性矩阵、4 个超大规模矩阵），覆盖不同稀疏度与规模。

#### 2. 核心性能提升

| GPU 型号       | 相对 Nsparse 加速比（几何均值） | 相对 spECK 加速比 | 相对 OpSparse 加速比 | 相对 cuSPARSE 加速比 |
| -------------- | ------------------------------- | ----------------- | -------------------- | -------------------- |
| RTX 3090 Ti    | 3.19（最高 131.64）             | 1.28              | 2.34                 | 5.06                 |
| RTX 4080 SUPER | 3.08（最高 125.25）             | 1.24              | 2.24                 | 6.92                 |
| RTX 1650 SUPER | 1.41（最高 70.14）              | 1.10              | 1.46                 | 1.73                 |

- 超大规模矩阵场景：HSMU-SpGEMM 避免内存溢出（OpSparse 在部分矩阵上失败），峰值内存仅为 OpSparse 的 1/3~1/2；
- 共享内存利用率：平均 68%，显著高于 spECK（≤66%）和 OpSparse（35.35%）。

#### 3. 适配性与扩展性

- 矩阵适应性：对非零元素紧凑分布的矩阵加速效果最显著（冲突敏感场景），对稀疏分布矩阵仍保持优势；
- 数据中心 GPU 支持：在 NVIDIA L40S 上性能进一步提升，相对 3090 Ti 的加速比高于 OpSparse；
- 迭代场景优化：格式转换开销仅为单次 SpGEMM 的 1/10，适合代数多重网格等迭代计算场景。

### 四、结论与未来方向

1. 核心贡献：突破哈希基方法的固有权衡，实现 “零冲突 + 高共享内存利用率”，在 3 款 GPU 上全面超越 4 类主流库；
2. 未来工作：将 HSMU-SpGEMM 应用于深度学习等实际场景，进一步优化大规模矩阵的符号阶段开销。

### 补充信息

- 代码开源：https://github.com/wuminqaq/HSMU-SpGEMM；
- 实验可复现性：提供完整的数据集下载脚本、编译流程及性能测试工具，支持在兼容 GPU 上复现全部结果。



## [<span id ="hpca2505-1">InstAttention: In-Storage Attention Offloading for Cost-Effective Long-Context LLM Inference</span>](#hpca2505)

​	提出InstAttention，一种面向长上下文LLM离线推理的新型系统，核心是将解码阶段关键的注意力计算与KV缓存卸载到计算存储驱动器（CSDs），通过设计闪存感知的SparF稀疏注意力机制、双地址映射的KV缓存管理方案及GPU与CSD间的P2PDMA直接传输，充分利用CSD的高内部带宽规避PCIe带宽限制，同时缓解CSD计算能力弱的问题，在NVIDIA A6000 GPU及OPT、Llama-2系列模型上的实验表明，其长序列推理吞吐量较FlexGen等现有SSD卸载方案最高提升11.1倍，为资源受限场景下低成本、高性能的LLM推理提供了有效解决方案。

### 一、研究背景与问题

1. **LLM 推理的核心挑战**：LLM 推理分为预填充（计算密集型）和解码（内存密集型）两个阶段，解码阶段依赖 KV 缓存存储历史令牌的中间数据以避免重复计算，但随着上下文长度和批处理规模扩大，KV 缓存体积激增（如 13B 模型在批大小 32、4K 令牌下需 100GB KV 缓存，是模型本身的 4.2 倍），给 GPU 显存带来巨大压力。
2. **现有解决方案的缺陷**：现有方案（如 DeepSpeed-MII、FlexGen）将 KV 缓存卸载到主机内存或 SSD 以降低成本，但受限于 PCIe 带宽低、存储与 GPU 无直接数据通路、存储软件栈复杂等问题，导致 KV 缓存访问延迟极高，性能严重下降（如 FlexGen 在 KV 缓存卸载到 SSD 时，解码阶段 KV 缓存访问开销占比达 98.94%）。
3. **CSD 的潜力与挑战**：计算存储驱动器（CSDs）集成存储与计算资源，兼具 SSD 的低成本大容量和内部高带宽（数十 GB/s，远超 PCIe 的 3-7GB/s），但存在计算能力弱（比 GPU 低 2-3 个数量级）、与 GPU 带宽差距大、闪存访问模式与内存不兼容等问题，无法直接承接完整推理任务。

### 二、InstAttention 的核心设计

InstAttention 构建 GPU-CSD 异构系统，仅将解码阶段性能关键的注意力计算和 KV 缓存卸载到 CSD，其余任务由 GPU 执行，通过软硬件协同优化解决存储、带宽和计算适配问题。

#### 1. 任务划分与系统架构

- **三大硬件组件**：
  - InstCSD：存储 KV 缓存，执行解码阶段注意力计算；
  - InstGPU：负责预填充阶段全量计算、解码阶段非注意力计算（如 QKV 投影、FFN）；
  - InstHost：调度任务、协调 GPU 与 CSD 的数据传输，作为控制平面。
- **数据传输优化**：采用 PCIe 对等 DMA（P2PDMA）直接连接 GPU 与 CSD，绕过主机内存，减少数据拷贝；预填充阶段采用分层流水线传输 KV 缓存，与计算重叠以隐藏延迟。

#### 2. 闪存感知的稀疏注意力机制（SparF）

- 基于 SparQ 算法优化，通过 “双步筛选” 适配闪存页访问粒度：先按通道 / 令牌组（与闪存页大小对齐）筛选有效数据，再在组内筛选关键令牌 / 通道，减少无效访问和带宽浪费；
- 在保证精度损失可忽略（压缩比 1/8 时准确率约 95%）的前提下，降低计算强度和 KV 缓存需求，缓解 CSD 计算压力。

#### 3. KV 缓存管理与 FTL 设计

- **双地址映射**：针对令牌索引和通道索引的 KV 缓存访问需求，设计两套地址映射机制，将 KV 缓存按闪存页大小分组，跨多个闪存块和芯片分布，利用闪存并行性；
- **批处理写入**：将 GPU 生成的 KV 缓存按注意力头批量写入 CSD，填充闪存块以避免写放大；
- **简化垃圾回收（GC）**：利用 KV 缓存的顺序写入特性，仅在 CSD 空闲时按 LRU 策略清理过期数据，无数据碎片， overhead 极低。

#### 4. 存储内注意力引擎硬件实现

在 CSD 的 FPGA 上实现 SparF 注意力引擎，通过细粒度并行设计隐藏闪存访问延迟，集成 GeMV、Softmax 等计算单元，适配稀疏数据处理，最大化 CSD 计算效率。

### 三、实验结果

基于 NVIDIA A6000 GPU、Zynq7045 FPGA-based CSD，在 OPT-13B/30B、Llama-2-13B 模型上进行测试，核心结果如下：

1. **吞吐量提升**：
   - 单 CSD 场景下，InstAttention（含 SparF）比 FlexGen 吞吐量最高提升 11.1×（13B 模型，批大小 256）；
   - 双 CSD 场景下，InstA-SparF 比 FlexGen-SparQ 吞吐量提升 3.11×，且支持更大批处理规模（如 13B 模型批大小可达 256，远超 FlexGen 的 64）；
   - 对 Llama-2-13B（4K 上下文），吞吐量比 FlexGen 最高提升 12.48×。
2. **性能瓶颈缓解**：解码阶段 KV 缓存访问开销占比从 FlexGen 的 98.9% 降至 InstA 的 80.7%（ dense 模式）和 74.0%（稀疏模式）。
3. **扩展性与实用性**：
   - 支持多 CSD 扩展，20 个 CSD 可使密集 / 稀疏推理吞吐量分别提升 8.99× 和 7.29×；
   - 闪存耐久性优化：通过简化 FTL 和保留时间松弛，可支持 5 年服务（如 4 个 CSD 可满足 920 名高负载用户需求），性能下降控制在 0.47‰以内。

### 四、核心贡献

1. 首次提出基于 CSD 的 LLM 推理系统，将解码阶段注意力计算与 KV 缓存卸载到 CSD，有效缓解 PCIe 带宽瓶颈，KV 缓存迁移开销降低 94.0%；
2. 软硬件协同设计闪存感知的稀疏注意力引擎和 KV 缓存管理机制，适配 CSD 的计算与存储特性；
3. 实验验证在长上下文、大批次离线推理场景下，性能远超现有 SSD 卸载方案，且成本可控、扩展性强。

### 五、结论

InstAttention 通过存储内计算卸载和软硬件协同优化，在资源受限场景下实现了低成本、高性能的长上下文 LLM 离线推理，比现有 SSD-based 方案吞吐量提升最高 11.1×，为解决 KV 缓存的存储与带宽难题提供了新型异构架构方案。



## [<span id ="hpca2506-1">LAD: Efficient Accelerator for Generative Inference of LLM with Locality Aware Decoding</span>](#hpca2506)

​	本文提出了一种名为LAD（Locality Aware Decoding）的LLM生成推理加速器，通过挖掘注意力分数在多解码步骤中的数值局部性，采用算法-硬件协同设计，将历史解码信息存储在固定大小的中间缓存中，仅对注意力分数超出主导区间的“活跃位置”访问KV缓存进行修正，在保证与原始模型平均97% ROUGE-1相似度的前提下，大幅减少KV缓存访问开销；当KV缓存长度超2048时，其高端配置相对A100 GPU的注意力机制平均提速10.7倍、能效提升52.4倍，端到端推理平均提速2.3倍、能效提升13.4倍，有效解决了LLM长序列生成中KV缓存访问导致的性能瓶颈。

### 一、研究背景与问题

1. **LLM 生成推理的核心瓶颈**：大语言模型（LLM）基于自回归解码，注意力机制需通过 KV 缓存存储全部生成历史（键和值），但随着序列长度增长，KV 缓存规模持续扩大，导致内存访问成为性能瓶颈。当 KV 缓存长度达 4096 时，注意力机制的推理耗时占比超 50%（如 LLaMA2 7B 模型）。
2. **现有方案的局限**：架构优化无法有效减少 KV 缓存访问，模型压缩会牺牲精度且需重训练，投机解码依赖小模型精度并引入额外计算开销，均未能彻底解决问题。

### 二、核心洞察与创新思路

1. **注意力分数的数值局部性**：分析发现，多个解码步骤中，多数位置的注意力分数集中在特定数值区间（数值局部性），top-1 区间命中率超 74%，top-1+top-2 命中率超 95%，且长序列下局部性更显著（LLaMA2 7B 在 KV 长度 4096 时 top-1 命中率超 90%）。
2. **核心创新**：利用数值局部性，将历史解码信息存储在固定大小的中间缓存中，无需每次解码都访问完整 KV 缓存，仅对注意力分数超出其主导区间的 “活跃位置” 进行 KV 缓存访问与修正，在不损失精度的前提下减少内存访问。

### 三、LAD 的关键设计

#### （一）算法层面

1. **分段线性近似**：将注意力分数的指数函数（softmax 核心）拆分为非均匀分段线性函数，近 0 区间更短以适配函数斜率变化，通过最小二乘优化获取线性系数，引入误差小于 10⁻⁶。
2. **局部性感知解码流程**：
   - 维护每个位置的 “主导区间”（注意力分数最常落入的区间）及对应线性系数；
   - 解码时先通过中间缓存快速计算注意力结果，再对活跃位置访问 KV 缓存进行修正；
   - 中间缓存动态更新：仅当位置主导区间变化时，利用活跃位置的系数差异更新缓存，无额外 KV 访问开销。
3. **高效活跃位置识别**：通过提取键的方向中心，近似计算注意力分数以判断区间归属，避免完整键的内存访问，同时对高权重位置计算精确分数以保证精度。

#### （二）硬件架构层面

1. **整体架构**：包含多个 LAD Tile，每个 Tile 集成注意力计算模块、特殊函数模块（SFM）、7 个向量处理单元（VPU）和片上 SRAM，所有 Tile 共享 HBM（高带宽内存）。
2. **核心模块**：
   - 高效注意力分数模块（EAS）：计算近似注意力分数、更新键中心；
   - 活跃位置识别模块（APID）：判断注意力分数是否超出主导区间，标记活跃位置；
   - 模式差异模块（MD）：计算活跃位置精确分数、更新主导区间及系数差异；
   - 注意力计算模块（AC）：基于中间缓存计算注意力结果、修正活跃位置、更新中间缓存。
3. **流水线与调度优化**：设计 6 级流水线平衡 HBM 访问与计算，通过预取利用活跃位置的时间局部性（连续解码步骤活跃位置重叠率超 80%），进一步减少 KV 访问；QKV 层与注意力层交错调度，掩盖内存延迟。

### 四、实验结果

#### （一）精度验证

- 与原始模型生成序列的 ROUGE-1 相似度平均达 97%，显著优于 Qserve（量化方案）和 H2O（KV 缓存剪枝）；
- 在 Wikitext2、OpenBookQA 等数据集上，困惑度（ppl）与准确率（acc）完全匹配原始模型，无精度损失。

#### （二）性能与能效

- **注意力层**：KV 缓存长度超 2048 时，LAD-3.5（3.5MB SRAM 配置）相对 A100 GPU 平均提速 10.7 倍，能效提升 52.4 倍；
- **端到端推理**：KV 缓存长度超 2048 时，平均提速 2.3 倍，能效提升 13.4 倍；
- 长序列优势更显著：KV 长度 4096 时，LLaMA2 13B 的注意力 latency 占比仅增长 3%，有效缓解长序列瓶颈。

#### （三）配置对比

- 三种 SRAM 配置（1.5MB/2.5MB/3.5MB）中，LAD-2.5 和 LAD-3.5 在长序列下性能接近，均显著优于小容量配置，SRAM 主要通过预取提升活跃位置命中率。

### 五、与相关工作对比

- 相比 KV 缓存剪枝（如 H2O）、量化（如 Qserve），LAD 不丢弃 KV 信息、不引入量化误差，精度更高；
- 相比投机解码、非自回归解码，LAD 无需额外模型或修改自回归本质，适配性更强；
- 相比 GPU 优化（如 vLLM），LAD 通过硬件定制化适配不规则访问模式，性能与能效优势显著。

### 六、结论

LAD 通过挖掘注意力分数的数值局部性，结合算法 - 硬件协同设计，在不损失 LLM 生成精度的前提下，大幅减少 KV 缓存访问，解决了长序列生成的内存瓶颈。在主流 LLM（OPT 2.7B/6.7B、LLaMA2 7B/13B）上的实验表明，其相对 A100 GPU 实现了 2.3 倍端到端提速和 13.4 倍能效提升，为大模型长序列生成提供了高效解决方案。





## [<span id ="hpca2507-1">M-ANT: Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type</span>](#hpca2507)

​	该研究针对大型语言模型（LLMs）部署中内存占用大、推理延迟高的问题，提出了基于数学自适应数值类型（MANT）的高效低比特组量化方案，通过灵活的数学编码范式（调整系数a适配不同数据分布）、编解码与计算融合机制，以及针对KV缓存动态特性设计的空间-时间二维实时量化策略，同时优化硬件架构（扩展脉动阵列处理单元、新增实时量化单元），在LLaMA、OPT等系列模型上实现了近FP16的精度（PPL损失低于0.3），相比现有SOTA加速器平均提速2.99倍（最高4.46倍）、能耗降低2.81倍（最高4.10倍），且硬件开销可忽略，有效解决了组量化中组内分布多样性、KV缓存实时量化等核心挑战。

### 一、研究背景与挑战

1. **LLMs 部署痛点**：LLMs（如 Llama3-405B）参数规模庞大，内存需求远超现有硬件（如 H100 GPU 仅 80GB 内存），且自回归推理导致计算资源利用率低，内存带宽成为瓶颈。
2. **现有量化方案的局限**：
   - 组量化已成为主流方案，但现有自适应数据类型（如 ANT、GOBO）难以适配组内数据分布的高多样性（不同组分布差异显著，而张量 / 通道级分布相近）。
   - 基于数据类型的自适应方法（如 ANT）适配性有限，扩展数据类型会增加硬件开销；基于聚类的方法（如 GOBO）虽适配性强，但编码和计算效率低。
   - 动态生成的 KV 缓存（占长序列推理内存的 70% 以上）难以实现实时组量化，现有方案多回避该问题。

### 二、核心方案：MANT（数学自适应数值类型）

#### 1. 核心设计理念

通过灵活的数学编码范式，支持 “无限” 连续的数据分布变体，同时融合解码与计算过程，兼顾适配性与效率。

- **映射表示**：定义量化网格公式 `Value grid = ±(a×|INT|+2^|INT|)`，通过调整系数`a`（0-128，8 比特编码）适配不同分布（如`a=0`匹配 PoT 类型，`a=17`近似 Float，`a=25`近似 NF 类型）。
- **编解码与计算融合**：权重离线编码（选择最优`a`最小化 MSE），激活采用 INT8 量化；解码过程与矩阵乘法融合，无需专用解码器，通过整数乘法和移位操作完成计算，降低开销。

#### 2. KV 缓存实时量化机制

针对 KV 缓存动态生成的特性，提出空间 - 时间二维量化策略：

- **K 缓存（空间量化）**：组内元素同步生成，分配专用量化单元实时处理。
- **V 缓存（时间量化）**：组内元素分迭代生成，采用两阶段量化（先 INT8 临时存储，积累满一组后通过方差选择`a`，再量化为 4 比特 MANT），平衡实时性与精度。

#### 3. 硬件架构优化

- 基于脉动阵列扩展处理单元（PE），集成 MAC（乘加）和 SAC（移位累加）组件，支持混合精度计算（2/4/8 比特权重 / KV 缓存与 8 比特激活）。
- 新增实时量化单元（RQU），通过流水线隐藏量化延迟，支持空间 / 时间数据流模式。

### 三、实验结果

1. **精度表现**：在 LLaMA、OPT 系列模型上，MANT（4 比特权重 + 8 比特激活 + 4 比特 KV 缓存）的困惑度（PPL）接近 FP16，精度损失低于 0.3；生成任务（TruthfulQA、TriviaQA）中，BLEU/F1 分数损失低于 1.7%。
2. **性能与能耗**：
   - 相比现有 SOTA 加速器（Tender、OliVe、ANT），平均提速 2.99 倍（最高 4.46 倍），平均能耗降低 2.81 倍（最高 4.10 倍）。
   - 长序列（128K）推理中，因 KV 缓存量化的优势，性能提升更为显著，远超未量化 KV 缓存的基线方案。
3. **硬件开销**：新增组件（RQU、PE 扩展）的面积开销可忽略，与现有架构兼容性强。

### 四、核心贡献

1. 提出 MANT 数值类型，首次实现组级细粒度自适应量化，兼顾分布适配性与计算效率。
2. 设计 KV 缓存实时量化框架，解决动态生成缓存的低比特量化难题。
3. 硬件 - 算法协同优化，实现权重、激活、KV 缓存的统一量化，在精度损失极小的前提下，大幅提升 LLMs 推理的速度与能效。





## [<span id ="hpca2508-1">throttLL’eM: Predictive GPU Throttling for Energy Efficient LLM Inference Serving</span>](#hpca2508)
​	该文档提出了名为throttLL’eM的LLM推理服务框架，针对LLM推理能耗高且需满足低延迟SLO的核心矛盾，通过生成长度预测、KV缓存与批大小预测、基于梯度提升决策树的性能预测模型、查询调度与准入控制、张量并行自动扩缩容及GPU频率节流控制等模块，动态适配LLM自回归特性、动态批处理和可变内存占用等挑战，在AWS p4d.24xlarge等环境下基于Llama系列及Mixtral 8x7B模型的实验表明，该框架相比NVIDIA Triton服务器，在满足p99分位响应延迟和TBT等SLO的前提下，最高可降低43.8%的能耗，能效提升至少1.71×，且在不同模型、硬件配置和负载场景下均表现稳定，为LLM推理服务提供了高效的能效优化方案。

### 一、研究背景与挑战

1. **LLM 推理的能效问题**：LLM 依赖高功耗 GPU，推理阶段占数据中心计算周期的 90% 以上，能源消耗巨大（单条 GPT-4 响应约 3Wh），2026 年相关能耗预计达 1050 太瓦时，兼具环境与成本压力。
2. **核心矛盾**：降低能耗与保障低延迟 SLO（如 99 分位响应延迟、符合人类阅读速率的令牌间隔时间 TBT）存在冲突。
3. **三大技术障碍**：
   - 自回归特性：令牌生成长度动态变化，难以预分配资源；
   - 动态批处理：批大小波动导致性能变异（延迟最高增加 45%）；
   - 可变内存占用：KV 缓存随序列长度增长动态变化，使用率提升可导致 18.2% 的性能下降。
4. **传统方案局限**：静态功率限制、超分配置、延迟批处理等技术无法适配 LLM 的动态特性，“竞速至空闲” 策略因持续请求流失效。

### 二、throttLL’eM 框架设计

框架核心是通过动态 GPU 频率调节和实例自动扩缩容，在满足 SLO 的前提下最小化能耗，关键模块包括：

1. **生成长度预测**：估算输入查询的输出令牌长度，为后续资源预测奠定基础（现有方案误差约 30%，框架通过保守调整缓解误差影响）。
2. **KV 缓存与批大小预测**：基于生成长度预测和调度元数据（如查询调度迭代、输入长度），通过解析模型预测未来各迭代的 KV 缓存使用率和批大小，平均误差分别为 2.26% 和 0.19%。
3. **性能预测模型**：采用梯度提升决策树（GBDT），输入引擎规模、批大小、KV 缓存使用率和 GPU 频率，预测迭代级吞吐量（IPS），R² 分数超 0.97，平均误差低于 1 IPS。
4. **查询调度与准入控制**：基于上述预测验证 SLO 合规性（KV 缓存容量、TBT、端到端延迟 E2E），避免违规查询直接调度，未合规查询进入队列等待。
5. **自动扩缩容机制**：基于请求率（RPS）动态调整张量并行（TP）级别，采用 “影子实例” 掩盖服务启动延迟（>20s），通过宽限期策略平衡扩缩容与能耗。
6. **GPU 频率节流控制器**：通过二分查找确定满足 SLO 的最小 GPU 频率，统一应用于所有服务 GPU，避免同步问题；对 SLO 风险查询采用最大频率。

### 三、关键实验发现

1. **性能 - 能效权衡**：批大小增大提升吞吐量和能效，但恶化延迟；GPU 频率存在 “能效甜点”（如 1050MHz），低于该频率会因吞吐量下降抵消能耗收益。
2. **KV 缓存影响**：KV 缓存使用率与 TBT 正相关（皮尔逊系数 0.92）、与吞吐量负相关（-0.92），可作为性能建模的有效代理指标。
3. **并行策略优劣**：张量并行（TP）在吞吐量和能效上显著优于数据并行（DDP）和流水线并行（PP），小并行度（如 TP2）比大并行度（如 TP4）更节能。
4. **工作负载特性**：LLM 查询的令牌长度（输入 0-4000 tokens、输出 10-700 tokens）和请求到达率（RPS 1-16）具有高波动性，需动态资源分配。

### 四、实验结果

1. **与 NVIDIA Triton 对比**：
   - 无自动扩缩容时，平均节能 24.7%，最高 30.7%，能效提升 30%-36.3%；
   - 启用自动扩缩容后，最高节能 43.8%（30% 预测误差下仍达 41.7%），能效提升 1.71×-1.78×；
   - 所有配置均满足 SLO（TBT 平均 < 50ms，E2E 99 分位延迟低于阈值）。
2. **与其他方案对比**：
   - 比 ReTail 等 DVFS 方案节能 11.9%， latency 预测更精准（R² 0.97 vs 0.4）；
   - 在 Mixtral 8x7B 模型 + NVIDIA A10G GPU 环境中，仍实现 35.9% 的功耗降低，且满足 SLO。
3. **额外收益**：频率节流间接增大批大小（几何平均提升 24.5%），减少迭代次数（降低 24.3%），进一步优化能效。

### 五、结论与局限

1. 核心贡献：提出基于预测模型的细粒度 GPU 频率调节与自动扩缩容结合方案，首次系统分析 LLM 推理的能效关键因素，实现 SLO 约束下的大幅节能。
2. 局限：未优化计算密集型的预填充阶段；跨节点扩展需结合流水线并行；生成长度预测误差仍可能影响性能。
3. 代码开源：https://github.com/WilliamBlaskowicz/throttLL-eM



## [<span id ="hpca2509-1">VQ-LLM: High-performance Code Generation for Vector Quantization Augmented LLM Inference</span>](#hpca2509)

​	VQ-LLM 是一款面向向量量化（VQ）增强型大语言模型（LLM）推理的高性能融合内核生成框架，旨在解决现有 VQ 算法虽能实现更高压缩比和精度却难以转化为实际 latency 提升的问题，核心通过“码本缓存”（基于访问频率在 GPU 多存储层级自适应分配码本条目，减少银行冲突与资源浪费）和“码本中心计算引擎”（含码本中心数据流与层级融合技术，优化码本加载与计算协同、解决数据布局不匹配问题），结合自适应启发式策略适配 VQ 算法与 LLM 计算内核的多样性，最终实现对现有开源实现 64.36%~99.1% 的 latency 降低，在 4 位等效精度下 latency 与主流逐元素量化方法（如 AWQ、QoQ）相当甚至更优，且准确率更高，端到端推理中能实现 2.2 倍提速，大幅降低内存占用，在带宽受限环境中表现尤为突出。

### 一、研究背景与挑战

1. **LLM 推理的内存瓶颈**：LLM（如 Llama 系列）的权重和 KV 缓存占内存足迹的 95% 以上，量化是关键优化手段。传统逐元素量化（如 AWQ、QoQ）受限于 4 位精度（进一步压缩会导致精度暴跌），而**向量量化（VQ）** 以向量为压缩单元，可在 2 位甚至 1 位精度下保持高准确率，且压缩比更高。
2. **VQ 的核心痛点**：现有 VQ 集成方案未能将内存节省转化为实际 latency 提升，反而因以下问题导致性能劣于 FP16 基线：
   - 码本访问低效：全量码本存入共享内存会导致线程块并发度下降、银行冲突严重；存入全局内存则访问延迟极高。
   - 码本加载与计算不协调：多线程块重复加载码本导致全局内存流量激增，且解量化数据布局与后续计算需求不匹配，引发额外共享内存 - 寄存器数据搬运。
   - 多样性适配困难：VQ 算法（向量大小、码本条目数、残差次数等参数不同）与 LLM 计算内核（GeMM、GeMV、注意力机制）组合多样，手动优化难以覆盖所有场景。

### 二、核心设计：VQ-LLM 框架

VQ-LLM 通过 “码本缓存” 和 “码本中心计算引擎” 两大核心组件，结合自适应启发式策略，解决上述挑战：

#### 1. 码本缓存（Codebook Cache）

- 核心思想：基于码本条目的访问频率，在 GPU 存储层级（全局内存、共享内存、寄存器）中自适应分配，而非全量存入单一内存。
  - 冷条目（低访问频率）：存于全局内存，节省共享内存空间。
  - 中热条目（中等访问频率）：存于共享内存，平衡访问速度与并发度。
  - 极热条目（高访问频率）：存于线程本地寄存器，彻底消除银行冲突。
- 自适应优化：通过离线 profiling 识别 GPU 资源冗余（共享内存 / 寄存器 slack），动态调整各层级码本条目数量，不影响原有计算的并发度。

#### 2. 码本中心计算引擎

围绕码本缓存设计，优化内存流量与计算协同：

- （1）码本中心数据流（Codebook-Centric Dataflow）
  - 按码本切换维度拆分计算任务，使每个线程块仅加载一次对应码本，避免重复加载导致的全局内存流量浪费。
  - 自适应调整拆分因子，平衡全局归约开销与码本访问效率。
- （2）码本中心层级融合（Codebook-Centric Hierarchical Fusion）
  - 寄存器级融合：利用 GPU 的 intra-warp 数据交换指令（shfl_xor），直接在寄存器中重排解量化数据，适配后续计算布局，跳过共享内存中间环节。
  - 共享内存级融合：针对需多次洗牌（shuffle）的场景，保留共享内存融合，自适应选择融合层级。

#### 3. 自适应启发式策略

根据 VQ 配置（向量大小、码本条目数）、计算内核类型（GeMM/GeMV/ 注意力）和 GPU 硬件特性，自动优化：

- 码本缓存的层级分配边界（n_reg、n_shared）。
- 计算任务的拆分因子。
- 融合层级（寄存器 / 共享内存）。

### 三、实验结果

1. **相对现有 VQ 实现**：在 RTX 4090/Tesla A40 GPU 上，VQ-LLM 对 GeMM、GeMV、注意力等内核的 latency 降低 64.36%~99.1%，平均降低 46.13%，最高提速 2.2 倍。
2. **与逐元素量化对比**：在 4 位等效精度下，VQ-LLM 的 latency 与 AWQ（GeMM/GeMV）、QoQ（注意力）相当甚至更优（如 GeMV 达 0.88×），且准确率更高（arc-challenge 任务提升 2.5%）。
3. **端到端性能**：对 Llama-7B/65B 模型，VQ-LLM 在 4 位精度下实现 2.2× 提速，内存占用从 22GB 降至 6GB 以下；2 位精度下仍保持实用准确率，且在带宽受限场景（如 Tesla A40）提速更显著。
4. **优化分解验证**：码本缓存解决了共享内存占用与银行冲突问题，计算引擎消除了重复码本加载与布局不匹配的额外开销，两者协同贡献主要性能提升。

### 四、核心贡献

1. 首次深入分析 VQ 在 LLM 推理中的性能瓶颈，明确码本访问低效与计算协同不足是关键问题。
2. 提出码本缓存抽象，实现码本在 GPU 存储层级的自适应分配，平衡访问速度与硬件利用率。
3. 设计码本中心计算引擎，通过数据流拆分与层级融合，优化内存流量与数据布局适配。
4. 构建自动化内核生成框架，支持多样 VQ 配置与 LLM 计算内核，验证了 VQ 在 LLM 推理中 “高压缩比 + 高准确率 + 低 latency” 的可行性。



#  [<span id ="mlsys-title"> MLsys</span>](#mlsys)

## 2025

<span id ="mlsys2501">[COMET: FINE-GRAINED COMPUTATION-COMMUNICATION OVERLAPPING  FOR MIXTURE-OF-EXPERTS](#mlsys2501-1)</span>

<span id ="mlsys2502">[RETHINKING KEY-VALUE CACHE COMPRESSION TECHNIQUES FOR LARGE  LANGUAGE MODEL SERVING](#mlsys2502-1)</span>

<span id ="mlsys2503">[EFFICIENT LLM INFERENCE USING  DYNAMIC INPUT PRUNING AND CACHE-AWARE MASKING](#mlsys2503-1)</span>

<span id ="mlsys2504">[TRAINING ULTRA LONG CONTEXT LANGUAGE MODEL WITH FULLY  PIPELINED DISTRIBUTED TRANSFORMER](#mlsys2504-1)</span>

<span id ="mlsys2505">[QSERVE: W4A8KV4 QUANTIZATION AND SYSTEM CO-DESIGN FOR  EFFICIENT LLM SERVING](#mlsys2505-1)</span>



## [<span id ="mlsys2501-1">COMET: FINE-GRAINED COMPUTATION-COMMUNICATION OVERLAPPING  FOR MIXTURE-OF-EXPERTS</span>](#mlsys2501)

​	COMET 是字节跳动提出的一款针对混合专家（MoE）模型的优化系统，旨在解决分布式部署中 MoE 模型通信开销大（占比可达 47%）且现有粗粒度计算 - 通信重叠方案效率不足的问题，其核心通过基于共享张量的依赖解析（分解张量并重组调度以消除粒度不匹配）和自适应负载分配（线程块专业化隔离与动态分配以平衡资源）实现细粒度计算 - 通信重叠，在 Nvidia H800/L20 集群上经 Mixtral 8x7B、Qwen2-MoE 等模型验证，单 MoE 层加速比达 1.96×，端到端平均加速比 1.71×，相比主流方案 latency 降低 31.8%-44.4%，已部署于万卡级生产集群，节省数百万 GPU 小时，且开源于https://github.com/bytedance/flux，支持多种 GPU 架构与并行策略，适配性强。

### 一、研究背景与问题

1. **MoE 模型的优势与挑战**：MoE 通过稀疏激活机制，在维持计算成本不变的前提下将模型参数扩展到万亿级，已广泛应用于 NLP、计算机视觉等领域。但分布式部署中，专家需跨 GPU 分配，导致设备间通信开销极高，热门模型的 MoE 层通信时间占比可达 47%。
2. **现有方案的不足**：现有方法通过粗粒度流水线实现计算与通信重叠，但存在两大缺陷：一是数据分块过细导致 GPU 计算资源利用率低，存在空闲时间；二是 MoE 动态路由导致 GPU 负载不均，独立内核调度难以实现高效重叠， latency 隐藏效果不佳。

### 二、COMET 系统核心设计

COMET 通过两大关键机制实现细粒度计算 - 通信重叠，兼顾效率与适应性：

1. **基于共享张量的依赖解析**
   - 分解共享张量：分析计算与通信操作对共享张量的访问模式，沿数据独立维度（如 token 维度 M 或嵌入维度 N）分解张量，打破粗粒度依赖。
   - 重组调度子张量：将分解后的子张量按计算 tile 粒度重组，优先调度本地数据或可立即使用的远程数据，让计算提前启动，减少等待。
2. **自适应负载分配**
   - 线程块专业化：将通信与计算任务隔离到不同线程块，避免通信对计算效率的干扰，同时保持 GEMM 等核心计算的原生高性能实现。
   - 动态线程块分配：预编译多种线程块分配比例的内核，运行时根据输入形状、模型配置（如并行策略）自适应选择最优比例，平衡通信与计算延迟，减少流水线气泡。

### 三、技术实现细节

1. **底层优化**：基于 CUTLASS 生成高效 GEMM 内核，利用 NVSHMEM 实现细粒度 GPU 间通信，支持统一虚拟地址空间的直接数据访问。
2. **框架集成**：兼容 Megatron-LM，提供 Python API，可无缝集成到现有 MoE 训练与推理流程，支持专家并行、张量并行及混合并行策略。
3. **资源开销**：通信缓冲区内存消耗可忽略（Mixtral 8x7B 模型在 M=8192 时仅需 64MB），适配 Hopper、Ampere 等主流 GPU 架构。

### 四、实验结果

1. **性能提升**：在 Nvidia H800/L20 集群上，针对 Mixtral 8x7B、Qwen2-MoE、Phi3.5-MoE 等模型，COMET 单 MoE 层加速比达 1.96×，端到端平均加速比 1.71×，相比 Megatron-CUTLASS、TUTEL 等主流方案， latency 降低 31.8%-44.4%。
2. **适应性验证**：在不同输入长度、专家数量（E）、路由策略（topk）、并行配置及负载不均衡场景下，均保持稳定优势；在带宽受限的 L20 集群中，仍实现 1.19×-1.46× 的加速。
3. **工程价值**：已部署于万卡级 GPU 生产集群，累计节省数百万 GPU 小时算力。

### 五、核心贡献与开源

1. 提出细粒度计算 - 通信重叠方案，解决了 MoE 中粒度不匹配与动态负载两大核心问题。
2. 设计融合内核与自适应调度机制，兼顾高性能与通用性。
3. 开源地址：https://github.com/bytedance/flux，支持编译器扩展（如 Triton、TVM），为 MoE 系统优化提供参考。



## [<span id ="mlsys2502-1">RETHINKING KEY-VALUE CACHE COMPRESSION TECHNIQUES FOR LARGE  LANGUAGE MODEL SERVING</span>](#mlsys2502)

​	该研究聚焦 LLM 服务中的 KV cache 压缩技术，指出量化和稀疏化两类主流算法虽能降低内存消耗，但在生产部署中因存在计算效率不足、易导致响应变长、个体样本准确性受损等问题未获广泛应用，且现有评估忽视吞吐量、响应长度分布及负面样本三大关键维度；通过对 LLaMA、Mistral 系列模型及代表性算法的实证分析，揭示了压缩算法在特定批次大小、序列长度下的性能劣化，以及对摘要、问答等任务的敏感性；为此，研究开源了吞吐量预测器、长度预测器及负面样本评估数据集三类工具，结合请求路由策略可有效提升压缩技术的实际部署效率，为 LLM KV cache 压缩的优化与落地提供了关键支撑。

### 一、研究背景与核心问题

1. **KV cache 的关键作用与挑战**：KV cache 是 LLM 服务的主要性能瓶颈，其高内存占用导致硬件成本高昂（如 LLaMA3-70B 模型 FP16 格式下 KV cache 需 512GB）。现有压缩技术虽能减少内存消耗，但在生产环境中应用不普遍，核心痛点在于未充分解决计算效率、响应长度影响及个体样本准确性等实际问题。
2. **压缩技术分类**：
   - 量化类方法：将 KV cache 转为低精度表示（如 INT8），需平衡内存节省与精度损失，代表算法有 KIVI、GEAR 等。
   - 稀疏类方法：剔除或迁移不重要的 KV 对，核心是精准评估 token 重要性，代表算法有 StreamingLLM、H2O 等。

### 二、核心研究发现

#### （一）现有技术的三大 “缺失环节”

1. 计算效率评估不足：多数研究仅基于 TRL 框架测量吞吐量，忽略 FlashAttention、PagedAttention 等生产级优化技术，且缺乏多 GPU 张量并行场景的评估。
2. 响应长度影响被忽视：压缩算法可能导致输出响应变长，抵消吞吐量提升优势，最终增加端到端延迟。
3. 个体样本准确性缺失：现有研究侧重整体精度，忽略 “负面样本”（压缩后从良性变劣化的样本），且在长文本任务中表现脆弱。

#### （二）实证分析关键结论

1. **计算效率**：
   - 压缩算法在特定批次大小、序列长度和张量并行配置下可能降低吞吐量（如 GEAR、H2O 在长 prompt 场景下预填充吞吐量低于 FP16 基线）。
   - 与 FlashAttention/PagedAttention 集成后，压缩算法的性能增益会显著减弱，因现有压缩设计与这些优化技术兼容性不足。
2. **响应长度**：
   - 有损压缩倾向于生成更长响应（超 20% 样本响应长度增加 1.5 倍以上），且压缩比越高，响应变长越明显，这可能是算法弥补精度损失的隐性方式。
   - 变长响应会抵消吞吐量提升，导致端到端延迟增加（如 GEAR 的尾延迟显著高于基线）。
3. **样本准确性**：
   - 所有压缩算法均存在 “负面样本”，即使整体精度损失轻微，部分样本（尤其长文本任务）精度下降超 10%。
   - 摘要、问答任务对压缩高度敏感，因这类任务依赖完整上下文信息，KV cache 的信息丢失会严重影响结果。

### 三、解决方案：三类支撑工具

为推动压缩技术的实际部署，研究开源了一套工具集（https://github.com/LLMkvsys/rethink-kv-compression）：

1. 吞吐量预测器：基于离线 profiling 数据，预测不同批次大小、序列长度下压缩算法的吞吐量表现，准确率超 85%，辅助服务调度决策。
2. 长度预测器：采用 BERT-based 分类器，预测压缩算法导致的响应长度变化，准确率超 85%，避免因变长响应增加延迟。
3. 负面样本评估数据集：筛选长文本任务中压缩后精度显著下降的样本，为算法优化提供公平评估基准。

### 四、实验验证

1. **实验设置**：基于 LLaMA、Mistral 系列模型，采用 ShareGPT（吞吐量 / 长度分析）和 LongBench（负面样本分析）数据集，在 A6000/H800 GPU 及 TRL、LMDeploy 等框架下验证。
2. **关键结果**：
   - 组合吞吐量与长度预测器的请求路由策略，可使端到端延迟降低 1.45-1.80 倍。
   - 压缩算法在长 KV cache、大批次场景下更具优势，但在短序列、小批次场景可能性能劣化。

### 五、结论

研究强调，KV cache 压缩技术的评估需涵盖吞吐量、响应长度分布、负面样本三大核心维度。现有算法需在兼容性（与 FlashAttention/PagedAttention 适配）、任务特异性（针对摘要 / 问答等敏感任务优化）、参数调优复杂性上进一步改进，而文中提出的工具集为实际部署提供了关键支撑。



## [<span id ="mlsys2503-1">EFFICIENT LLM INFERENCE USING  DYNAMIC INPUT PRUNING AND CACHE-AWARE MASKING</span>](#mlsys2503)

​	该研究针对移动设备 DRAM 带宽与容量不足导致的 LLM 推理瓶颈，尤其是现代 LLM 采用 SwiGLU 激活函数缺乏天然稀疏性的问题，提出了无需预测器的动态输入剪枝（DIP）方案，通过逐 token top-K 幅度剪枝 MLP 层输入和中间激活，并结合轻量级 LoRA 适配器补偿精度损失，同时设计了缓存感知变体（DIP-CA），通过优先选择缓存内权重提升命中率，在 Phi-3-Medium 等模型上实现了 46% 内存占用降低、40% 吞吐量提升且困惑度损失 < 0.1 的效果，显著优于现有静态剪枝、动态稀疏及量化方法，为移动设备高效运行 LLM 提供了有效解决方案。

### 一、研究背景与挑战

1. **硬件矛盾**：移动设备 NPU 计算能力快速提升，但 DRAM 带宽和容量增长缓慢，而 LLM 参数量（数十亿至万亿级）需频繁访问权重，导致内存成为推理瓶颈（如 14B 参数的 Phi-3-Medium 经 INT4 量化后约占 7GB，接近手机平均 DRAM 容量）。
2. **现有方法局限**：传统动态稀疏性方法依赖 ReLU 激活函数的天然稀疏性，通过预测零激活跳过部分权重加载，但现代 LLM 多采用 SwiGLU，几乎无天然稀疏性；若通过幅度剪枝实现稀疏，其模式难以预测，且现有方法需昂贵的重训练（如替换为 ReLU 再微调），效果不佳。

### 二、核心方案

#### 1. 动态输入剪枝（DIP）

- **核心思想**：无需预测器，通过逐 token 的 top-K 幅度剪枝，直接移除 MLP 层输入和中间 GLU 块中的小幅度激活，同时对 MLP 的 up、gate、down 三个权重矩阵进行稀疏化，避免预测误差。
- **性能补偿**：引入轻量级 LoRA 适配器，通过少量微调（匹配稠密模型 logits 的知识蒸馏损失）恢复剪枝导致的精度损失，且适配器可融合到原始矩阵，无额外内存开销。

#### 2. 缓存感知掩码策略（DIP-CA）

- **优化目标**：在剪枝时考虑 DRAM 缓存状态，提升缓存命中率，减少 Flash 加载延迟（Flash 读取速度远低于 DRAM）。
- **实现方式**：通过权重调整机制，对缓存中已存在的神经元激活幅度添加惩罚系数 γ，优先选择缓存内权重，同时不影响强激活神经元的选择（即使未命中缓存），平衡精度与吞吐量。

### 三、关键实验与结果

1. **实验设置**：基于 Apple A18 硬件模拟器，在 Phi-3-Medium、Llama 3 8B 等 4 个 SwiGLU LLM 上，对比 DIP 与静态剪枝（SparseGPT）、动态剪枝（DejaVu、CATS）、量化（GPTQ）等方法，评估指标包括困惑度（WikiText-2）、下游任务精度（MMLU 5-shot）、内存占用和吞吐量。
2. **核心结果**
   - **精度 - 内存权衡**：在 50% MLP 稀疏度下，DIP+LoRA 的困惑度（Phi-3-Medium 为 5.01）和 MMLU 精度（75.89%）显著优于 DejaVu、CATS+LoRA 等方法，仅略低于稠密模型。
   - **内存与吞吐量优化**：Phi-3-Medium 上，DIP-CA 实现 46% 内存占用 reduction，困惑度损失 < 0.1 时吞吐量提升 40%；困惑度损失 0.5 时，吞吐量较稠密模型提升 93%，且优于所有对比方法。
   - **兼容性**：DIP 可与量化（如 4 位 BQ、3 位 VQ）结合，进一步提升效率，表现优于单纯的低比特量化或静态剪枝。
3. **缓存策略对比**：DIP-CA 采用的 LFU 缓存 + 缓存感知掩码，在相同困惑度下吞吐量超过 LRU、Belady 最优缓存等策略，验证了缓存与剪枝协同优化的有效性。

### 四、主要贡献

1. 揭示了传统动态稀疏方法在 SwiGLU LLM 上的局限性，证明预测器难以适配其激活模式。
2. 提出无需预测器、仅需少量微调的 DIP 方案，结合 LoRA 有效平衡稀疏性与精度。
3. 设计缓存感知掩码策略（DIP-CA），显著提升移动设备 LLM 推理吞吐量，降低内存压力。

### 五、未来方向

进一步优化稀疏性技术，拓展其在更多模型和硬件配置中的适用性，包括设备端模型性能仿真。





## [<span id ="mlsys2504-1">TRAINING ULTRA LONG CONTEXT LANGUAGE MODEL WITH FULLY  PIPELINED DISTRIBUTED TRANSFORMER</span>](#mlsys2504)

​	该论文提出全流水线分布式 Transformer（FPDT）方案，旨在解决超长上下文大型语言模型（LLM）训练中 GPU 资源需求高、内存压力大的核心问题，其通过序列分块、CPU 内存卸载、双缓冲设计等关键技术，结合 DeepSpeed ZeRO-3 等优化方案，在相同硬件条件下将 GPT、Llama 等 2.7B-70B 规模模型的训练序列长度提升 16 倍，仅需 4 块 GPU 即可训练 8B 模型（200 万 tokens 序列）、32 块 GPU 可训练 70B 模型（400 万 tokens 序列），同时保持 55% 以上的模型浮点运算利用率（MFU），且兼容现有训练技术，为资源受限场景下的超长上下文 LLM 训练提供了高效可行的解决方案。

### 一、研究背景与挑战

1. **需求迫切性**：LLM 的超长上下文能力对法律文档分析、长文本生成、多领域复杂推理等任务至关重要，但现有模型上下文窗口多局限于 8K-32K tokens。
2. **核心瓶颈**：
   - 直接训练超长上下文需海量 GPU 资源，激活值和中间缓冲区占用内存随序列长度激增，易导致显存溢出；
   - 现有位置编码（如 RoPE）的缩放调整会导致模型性能下降；
   - 主流方案（如 Megatron-SP、DeepSpeed Ulysses）需大量 GPU，硬件成本极高，难以普及。

### 二、核心方案：全流水线分布式 Transformer（FPDT）

1. **设计理念**：利用 GPU 与主机 CPU 的多级存储架构，通过分块、卸载和双缓冲设计，在降低显存占用的同时隐藏数据传输延迟，实现高效流水线训练。
2. **关键技术**：
   - **序列分块与卸载**：将输入序列切片为多个子块，仅在 GPU 显存中保留当前计算所需块，其余块卸载至 CPU 内存，减少显存占用；
   - **双缓冲机制**：通过多 CUDA 流并行处理数据预取与计算，解决 PCIe 传输带宽与 GPU 计算吞吐量不匹配问题，避免 GPU 闲置；
   - **适配现有优化技术**：与 DeepSpeed ZeRO 系列、PyTorch FSDP 等内存优化方案正交兼容，可应用于 GPT、Llama 等各类 Transformer 模型。

### 三、实验结果

1. **硬件效率突破**：在相同硬件条件下，FPDT 支持的序列长度较现有方案提升 16 倍，如 4 块 GPU 可训练 8B 模型（200 万 tokens 序列），且保持 55% 以上的模型浮点运算利用率（MFU）。
2. **多模型适配性**：支持 2.7B-70B 规模的 GPT 和 Llama 模型，32 块 GPU 可训练 70B 模型（400 万 tokens 序列）。
3. **关键参数优化**：64K 为最优分块大小，可平衡显存占用与计算效率；FFN 层分块数设为注意力层的 2 倍时，能有效控制内存峰值。
4. **兼容性验证**：与激活检查点、CPU 卸载、稀疏注意力等技术结合，进一步优化内存占用与训练效率，且不影响模型收敛性。

### 四、对比与优势

| 方案              | 核心不足                       | FPDT 优势                    |
| ----------------- | ------------------------------ | ---------------------------- |
| Megatron-SP       | 跨节点通信性能退化，需大量 GPU | 低硬件需求，跨节点扩展高效   |
| DeepSpeed Ulysses | 长序列训练显存压力大           | 分块 + 卸载大幅降低显存占用  |
| Ring Attention    | 依赖设备数量缩放，网络延迟敏感 | 负载均衡，对网络环境适应性强 |

### 五、未来方向

1. 解决 PyTorch 梯度归约过程中的内存峰值问题；
2. 扩展对任意长度序列的支持（当前仅支持 2 的幂次长度序列）。

### 核心贡献

提出了一套低资源需求、高硬件效率的超长上下文 LLM 训练方案，突破了现有技术的硬件限制，为科研机构和中小企业开展相关研究提供了可行路径，且代码已开源集成至 DeepSpeed 生态。





## [<span id ="mlsys2505-1">QSERVE: W4A8KV4 QUANTIZATION AND SYSTEM CO-DESIGN FOR  EFFICIENT LLM SERVING</span>](#mlsys2505)

​	QServe 是一款通过算法与系统协同设计实现 LLM 高效部署的框架，其核心是 QoQ 量化算法与配套系统优化，采用 W4A8KV4（4 位权重、8 位激活、4 位 KV 缓存）量化精度，通过渐进式分组量化降低反量化开销、SmoothAttention 缓解 KV4 量化的精度损失，结合计算感知权重重排序、寄存器级并行等系统优化，在 A100 和 L40S GPU 上对多款主流 LLM 实现了 1.2-3.5 倍于 TensorRT-LLM 的吞吐量提升，且 L40S 上的性能可超越 A100 上的 TensorRT-LLM，同时保持接近 FP16 的模型精度，大幅降低了云端 LLM 服务的硬件成本。

## 核心背景与问题

大型语言模型（LLM）虽能力强大，但庞大的模型规模给部署带来巨大挑战，量化是实现高效推理的关键技术。当前主流整数量化方案（W8A8、W4A16、W4A4）存在明显缺陷：W4A4 虽理论上可借助 4 位张量核提升吞吐量，但存在显著精度损失，且在 GPU 上执行时因反量化操作依赖低吞吐量的 CUDA 核心，导致实际性能甚至低于 W8A8 和 W4A16；W4A16 则需在 GEMM 内核中进行权重反量化，带来额外开销。现有 4 位量化方案在云端大批次 LLM 服务场景中，反量化过程会产生 20%-90% 的运行时开销，无法充分发挥低精度量化的性能优势。

## 核心方案：QoQ 量化算法与 QServe 系统

### 1. QoQ 量化算法（W4A8KV4）

QoQ（Quattuor-Octō-Quattuor）采用 4 位权重（W4）、8 位激活（A8）、4 位 KV 缓存（KV4）的量化精度组合，通过两大核心设计平衡精度与性能：

- **渐进式分组量化**：先对权重进行通道级对称 INT8 量化（保护范围 [-119, 119]，避免反量化溢出），再对中间结果进行分组级非对称 INT4 量化。该设计确保所有 GEMM 运算可在 INT8 张量核上执行，大幅降低反量化开销。
- **SmoothAttention**：针对 KV4 量化导致的精度损失，将 Key 中的异常值缩放转移到未量化的 Query 上，通过通道级缩放因子 λ 平滑 Key 的分布，同时保证与旋转位置编码（RoPE）的兼容性，有效缓解 KV 低精度量化的精度退化问题。

### 2. QServe 系统优化

QServe 为 QoQ 算法提供高效系统支持，聚焦降低 GEMM 主循环开销和加速 KV4 注意力计算：

- **计算感知权重重排序**：按计算时的访问顺序重新排列权重，减少 CUDA 核心的指针运算开销，同时保证 128 位高带宽内存访问，解决 W4A8 存储与计算的数据类型不匹配问题。
- **快速反量化**：利用渐进式量化的保护范围，采用 “先乘后减” 的计算顺序，支持 4 路寄存器级并行（RLP），将 INT4 权重高效反量化为 INT8，避免 CUDA 核心成为瓶颈。
- **KV4 注意力加速**：通过将 FP32 运算替换为 FP16、采用位运算优化反量化过程、预取缩放因子等手段，延迟 CUDA 核心性能上限拐点，使注意力算子保持在内存受限区域，充分发挥 KV4 量化的带宽优势，相比 8 位 KV 缓存实现 1.5 倍速度提升。
- **其他优化**：融合激活量化与归一化层、采用分页 KV 缓存管理避免内存碎片、支持动态批处理等。

## 实验结果

### 1. 性能提升

在 A100 和 L40S GPU 上，针对 Llama-3-8B、Qwen1.5-72B 等 7 个主流 LLM 的测试显示：

- 相比 TensorRT-LLM，A100 上吞吐量提升 1.2-2.4 倍，L40S 上提升 1.5-3.5 倍；
- L40S 上的 QServe 吞吐量甚至超过 A100 上的 TensorRT-LLM，且硬件成本降低 3 倍；
- 相比 Atom、QuaRot 等 W4A4 量化系统，A100 上吞吐量提升 2.5-2.9 倍，同时精度显著更优。

### 2. 精度保持

- 在 WikiText2 数据集上，QoQ 的困惑度接近 W8A8 和 W4A16，远优于 W4A4 方案（如 Llama-2-7B 的困惑度仅比 FP16 高 0.2）；
- 零样本任务（PIQA、ARC、HellaSwag 等）中，相比 FP16 仅损失 0.4%-1.03% 的精度，显著优于 Atom 和 QuaRot；
- 长上下文任务（LongBench）中，与 BF16 基线相比精度退化极小。

### 3. 资源效率

通过 W4 和 KV4 量化，模型内存占用大幅降低，L40S（48GB 显存）可支持与 A100（80GB 显存）相同的批处理大小，同时保持更高吞吐量，显著降低云端 LLM 服务的硬件成本。

## 结论

QServe 通过量化算法与系统的协同设计，突破了现有低精度量化在云端大批次 LLM 服务中的性能瓶颈。W4A8KV4 的精度组合结合渐进式分组量化、SmoothAttention 等算法创新，以及计算感知权重重排序、快速反量化等系统优化，在保证模型精度的前提下，大幅提升了 LLM 推理的吞吐量，同时降低了硬件成本，为高效 LLM 云端部署提供了新方案。代码已开源（https://github.com/mit-han-lab/omniserve）。





# [<span id ="sosp-title">SOSP</span>](#sosp)

## 2025

<span id ="sosp2501">[Aegaeon: Effective GPU Pooling for Concurrent LLM Serving on the Market](#sosp2501-1)</span>

<span id ="sosp2502">[Oasis: Pooling PCIe Devices Over CXL to Boost Utilization](#sosp2502-1)</span>

<span id ="sosp2503">[PrefillOnly: An Inference Engine for Prefill-only Workloads in Large Language Model Applications](#sosp2503-1)</span>

<span id ="sosp2504">[Robust LLM Training Infrastructure at ByteDance](#sosp2504-1)</span>



##  [<span id ="sosp2501-1">Aegaeon: Effective GPU Pooling for Concurrent LLM Serving on the Market</span>](#sosp2501)

​	Aegaeon 是由北京大学与阿里巴巴集团联合提出的多模型服务系统，发表于 SOSP ’25 会议，其核心创新是采用令牌级自动扩展实现高效 GPU 池化，通过拆分预填充与解码阶段并设计差异化调度策略（预填充阶段分组 FCFS 调度、解码阶段加权轮询调度），结合组件复用、显式内存管理、细粒度 KV 缓存同步等优化将自动扩展开销降低 97%，解决了传统多路复用和请求级自动扩展方案支持模型数量有限、资源浪费严重的问题，实验及生产部署显示，该系统单 GPU 可支持多达 7 个模型并发服务，相比现有方案请求到达率提升 2-2.5 倍或吞吐量提升 1.5-9 倍，在阿里巴巴云模型工作室部署后实现 82% 的 GPU 资源节省，同时满足服务等级目标（SLO）要求。

### 一、研究背景与问题

1. **现状挑战**：模型市场（如 Hugging Face）存在大量特性各异、热度不均的 LLM，并发推理请求具有随机性和突发性 —— 长尾模型调用频率低导致 GPU 闲置（17.7% 的 GPU 仅处理 1.35% 的请求），热门模型则面临请求突发过载，传统专用 GPU 部署资源浪费严重。
2. **现有方案局限**：
   - 多路复用方案：受 GPU 内存限制，单 GPU 最多支持 2-3 个模型，池化效率低；
   - 自动扩展方案：基于请求粒度扩展，因 LLM 请求执行时间长，活跃模型数量多，导致队列头部阻塞（HOL blocking），实际仍难以突破 3 个模型 / GPU 的限制。

### 二、核心创新：令牌级自动扩展（Token-level Auto-scaling）

Aegaeon 的核心突破是将扩展粒度从 “请求级” 细化到 “令牌级”，通过抢占式缩放活跃模型、启动待处理模型，缓解阻塞并提升 GPU 利用率，支持单 GPU 最多 7 个模型并发服务。

#### 1. 关键技术方案

##### （1）令牌级调度策略

- 拆分预填充（Prefill）与解码（Decoding）阶段：预填充阶段处理用户提示所有令牌，解码阶段迭代生成后续令牌，两者分别调度以优化不同 SLO（服务等级目标）；
- 预填充阶段：采用分组 FCFS 调度，将同模型请求分组，减少频繁扩展开销，保证首令牌生成延迟（TTFT）；
- 解码阶段：采用加权轮询调度，利用令牌生成的缓冲特性（TBT，令牌间隔延迟可通过缓冲隐藏），分配时间配额，平衡多模型 SLO 达标率。

##### （2）高效抢占式自动扩展优化（降低 97% 扩展开销）

- 组件复用：推理引擎仅初始化一次，缓存分词器、分布式执行器等通用组件，仅动态加载模型权重和 KV 缓存；
- 显式内存管理：GPU 侧采用自管理缓冲区避免碎片，CPU 侧通过模型缓存和预取加速权重加载，KV 缓存采用 Slab 分配机制适配多模型形状；
- 细粒度 KV 缓存同步：通过 CUDA 事件跟踪传输状态，实现 KV 缓存换入 / 换出与推理的异步重叠，避免数据竞争。

### 三、系统架构

Aegaeon 包含代理层（负载均衡与状态同步）、预填充实例池、解码实例池，以及统一 CPU KV 缓存和模型缓存。请求经代理分发后，预填充和解码任务分别在专用 GPU 分区执行，调度器与抢占式扩展模块协同工作。

### 四、实验结果

1. **性能优势**：相较于 ServerlessLLM 和 MuxServe，Aegaeon 支持 2-2.5 倍更高的请求到达率，或 1.5-9 倍的吞吐量（Goodput），单 GPU 可稳定支持 7 个模型；
2. **资源节省**：在阿里巴巴云模型工作室部署后，服务数十个 1.8B-72B 参数模型的 GPU 需求从 1192 台降至 213 台，节省 82% GPU 资源；
3. **鲁棒性**：在不同请求强度、输入 / 输出长度、SLO 约束（最低 2s TTFT、20ms TBT）下均保持优势，适配低配置 GPU（A10）和超大规模模型（72B）。

### 五、核心贡献

1. 首次揭示市场并发 LLM 服务的资源浪费问题及量化分析；
2. 提出首个令牌级自动扩展的多 LLM 服务方案，针对预填充 / 解码阶段设计差异化调度；
3. 全栈优化抢占式扩展流程，降低 97% 开销；
4. 生产环境验证，显著降低运营成本（OPEX）。





##  [<span id ="sosp2502-1">Oasis: Pooling PCIe Devices Over CXL to Boost Utilization</span>](#sosp2502)

​	Oasis 是一款基于 CXL 内存池的软件化 PCIe 设备池化系统，旨在解决云平台中 NIC、SSD 等 PCIe 设备利用率低、成本高的问题，其通过复用已部署的 CXL 内存池作为跨主机数据通路，采用“前端驱动+后端驱动”的引擎架构及优化的非相干共享内存消息通道，在避免 PCIe 交换机高成本和 RDMA 方案局限性的同时，实现了设备灵活共享、负载均衡与故障转移，仅增加 4-7 μs 延迟开销，就能将 NIC 利用率提升 2 倍，NIC 故障转移时间控制在 38 ms 内，为数据中心提供了低成本、高性能的 PCIe 资源优化解决方案。

## 核心背景与问题

- **设备高成本与低利用率矛盾**：NIC 和 SSD 占服务器软硬件成本的 20%-40%，且功耗占比约 13% ，但普遍存在低利用率问题 ——Azure 数据显示，NIC 带宽 P99.99 利用率仅 20%，33% 的 SSD 容量和 27% 的 NIC 带宽处于闲置状态。
- **低利用率三大成因**：为满足峰值需求的保守分配、主机单一资源耗尽导致其他设备资源闲置、为容灾部署的冗余设备未充分利用。
- **现有方案局限**：PCIe 交换机成本高昂（单机架部署成本达 8 万美元）且灵活性差；RDMA 方案不支持 NIC 等部分 PCIe 设备，且延迟较高。

## 系统设计核心

- **底层依托 CXL 内存池**：复用已在数据中心部署的 CXL 内存池（用于提升内存利用率），以接近零额外成本构建 PCIe 设备池化的数据通路。CXL 2.0 提供充足带宽（单 CPU 插槽 256 GB/s）和低延迟（仅为 DDR5 内存的 2 倍左右），满足设备池化需求。
- **核心架构**：采用 “前端驱动 + 后端驱动” 的引擎设计，支持不同类型 PCIe 设备扩展。前端驱动运行于所有主机，为本地实例提供设备接口；后端驱动仅运行于连接物理设备的主机，通过原生驱动与设备交互，两者通过 CXL 共享内存的消息通道通信。
- **关键技术突破**：针对 CXL 2.0 无跨主机缓存一致性的问题，设计优化的非相干共享内存消息通道，吞吐量提升 29 倍；通过让 PCIe 设备直接 DMA 访问共享内存，避免不必要的缓存操作，降低性能开销。
- **控制平面**：通过 pod 级分配器管理设备与实例的映射，支持负载均衡、故障转移和资源调度策略。

## 核心功能与性能表现

- **设备池化与利用率提升**：实现多主机共享单一 PCIe 设备，NIC 利用率提升 2 倍；模拟实验显示，8 台主机组成的 pod 可减少 16% 的 NIC 带宽需求和 26% 的 SSD 容量需求。
- **低性能开销**：数据通路仅增加 4-7 μs 延迟，远低于云网络典型延迟（50-110 μs）；CXL 链路带宽占用低，忙时总带宽消耗不超过 13.5 GB/s。
- **高效故障转移**：NIC 故障时可在 38 ms 内切换至备份设备，UDP 传输仅短暂丢包；TCP 场景下恢复时间约 133 ms，无需应用层干预。
- **扩展支持**：已实现 NIC 池化的网络引擎，设计了 SSD 池化的存储引擎，支持容器和虚拟机环境。

## 总结

Oasis 创新性地复用 CXL 内存池实现 PCIe 设备软件池化，在控制成本的同时，显著提升设备利用率、降低性能开销并保障高可用性，为云数据中心资源优化提供了高效解决方案。



## [<span id ="sosp2503-1">PrefillOnly: An Inference Engine for Prefill-only Workloads in Large Language Model Applications</span>](#sosp2503)

​	该研究针对LLM在推荐、信用验证等判别式任务中仅需生成单个输出令牌的“预填充专属工作负载”，提出首个专用推理引擎PrefillOnly。其核心通过混合预填充（非注意力层分块处理、注意力层整体处理）大幅降低GPU内存占用，结合后缀KV缓存丢弃/卸载支持超长输入，再以带连续预填充时间估计的最短预填充优先调度提升缓存命中率与公平性；基于vLLM实现，经4类硬件、3种模型、2类任务评估，PrefillOnly在不增加平均延迟和P99延迟的前提下，可处理高达4倍的查询量，最大输入长度提升至5倍，为LLM在判别式任务中的规模化应用提供高效解决方案。

## 1. 研究背景与动机

### 1.1 新兴工作负载：预填充专属（Prefill-only）

大型语言模型（LLM）除传统生成式任务（如 ChatGPT 对话、GitHub Copilot 代码生成）外，正日益应用于推荐、信用验证、数据标注等传统判别式任务。这类场景的核心特征是 LLM 仅需生成单个输出令牌（如推荐场景的 “是 / 否” 概率分布），而非变长序列，此类工作负载被定义为 “预填充专属工作负载”—— 仅需执行 LLM 推理的预填充阶段，无需后续解码阶段。此外，文档嵌入生成、预填充 - 解码解耦架构中的预填充节点任务也属于该类工作负载。

### 1.2 现有 LLM 推理引擎的局限

现有 LLM 引擎为支持变长输出，存在两大核心缺陷：

- 内存效率低：需存储所有层的 KV 缓存以支持解码阶段复用，导致 GPU 内存占用量大，处理长输入时需依赖跨 GPU 并行或分块预填充等降低吞吐量的方案；
- 调度优化不足：因输出长度不确定，无法精准预测任务完成时间（JCT），只能采用先来先服务等非 JCT 感知调度策略，效率低下。

### 1.3 优化机遇

预填充专属工作负载具备两大未被充分利用的特性：

- 内存优化空间：仅生成单个令牌，多数 KV 缓存无需复用，可大幅减少活跃 GPU 内存占用；
- JCT 确定性：输出长度固定为 1，可精准预测任务完成时间，支持 JCT 感知调度。

## 2. PrefillOnly 核心设计

PrefillOnly 是首个专为预填充专属工作负载设计的 LLM 推理引擎，基于企业级引擎 vLLM 构建，通过三大核心技术充分发挥上述特性：

### 2.1 混合预填充（Hybrid Prefilling）

- 设计思路：非注意力层分块处理，注意力层保持整体处理。非注意力层（如 MLP）的中间张量是内存占用的主要来源，分块处理可减少峰值内存占用；注意力层不分块则能保持内核性能最优。
- 关键优化：通过 torch.compile 修改计算图，预分配输出张量并采用原地计算，避免分块拼接导致的内存翻倍问题。

### 2.2 后缀 KV 缓存丢弃 / 卸载（Suffix KV Cache Discarding/Offloading）

- 核心逻辑：仅在 GPU 中保留前缀 KV 缓存（支持前缀缓存复用），超出 GPU 内存限制时丢弃或卸载后缀 KV 缓存，无需依赖跨 GPU 并行即可处理超长输入。
- 实现方式：复用 vLLM 的滑动窗口注意力抽象，当前版本默认丢弃后缀缓存，可扩展至 CPU 卸载（如基于 LMCache、Mooncake Store）。

### 2.3 带连续预填充时间估计的最短预填充优先调度

- 调度逻辑：每次调度前重新估算所有等待请求的预填充时间（基于输入令牌数和前缀缓存命中数），优先执行估算时间最短的请求，提升缓存命中率。
- 公平性优化：引入公平性参数 λ，通过`score = T_prefill - λ·T_queue`调整得分，避免长等待请求饥饿，平衡平均延迟与 P99 延迟。

## 3. 系统工作流程

1. 配置阶段：用户指定最大输入长度（MIL），PrefillOnly 通过合成请求 profiling 估算 GPU 内存需求，预留部分内存用于前缀 KV 缓存；
2. 运行阶段：通过兼容 OpenAI API 的 HTTP 服务器接收请求，令牌化后经 ZeroMQ RPC 发送至调度器；调度器按上述策略选择请求，执行器完成预填充推理并返回单个令牌的概率分布。

## 4. 实验评估

### 4.1 实验设置

- 硬件：NVIDIA L4（24GB）、A100（40GB）、H100（80GB，含 NVLink 版本）；
- 模型：Llama-3.1-8B、Qwen-32B（FP8 量化）、Llama-3.3-70B（FP8 动态量化）；
- 数据集：两类模拟数据集（推荐任务：11k-17k 令牌用户画像，50 请求 / 用户；信用验证任务：40k-60k 令牌信用历史，1 请求 / 用户）；
- 基线：PagedAttention、分块预填充、管道并行、张量并行。

### 4.2 核心结果

- 吞吐量与延迟：在高查询每秒（QPS）场景下，PrefillOnly 的平均延迟和 P99 延迟显著低于基线，支持高达 4 倍的 QPS 提升；低 QPS 场景下延迟略高于并行基线，但无并行通信开销。
- 最大输入长度（MIL）：相比非并行基线提升 1.4-5 倍，如 A100 上 Qwen-32B 的 MIL 从 11k 提升至 87k，无需并行即可支持信用验证等超长输入任务。
- 缓存命中率：连续预填充时间估计使缓存命中率显著高于 FIFO 和静态最短优先调度，在推荐任务中实现理想缓存命中次数（如 2 次命中 vs 基线 1 次）。

## 5. 局限与未来方向

- 现有局限：后缀缓存丢弃无法支持后续复用，大规模部署时调度仅基于单引擎可见性；
- 未来优化：扩展 KV 缓存卸载、全局最短预填充优先路由、 contiguous KV 缓存分配（提升访问局部性）、相同前缀请求批处理。

## 6. 结论

PrefillOnly 通过适配预填充专属工作负载的特性，在无需跨 GPU 并行的情况下，实现了超长输入支持、高吞吐量与低延迟的平衡。实验证明，其在 4 类硬件、3 种模型、2 类任务中可处理高达 4 倍的 QPS，同时保持更低的平均延迟和 P99 延迟，为 LLM 在判别式任务中的规模化应用提供了高效推理方案。



## [<span id ="sosp2504-1">Robust LLM Training Infrastructure at ByteDance</span>](#sosp2504)

​	该论文来自字节跳动，于2025年SOSP会议发布，提出了大规模LLM训练基础设施ByteRobust，针对数万GPU规模训练中频发的显性（如CUDA错误）、隐性故障（如任务挂起、静默数据损坏）及代码迭代风险，通过控制平面与数据平面协同，采用实时检查、分层停机诊断、数据驱动过驱逐、原地热更新、预热备用机器、跨并行组 checkpoint 备份等核心技术，优先快速隔离故障而非精准定位，纳入人为错误因素并控制恢复过程变异性，在生产环境中实现了97%的有效训练时间占比（ETTR），大幅提升了超大规模LLM训练的稳定性与效率。

### 一、研究背景与挑战

1. **LLM 训练的规模与故障痛点**：当前 LLM 训练已达数万 GPU 规模（如 LLaMA 3 用 16,384 块 H100 GPU 训练 54 天），但故障（CUDA 错误、NaN 值、任务挂起等）频发 ——Meta 报告 16,000 GPU 训练时每 2.78 小时就出现一次硬件故障。故障导致训练中断，传统依赖日志分析和远程 checkpoint 重载的恢复方式耗时数小时至数天，严重降低有效训练时间占比（ETTR）。
2. **三大核心挑战**：
   - 隐性故障难检测：如任务无进展挂起、性能抖动、静默数据损坏（SDC）等无明确错误信号，传统超时终止机制浪费 GPU 资源；
   - 超大规模恢复难：数万 GPU 集群中故障机器替换缺乏足够备用资源，定位隔离故障节点成为关键；
   - 代码迭代引入风险：数月长周期训练中，用户需持续优化代码、调整算法，新增代码可能暗藏未被小规模测试发现的错误。

### 二、ByteRobust 核心设计

ByteRobust 是专为 LLM 训练设计的 GPU 基础设施管理系统，通过控制平面（Control Plane）和数据平面（Data Plane）协同，实现故障的自动检测、定位与快速恢复，核心设计理念包括：

1. **优先快速隔离而非精准定位**：结合轻量级实时检测与分层停机诊断，通过数据驱动的栈轨迹聚类隔离可疑机器，避免精准定位导致的大量 GPU 闲置；
2. **纳入人为错误因素**：支持代码回滚机制，利用训练故障的高发性，通过延迟更新策略融合用户代码变更与确定性故障；
3. **控制恢复过程的变异性**：采用原地热更新、预热备用机器、跨并行组 checkpoint 备份等机制，确保恢复过程稳定高效。

#### 关键模块与技术

1. **自动化容错框架**：
   - 实时检查（Proactive Real-time Checks）：秒级间隔监测网络、GPU、主机状态（如 NIC 故障、GPU 温度、内核事件），快速发现显性故障；
   - 分层停机检查（Hierarchical Stop-time Checks）：故障后运行领域特定测试（如 NCCL 通信测试），区分基础设施故障与用户代码错误；
   - 双阶段重放（Dual-Phase Replay）：针对 SDC 等未知故障，通过水平 + 垂直分组重放训练，精准定位故障机器。
2. **数据驱动过驱逐（Data-Driven Over-Eviction）**：
   - 捕获训练进程栈轨迹，通过聚合分析区分健康节点与异常节点，对异常节点所在的并行组进行 “过驱逐”（宁可多驱逐少量健康节点，也要快速恢复训练），解决隐性故障定位难题。
3. **高效恢复机制**：
   - 原地热更新（In-Place Hot-Update）：代码迭代或回滚时无需重建 Pod 环境，直接原地更新，减少重启开销；
   - 预热备用机器（Warm Standby Machines）：维持备用机器池，提前完成环境初始化和自检，故障时快速替换，避免调度延迟；
   - 跨并行组 checkpoint 备份：将模型参数和优化器状态备份到其他并行组节点的 CPU 内存和本地磁盘，避免依赖远程存储，实现近零开销的全步骤 checkpoint。

### 三、实验效果

ByteRobust 已在字节跳动生产环境部署超过一年，支持 70B + 稠密模型和 200B+ MoE 模型训练，核心效果如下：

1. **核心指标突破**：9,600 GPU 集群上三个月训练任务的 ETTR 达 97%，MFU（模型算力利用率）较初始版本提升 1.25×（稠密模型）和 1.58×（MoE 模型）；
2. **故障处理效率**：
   - 检测时间大幅缩短：如 NIC 故障检测从默认 10 分钟缩短至 30 秒，GPU 丢失从 10 分钟缩短至 10 秒；
   - 恢复速度提升显著：预热备用机器和热更新机制分别实现 10.87× 和 11.04× 的恢复加速；
   - checkpoint 开销极低：全步骤 checkpoint 仅增加 0.9% 以下开销，较 Megatron-LM 的阻塞式 checkpoint 减少 99.69% 的阻塞时间。
3. **对比传统方案**：故障解决时间平均降低 84.5%（如 CUDA 错误从 518 秒降至 93 秒），对代码调整等人为错误，传统压力测试无法定位，而 ByteRobust 通过回滚机制快速恢复。

### 四、经验与局限

1. **经验**：简单方法可解决大部分故障 ——32.52% 的故障通过实时检查直接驱逐机器解决，22.70% 通过重试恢复，仅 1.23% 需双阶段重放；
2. **局限**：GPU 诊断工具不成熟导致部分故障难溯源，过驱逐策略存在少量误判，静默数据损坏（SDC）的检测和隔离仍需高开销验证。

### 五、结论

ByteRobust 针对 LLM 超大规模训练的故障痛点，通过 “快速隔离 + 自动容错 + 高效恢复” 的一体化设计，在生产环境中实现了 97% 的高 ETTR，为大规模 LLM 训练的稳定性和效率提供了关键支撑，其设计理念对后续 LLM 训练基础设施优化具有重要参考价值。



# [<span id ="osdi-title">OSDI</span>](#osdi)

## 2025

<span id ="osdi2501">[BlitzScale: Fast and Live Large Model Autoscaling with O(1) Host Caching](#osdi2501-1)</span>



## <span id ="osdi2501-1">[BlitzScale: Fast and Live Large Model Autoscaling with O(1) Host Caching](#osdi2501)</span>

​	BLITZSCALE 是一款面向模型即服务（MAAS）的大模型自动扩缩容系统，针对现有方案数据平面加载慢、扩缩容需停止服务、缓存命中率低等痛点，通过利用 GPU 间高带宽且低利用率的计算网络（RDMA/NVLink）实现 O (1) 缓存的组播参数加载，结合细粒度分层调度与 ZigZag 流水线调度实现实时扩缩容（新实例加载部分层即可承接过载实例计算），在 BurstGPT、AzureCode 等真实工作负载及 Llama3-8B、Qwen2.5-72B 等模型上的测试显示，其相比 ServerlessLLM 最多降低 94% 尾延迟，相比 DistServe、vLLM 等无扩缩容能力的系统减少 49% GPU 服务时间，同时满足相同服务等级目标（SLO），有效平衡了服务吞吐量与硬件利用率。

### 核心背景与问题

- **MAAS 系统需求**：需平衡吞吐量（满足 SLO 的请求数）和硬件利用率，但大模型请求量存在秒级突发波动（2 秒内可达 5 倍），且 KVCache 等内存需求不可预测，传统扩缩容难以应对。
- **现有方案痛点**：数据平面加载模型参数速度慢（SSD 带宽仅 2-10 Gbps/GPU），且扩缩容是 “停止服务式”（需等待参数全加载）；依赖主机缓存的方案（如 ServerlessLLM）存在 20%-46% 缓存命中率，无法覆盖多模型场景。

### 核心创新设计

#### 1. 高速数据平面：O (1) 缓存的网络组播

- 利用 GPU 间计算网络（RDMA 100-400 Gbps、NVLink 1.6 Tbps），其带宽接近 CPU-GPU 链路且利用率低（峰值仅 7.4%），替代 SSD 加载参数。
- 采用网络优化组播：已部署实例直接组播参数（无需缓存），无部署实例时通过 O (1) 主机缓存广播（集群总内存足够缓存所有模型），避免缓存失效。

#### 2. 实时扩缩容：细粒度分层与协同执行

- 打破实例级粗粒度调度，转为层级细粒度调度：新实例无需等待参数全加载，可承接过载实例的部分层计算，通过协同执行提升吞吐量。
- 关键技术：模型感知组播规划器（快速生成无干扰的组播方案，避免扩缩容与服务竞争带宽）、ZigZag 流水线调度（动态分配层计算任务，平衡负载）。

### 系统架构

- 核心组件包括全局参数池（跟踪模型参数在 GPU/CPU 的位置）、扩缩容规划器（生成组播方案）、实时执行调度器（协调过载实例与新实例）、负载监控触发器（触发扩缩容）。
- 支持预填充 / 解码分离（PD 分离）和共存部署，适配 LLM 特有的预填充（TTFT）和解码（TBT）阶段特性。

### 性能评估结果

- 延迟优化：相比 ServerlessLLM，尾延迟降低最高 94%，首次令牌时间（TTFT）缩短 47%-75%，令牌间隔时间（TBT）缩短最高 94.1%。
- 资源利用率：与无扩缩容能力的 DistServe、vLLM 相比，在满足相同 SLO 前提下，GPU 服务时间减少 49%。
- 缓存效率：仅需 O (1) 主机缓存，缓存占用远低于 ServerlessLLM，避免多模型场景下的缓存污染。

### 适用场景与未来方向

- 适配 Llama3、Mistral、Qwen2.5 等不同规模 LLM，支持真实世界突发工作负载（BurstGPT、AzureCode 等）。
- 未来方向：优化扩缩容策略以适配更多工作负载特性，支持混合专家（MoE）等模型的内部并行配置扩缩容。





# [<span id ="anchor-title">anchor</span>](#anchor)
